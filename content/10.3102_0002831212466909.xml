<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">AER</journal-id>
<journal-id journal-id-type="hwp">spaer</journal-id>
<journal-title>American Educational Research Journal</journal-title>
<issn pub-type="ppub">0002-8312</issn>
<issn pub-type="epub">1935-1011</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/0002831212466909</article-id>
<article-id pub-id-type="publisher-id">10.3102_0002831212466909</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Section on Teaching, Learning, and Human Development</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Effects of a Data-Driven District Reform Model on State Assessment Outcomes</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Slavin</surname><given-names>Robert E.</given-names></name>
<aff id="aff1-0002831212466909">Johns Hopkins University</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Cheung</surname><given-names>Alan</given-names></name>
<aff id="aff2-0002831212466909">The Chinese University of Hong Kong Johns Hopkins University</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Holmes</surname><given-names>GwenCarol</given-names></name>
<aff id="aff3-0002831212466909">Alexandria City Public Schools</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Madden</surname><given-names>Nancy A.</given-names></name>
<aff id="aff4-0002831212466909">Success for All Foundation</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Chamberlain</surname><given-names>Anne</given-names></name>
<aff id="aff5-0002831212466909">Social Dynamics, LLC</aff>
</contrib>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2013</year>
</pub-date>
<volume>50</volume>
<issue>2</issue>
<fpage>371</fpage>
<lpage>396</lpage>
<history>
<date date-type="received">
<day>14</day>
<month>2</month>
<year>2011</year>
</date>
<date date-type="rev-recd">
<day>8</day>
<month>6</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>13</day>
<month>9</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012 AERA</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>A district-level reform model created by the Center for Data-Driven Reform in Education (CDDRE) provided consultation with district leaders on strategic use of data and selection of proven programs. Fifty-nine districts in seven states were randomly assigned to CDDRE or control conditions. A total of 397 elementary and 225 middle schools were followed over a period of up to 4 years. In a district-level hierarchical linear modeling (HLM) analysis controlling for pretests, few important differences on state tests were found 1 and 2 years after CDDRE services began. Positive effects were found on reading outcomes in elementary schools by Year 4. An exploratory analysis found that reading effects were larger for schools that selected reading programs with good evidence of effectiveness than for those that did not.</p>
</abstract>
<kwd-group>
<kwd>data-driven instruction</kwd>
<kwd>formative assessment</kwd>
<kwd>district-level reform models</kwd>
<kwd>cluster randomized experiments</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>For at least a quarter century, schools in the United States have been in a constant state of reform. Commission reports, white papers, politicians, and the press periodically warn of dire consequences if America’s schools are not substantially improved. In fact, on the 2009 National Assessment of Educational Progress (NAEP; <xref ref-type="bibr" rid="bibr28-0002831212466909">National Center for Education Statistics [NCES], 2010</xref>) and on some international measures such as the <italic>Trends in International Mathematics and Science Study</italic> (TIMSS; <xref ref-type="bibr" rid="bibr27-0002831212466909">Mullis, Martin, &amp; Foy, 2007</xref>), Program for International Student Assessment (PISA; <xref ref-type="bibr" rid="bibr29-0002831212466909">Organization for Economic Cooperation and Development, 2006</xref>), and Progress in International Reading Literacy Study (PIRLS; <xref ref-type="bibr" rid="bibr27-0002831212466909">Mullis, Martin, Kennedy, &amp; Foy, 2007</xref>), U.S. schools have shown some gains in recent years, but the pace of change is slow. In particular, although the academic performance of middle-class students is comparable to that of similar students in other countries, the most important problem in the United States is the continuing low achievement of disadvantaged and minority students. For example, on the 2009 NAEP, 42% of White students scored proficient or better, while only 16% of African American, 17% of Hispanic, and 20% of American Indian students scored at this level. Among students who do not receive free lunches, 45% scored at proficient or better, while among those who receive free lunches, only 17% scored at proficient or better. Results in mathematics and at different grade levels showed similar gaps.</p>
<p>The continuing low performance of disadvantaged and minority students must be considered in light of substantial evidence showing positive effects of a wide range of educational innovations. Many interventions have been evaluated in rigorous experiments and found to improve student achievement, especially in reading and math, in comparison to traditional methods. Yet programs with strong evidence of effectiveness are rarely widely used, and those that are widely used rarely have much, if any, evidence of effectiveness. For example, there were five commercial reading texts that were emphasized in the federal Reading First program and were among the most widely used in the United States during the period from 2000 to the present. The <xref ref-type="bibr" rid="bibr52-0002831212466909">What Works Clearinghouse (2011a)</xref>, in its beginning reading review, found supportive evidence for none of them. The same lack of evidence for these programs was reported in a review by <xref ref-type="bibr" rid="bibr39-0002831212466909">Slavin, Lake, Chambers, Cheung, and Davis (2009)</xref>. Reading programs that did have evidence of effectiveness from rigorous evaluations, such as various forms of tutoring, cooperative learning, and comprehensive school reform, are not used widely enough to have any meaningful impact on the national achievement gap. The same disconnect exists in math, where widely used textbook and computer-assisted instruction (CAI) programs have little evidence of effectiveness (<xref ref-type="bibr" rid="bibr38-0002831212466909">Slavin &amp; Lake, 2008</xref>; <xref ref-type="bibr" rid="bibr40-0002831212466909">Slavin, Lake, &amp; Groff, 2009</xref>; <xref ref-type="bibr" rid="bibr53-0002831212466909">What Works Clearinghouse, 2011b</xref>, <xref ref-type="bibr" rid="bibr54-0002831212466909">2011c</xref>), while programs that do have extensive evidence of effectiveness are not widely used.</p>
<p>The limited application of proven programs is perhaps surprising in light of the extraordinary pressure schools have been under in recent years to improve student achievement. Under No Child Left Behind, schools have been subject to increasing sanctions leading up to closure or reconstitution if they do not meet standards on state accountability measures for a period of years. Because of the universal availability of data on student performance and the pressure to increase scores, it might be assumed that schools and districts would be intent on finding and adopting programs with strong evidence of effectiveness on the types of measures for which they are held accountable. Yet this is rarely the case.</p>
<sec id="section1-0002831212466909">
<title>Data-Driven Reform</title>
<p>The push to improve test scores has led to substantial interest in the use of data within schools and districts to drive decisions and motivate change. The focus of data-driven reform approaches is on obtaining timely, useful information; trying to understand the “root causes” behind the numbers; and designing interventions targeted to the specific areas most likely to be inhibiting success. The idea is both to focus resources and efforts most efficiently where they will make the biggest difference and to break the daunting task of turning around entire schools and districts into smaller, achievable tasks that can be accomplished in a reasonable time period, building a sense among front-line educators that they are capable of making a difference on enduring problems.</p>
<p>Data-driven reform involves collection, interpretation, and dissemination of data intended to inform and guide district and school reform efforts. <xref ref-type="bibr" rid="bibr3-0002831212466909">Bernhardt (2003)</xref> identified four categories of data districts may analyze: student learning, demographics, school process, and teacher perceptions. These enable school leaders to identify specific problems faced by students and teachers, break down the data to identify individual schools and demographic groups in need of particular help, and suggest reasons for achievement gaps (<xref ref-type="bibr" rid="bibr23-0002831212466909">Kennedy, 2003</xref>; <xref ref-type="bibr" rid="bibr35-0002831212466909">Schmoker, 2003</xref>). Data-based decision making usually involves extensive professional development for school leaders to help them use data to set goals, prioritize resources, and make intervention plans (<xref ref-type="bibr" rid="bibr10-0002831212466909">Conrad &amp; Eller, 2003</xref>).</p>
<p>There is surprisingly little evidence on the effectiveness of data-driven reform strategies. That which does exist consists primarily of case studies of schools or districts that have made significant progress on state assessments. For example, the <xref ref-type="bibr" rid="bibr12-0002831212466909">Council of the Great City Schools (2002)</xref> identified big-city districts that consistently “beat the odds” in raising student achievement, concluding that these districts were characterized by coherence, planfulness, and extensive use of data to inform district and school decisions. Case studies of other “positive outlier” districts and states have reached similar conclusions (<xref ref-type="bibr" rid="bibr11-0002831212466909">Council of Chief State School Officers, 2002</xref>; <xref ref-type="bibr" rid="bibr17-0002831212466909">Grissmer &amp; Flanagan, 2001</xref>; <xref ref-type="bibr" rid="bibr43-0002831212466909">Snipes, Doolittle, &amp; Herlihy, 2002</xref>; <xref ref-type="bibr" rid="bibr45-0002831212466909">Streifer, 2002</xref>; <xref ref-type="bibr" rid="bibr47-0002831212466909">Symonds, 2003</xref>). However, such case studies only provide after-the-fact explanations of good results. We do not know, for example, whether schools and districts that did not make impressive gains may also have been trying to use the same data-driven strategies (see <xref ref-type="bibr" rid="bibr20-0002831212466909">Herman et al., 2008</xref>).</p>
<p>Frequently, districts embarking on data-driven reform adopt benchmark assessments given several times a year to determine whether students are on track toward improvement on their state assessments. The idea is to find out early where problems may exist so that changes can be made before it is too late. There is evidence that more frequent assessment is more effective than annual assessment (e.g., <xref ref-type="bibr" rid="bibr2-0002831212466909">Bangert-Drowns, Kulik, Kulik, &amp; Morgan, 1991</xref>; <xref ref-type="bibr" rid="bibr14-0002831212466909">Dempster, 1991</xref>; <xref ref-type="bibr" rid="bibr34-0002831212466909">Schmoker, 1999</xref>), and in recent years, a few experimental and quasi-experimental evaluations of the use of such benchmark assessments have been reported. The findings are mixed. A Boston program, Formative Assessments of Student Thinking in Reading (FAST-R), provided teachers with data aligned with the Massachusetts MCAS reading assessments, which they gave to students every 3 to 12 weeks. Data coaches in each school helped teachers interpret and use the formative test data. A 2-year evaluation of the program in 21 elementary schools found small, nonsignificant effects for third and fourth graders on MCAS and SAT-9 reading measures (<xref ref-type="bibr" rid="bibr31-0002831212466909">Quint, Sepanik, &amp; Smith, 2008</xref>). A 1-year study of the use of benchmark assessments in 22 Massachusetts middle schools also showed no differences (<xref ref-type="bibr" rid="bibr19-0002831212466909">Henderson, Petrosino, Guckenburg, &amp; Hamilton, 2007</xref>). An analysis of first-year data from the present study by <xref ref-type="bibr" rid="bibr6-0002831212466909">Carlson, Borman, and Robinson (2011)</xref> found significant but very small effects of the use of benchmark assessments on state mathematics assessments (effect size [ES] = +.06), but no significant effects on reading assessments (ES = +.03).</p>
<p>A study by <xref ref-type="bibr" rid="bibr24-0002831212466909">May and Robinson (2007)</xref> evaluated a benchmark assessment program used in high schools to prepare students for the Ohio Graduation Tests. The Personalized Assessment Reporting System (PARS) provided test reports for teachers, but also for students and their parents. Sixty districts were randomly assigned to use PARS or not to do so. There were no significant differences for 10th graders taking the Ohio Graduation Test for the first time, but there were positive effects for a subset of students who had initially failed the test. The second-chance students in PARS districts were more likely to take the test again and to score well on it.</p>
<p>Numerous studies have described “best practices” in the use of formative assessment data to help guide instruction. Examples include a study of “performance driven” school systems in California, Connecticut, and Texas by <xref ref-type="bibr" rid="bibr13-0002831212466909">Datnow, Park, and Wohlstetter (2007)</xref>; studies of “data-informed districts” by <xref ref-type="bibr" rid="bibr50-0002831212466909">Wayman, Jimerson, and Cho (2010)</xref>; <xref ref-type="bibr" rid="bibr51-0002831212466909">Wayman and Stringfield (2006)</xref>; <xref ref-type="bibr" rid="bibr49-0002831212466909">Wayman, Cho, and Shaw (2009)</xref>; and <xref ref-type="bibr" rid="bibr48-0002831212466909">Wayman, Cho, and Johnston (2007)</xref>; and studies of evidence-based decision making in school district central offices (e.g., <xref ref-type="bibr" rid="bibr5-0002831212466909">Bulkley, Christman, Goertz, &amp; Lawrence, 2010</xref>; <xref ref-type="bibr" rid="bibr21-0002831212466909">Honig, 2006</xref>; <xref ref-type="bibr" rid="bibr22-0002831212466909">Honig &amp; Coburn, 2008</xref>). All of these descriptive studies emphasize the need to make data important within systems, timely, and actionable and to provide professional development and ongoing assistance to help teachers and administrators use the data intelligently, collaborate to decide on actions in response to findings, and follow through on solutions that flow from the data. Yet these studies do not establish a clear connection between effective use of data tools and student outcomes. Clearly, further research is needed to draw on the lessons of best practice and assess student outcomes over time.</p>
</sec>
<sec id="section2-0002831212466909">
<title>Adding Programs With Good Evidence of Effectiveness</title>
<p>In all studies to date, the effects of implementing benchmark assessments with professional development to help educators interpret and respond appropriately to these assessments have been quite modest. It is perhaps too early to say that implementation of benchmark assessments is ineffective, but the expectation that providing periodic data on students’ performance to teachers and administrators will greatly enhance achievement on accountability measures has not been convincingly demonstrated.</p>
<p>However, it may be that data-driven instruction will have more effect on achievement if assessment data are used not only to provide frequent feedback, but also to select programs that are known from rigorous experimental research to be likely to improve outcomes in areas where weaknesses are observed. The theory of action implied in studies of benchmark assessments assumes that given frequent information on students’ progress, teachers and administrators will adjust teaching strategies or school policies to respond to documented deficiencies. No one imagines that the assessment information in itself would lead to improved achievement; it is the educators’ response to this information, the specific actions they take to remedy deficits, that are crucial. Yet these actions may or may not be implemented and may or may not be effective.</p>
<p>An alternative theory of action emphasizes the role of data-driven reform in encouraging educators to implement specific interventions known from research to be effective in improving student outcomes. By analogy, a physician’s diagnostic procedures do not cure anything in themselves but inform the selection from an armamentarium of proven treatments.</p>
<p><xref ref-type="bibr" rid="bibr7-0002831212466909">Cohen and Moffitt (2009)</xref> make this point in their discussion of the accountability movement:
<disp-quote>
<p>The states’ initiative (test-based accountability) is a version of standards-based reform, in which state policy makers and their allies seek to drive change in practice from the outside. Our analysis strongly suggests that this would be unlikely to work well, absent parallel efforts to build capacity from the inside. (p. 226)</p>
</disp-quote></p>
<p><xref ref-type="bibr" rid="bibr7-0002831212466909">Cohen and Moffitt (2009</xref>, pp. 226–227) go on to note that in order to build this capacity, the “most promising initial answers are comprehensive school reform designs . . . in which educational entrepreneurs carefully worked out designs for instruction.”</p>
<p>The study reported in this article evaluated an approach like the one suggested by <xref ref-type="bibr" rid="bibr7-0002831212466909">Cohen and Moffit (2009)</xref>, in which district and school leaders were given data and assistance to identify key problems and provide frequent feedback at the student level to teachers, as in all implementations of benchmark assessments, but were also provided with support to select and implement programs with good evidence of effectiveness likely to improve the identified areas of concern. The longitudinal design allows for evaluation of the effects of adding quarterly benchmark assessments and then the effects of adding adoption of research-based programs.</p>
</sec>
<sec id="section3-0002831212466909">
<title>Center for Data-Driven Reform in Education</title>
<p>In 2004, the U.S. Department of Education funded a research center at Johns Hopkins University to create and evaluate a replicable approach to whole-district change based on the concepts of data-driven reform. The Center for Data-Driven Reform in Education (CDDRE) was intended to try to solve the problem of scale in educational reform by working with entire school districts. The idea was to help district and school leaders understand and supplement their data, as in the studies of benchmark assessments cited previously. However, the emphasis of CDDRE was on going beyond formative assessments to help school leaders identify root causes underlying important problems and then select and effectively implement programs with good evidence of effectiveness directed toward solving those problems. The theory of action proposes that institutional change is facilitated by helping local decision makers not only understand their problems, but also making them aware of programs found in rigorous research to solve the problems identified in benchmark assessments or other data. A similar approach has been successfully used to improve outcomes such as reduced alcohol use and delinquent behaviors in a program called Communities That Care (<xref ref-type="bibr" rid="bibr15-0002831212466909">Fagan, Brooke-Weiss, Cady, &amp; Hawkins, 2009</xref>; <xref ref-type="bibr" rid="bibr16-0002831212466909">Fagan, Hawkins, &amp; Catalano, 2008</xref>; <xref ref-type="bibr" rid="bibr18-0002831212466909">Hawkins et al., 2009</xref>). Another program, called PROSPER, which also helped communities select and implement proven programs, demonstrated positive effects on substance abuse (<xref ref-type="bibr" rid="bibr44-0002831212466909">Spoth et al., 2007</xref>).</p>
<p>The CDDRE program offered to help schools adopt any program with good evidence of effectiveness and partnered with several nonprofit organizations that provide training and materials to support whole-school turnaround and have good evidence of effectiveness: Success for All (<xref ref-type="bibr" rid="bibr42-0002831212466909">Slavin, Madden, Chambers, &amp; Haxby, 2009</xref>), Direct Instruction (<xref ref-type="bibr" rid="bibr1-0002831212466909">Adams &amp; Engelmann, 1996</xref>), America’s Choice (<xref ref-type="bibr" rid="bibr46-0002831212466909">Supovitz, Poglinco, &amp; Snyder, 2001</xref>), <xref ref-type="bibr" rid="bibr25-0002831212466909">Modern Red Schoolhouse (2002)</xref>, and Co-nect (<xref ref-type="bibr" rid="bibr33-0002831212466909">Russell &amp; Robinson, 2000</xref>). All of these were found to have “moderate” or better evidence of effectiveness by the <xref ref-type="bibr" rid="bibr8-0002831212466909">Comprehensive School Reform Quality Center (2006a</xref>, <xref ref-type="bibr" rid="bibr9-0002831212466909">2006b)</xref>.</p>
</sec>
<sec id="section4-0002831212466909">
<title>Identifying Programs With Evidence of Effectiveness: Best Evidence Encyclopedia</title>
<p>In addition to information on whole-school reform models with good evidence of effectiveness, CDDRE offered information to schools and districts on reading and math programs that also had good evidence of effectiveness. Initially, it was expected that reviews of the evidence on such programs would soon be forthcoming from the What Works Clearinghouse (WWC), but the WWC reviews did not appear in time, so CDDRE created its own set of reviews, called the Best Evidence Encyclopedia (BEE; see <ext-link ext-link-type="uri" xlink:href="http://www.bestevidence.org">www.bestevidence.org</ext-link>). These eventually covered elementary math (<xref ref-type="bibr" rid="bibr38-0002831212466909">Slavin &amp; Lake, 2008</xref>), secondary math (<xref ref-type="bibr" rid="bibr40-0002831212466909">Slavin, Lake, &amp; Groff, 2009</xref>), elementary reading (<xref ref-type="bibr" rid="bibr39-0002831212466909">Slavin, Lake, Chambers, et al., 2009</xref>), and secondary reading (<xref ref-type="bibr" rid="bibr37-0002831212466909">Slavin, Cheung, Groff, &amp; Lake, 2008</xref>). The BEE categorizes programs based on both quality of research designs and sample size–weighted mean outcomes as having “strong,” “moderate,” or “limited” evidence of effectiveness or insufficient evidence. In the CDDRE project, programs were recommended if they met the “strong” or “moderate” criteria, which require at least two large (<italic>n</italic> ≥ 250) well-designed randomized or matched studies that used valid measures, had a duration of at least 12 weeks, and had a sample size–weighted mean effect size of at least +.20 (see <ext-link ext-link-type="uri" xlink:href="http://www.bestevidence.org">www.bestevidence.org</ext-link> for more details on these standards and procedures).</p>
</sec>
<sec id="section5-0002831212466909">
<title>The CDDRE Intervention</title>
<p>The services provided by CDDRE were designed to help district leaders understand and manage their own data, identify key areas of weakness and root causes for these deficits, recognize strengths and resources for reform, and then select and implement programs with good evidence of effectiveness targeted to their identified areas of need. CDDRE consultants, all of whom had experience as superintendents, principals, or other leadership roles in education, provided approximately 30 days of on-site consultation to each district over a 2-year period, depending on district size.</p>
<sec id="section6-0002831212466909">
<title>Data review</title>
<p>CDDRE consultants cooperatively planned a series of meetings with district leaders and school teams (principal and key staff) to engage in a process of exploring all sources of data already collected by the district, including standardized test scores, attendance, disciplinary referrals, retentions, special education placements, and dropouts. CDDRE consultants and district leaders discussed the district’s experiences with reform programs already in place, resources, state and federal mandates and constraints, and other factors relevant to the district’s readiness for reform. Surveys of teachers collected information on their perceptions of school strengths and needs.</p>
</sec>
<sec id="section7-0002831212466909">
<title>Benchmark assessments</title>
<p>CDDRE created a set of state-specific benchmark assessments that assessed reading and mathematics achievement in Grades 3–8 (in Pennsylvania, Grades 3–11). These quarterly benchmark assessments, called 4Sight, were created from the same assessment blueprints as those used to construct the state assessments and were written to mirror the state assessment’s content, coverage, difficulty, item types, proportions of open-ended items, and use of illustrations and other supports. The 4Sight benchmarks correlated with scores on the state tests in the range of +.80 to +.85. 4Sight benchmarks were used four to five times per year to predict what students, student subgroups, classes, and schools would have scored on the state assessments. Special software enabled school leaders and teachers to examine the data by state standard, grade, class, student subgroup, and so on. The benchmark assessments provided district and school leaders with detailed, timely, actionable information on student achievement, giving them an opportunity to take action in time to affect yearly outcomes.</p>
</sec>
<sec id="section8-0002831212466909">
<title>School walk-throughs</title>
<p>CDDRE consultants accompanied district leaders on visits to a cross-section of the district’s elementary, middle, and high schools. These structured walk-throughs provided insight for both the CDDRE consultants and the district administrators into the quality of instruction, classroom management, motivation, and organization of each school. They examined the implementation of various programs the schools were using and focused on student engagement. In addition to informing CDDRE consultants, these walk-throughs were intended to help district leaders understand the real state of education in their own schools, find out which of the many programs provided to their schools were actually in use, and create a sense of urgency to take action.</p>
</sec>
<sec id="section9-0002831212466909">
<title>Data-based solutions</title>
<p>Although many of the school leaders believed that the knowledge provided by benchmark assessments, data reviews, and walk-throughs were sufficient to cause reform to take place, the CDDRE model emphasized the idea that systematic reforms based on the data are essential if genuine progress is to be made. CDDRE consultants helped district and school leaders review potential solutions to the problems they identified. They emphasized programs and practices with good evidence of effectiveness, those identified as effective by the Best Evidence Encyclopedia. As noted earlier, the minimum standard for “good evidence” was a rating of “moderate evidence of effectiveness,” which requires two large (<italic>n</italic> ≥ 250) well-designed randomized or matched experiments with a sample size–weighted mean effect size of at least +.20. CDDRE consultants helped district and school leaders learn about well-evaluated solutions and then advised them through a process of adopting and implementing them: obtaining teacher buy-in, ensuring high-quality professional development and follow-up, and doing formative assessments of program outcomes.</p>
</sec>
</sec>
<sec id="section10-0002831212466909">
<title>Implementation Issues</title>
<p>Implementation of the CDDRE model varied substantially across the 59 districts and 608 schools, but there were patterns across the sites. First, school leaders were generally very positive about the benchmark assessments and consultation. Their initial orientation was very much toward the idea that understanding data would improve outcomes in itself. A key turning point in the series of meetings in several districts was the “walk-through,” when district and school leaders visited classes in session in their own district. Often, these leaders had reported use of various best practices, but on walk-throughs, it became apparent that most teachers were using traditional teaching methods.</p>
<p>The progression observed in most CDDRE districts was from a belief by district leaders that knowledge about data (from benchmark assessments and other sources) among teachers and administrators was sufficient, to a belief that their focus needed to be on the quality of instruction throughout the district, to a search for effective strategies to enable high-quality instructional methods to be widely and effectively used. However, it is important to note that while all CDDRE services were provided at no cost to the districts, there were significant costs as well as changes involved in adopting programs. Perhaps as a result, only about a third of all schools eventually adopted a reading program and only 8% adopted a math program, so even in these relatively motivated districts with significant time spent on data-driven reform strategies, a universal embrace of evidence-based reform was difficult to achieve.</p>
</sec>
<sec id="section11-0002831212466909">
<title>Focus of the Evaluation</title>
<p>The evaluation of the CDDRE process was intended to determine the value added to student achievement by the intervention throughout the districts involved. The intervention was delivered over a period of up to 4 years and had distinct components at different points in time that were expected to affect outcomes differentially. In the first year, all participating districts received extensive consulting on data-driven reform and almost all implemented benchmark assessments (unless they were already in use). Early-years outcomes therefore were exclusively evaluations of the data interpretation aspects of CDDRE. In later years, as many schools began to select and then implement proven programs, outcomes begin to reflect the effects of these programs. It was not the intention of the evaluation to examine impacts of particular programs, but rather to focus on the impact across the districts of the process that led to the selection and implementation of programs with good evidence of effectiveness that were attuned to their needs. Since schools that implemented programs did so at different times in different subjects, the effects of the process would be expected to appear gradually over time.</p>
<p>The research question was as follows:</p>
<list id="list1-0002831212466909" list-type="simple">
<list-item>
<p><italic>Research Question</italic>: In comparison to control groups, what were the effects of CDDRE participation (controlling for pretests) on state tests of reading and mathematics at the elementary and middle school levels?</p>
</list-item>
</list>
<p>In addition to the overall impacts, the analyses enabled us to explore alternative theoretical models to explain outcomes. If positive achievement effects were seen in the early years, or if positive effects were found in schools that never adopted a program with evidence of effectiveness, this would support a conclusion that consultation and benchmark assessments have an independent effect on achievement. If positive effects were limited to the later years and to schools that did adopt one or more programs with good evidence, this would support a conclusion that the program’s effects are mediated primarily by adoption of the programs.</p>
<p>The size of the experiment was an important part of the design. With a total of 622 elementary and middle schools serving more than 300,000 students within 59 districts, this is one of the largest cluster randomized experiments ever done in education. The large sample of schools and districts is appropriate to the study’s purpose, in that the CDDRE design was created as a means of bringing about large-scale change at the system level.</p>
</sec>
<sec id="section12-0002831212466909" sec-type="methods">
<title>Methods</title>
<sec id="section13-0002831212466909">
<title>Sample Selection</title>
<p>CDDRE districts were recruited by forming partnerships with state departments of education in seven states: Pennsylvania, Arizona, Mississippi, Indiana, Ohio, Tennessee, and Alabama. The state departments then nominated districts with many low-achieving schools. The leadership of the nominated districts was approached by CDDRE staff and offered the opportunity to participate in the project, understanding that they would be randomly assigned to receive CDDRE services in the following year or to serve as a control group. Agreement to participate was obtained before random assignment, so all participating districts were ones in which district leaders were willing to implement the CDDRE design. Within each district, district leaders could designate (in advance, before random assignment) either all schools or a subset of low-achieving schools to receive CDDRE services. Control districts received CDDRE services a year later, so this was a delayed treatment control group design. The districts were recruited in three cohorts, beginning in spring of 2005 (<italic>n</italic> = 20), 2006 (<italic>n</italic> = 13), and 2007 (<italic>n</italic> = 26). Most of the 59 districts were in Pennsylvania (32), and there were 10 in Tennessee, 4 in Alabama, 4 in Arizona, 4 in Mississippi, 3 in Indiana, and 2 in Ohio. All were high-poverty Title I districts and schools, but they ranged from small rural districts to midsized urban ones.</p>
</sec>
<sec id="section14-0002831212466909">
<title>Randomized Design</title>
<p>Pairs of volunteering districts within each state matched on demographic characteristics and prior achievement were assigned at random (by coin flip) to the experimental or control groups (although matching was not used as factor in the analyses). In four cases, no match was available, and districts were assigned individually by coin flip. The matching before random assignment was done just to reduce the possibility of inequalities within states and cohorts and was not used in the design or analysis. There were a total of 397 elementary and 225 middle schools in the 59 randomly assigned districts.</p>
<p><xref ref-type="table" rid="table1-0002831212466909">Table 1</xref> shows demographic and pretest characteristics of all experimental and control schools in the randomized sample. As the table shows, the schools were very impoverished, with 64% of students qualifying for free- or reduced-price lunches. Simple <italic>t</italic> tests were used to compare baseline variables. About 29% (Grade 5) and 31% (Grade 8) of the students were African American, 20% (at both levels) were Hispanic, and 48% (Grade 5) and 46% (Grade 8) were White. There were no significant differences between treatment and control schools on any of the baseline demographic characteristics. Enrollments, however, were significantly higher in the eighth-grade treatment group (630 vs. 510, <italic>p</italic> &lt; .001). In terms of pretest characteristics, no statistically significant differences were found between treatment and control schools for fifth-grade reading and math and eighth-grade reading. However, treatment schools scored significantly lower than control schools on eighth-grade math (<italic>p</italic> &lt; .05).</p>
<table-wrap id="table1-0002831212466909" position="float">
<label>Table 1</label>
<caption>
<p>Comparison of Baseline Achievement and Demographic Characteristics</p>
</caption>
<graphic alternate-form-of="table1-0002831212466909" xlink:href="10.3102_0002831212466909-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Variable</th>
<th align="center">Condition</th>
<th align="center"><italic>N</italic></th>
<th align="center">Mean</th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>p</italic> Value</th>
</tr>
<tr>
<th align="left" colspan="6"><hr/></th>
</tr>
<tr>
<th align="left" colspan="6">Fifth Grade</th>
</tr>
</thead>
<tbody>
<tr>
<td>Reading pretest (<italic>z</italic>)</td>
<td>Treatment</td>
<td>191</td>
<td>−0.07</td>
<td>1.04</td>
<td>.25</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>0.06</td>
<td>0.96</td>
<td/>
</tr>
<tr>
<td>Math pretest (<italic>z</italic>)</td>
<td>Treatment</td>
<td>191</td>
<td>0.01</td>
<td>1.06</td>
<td>.97</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>−0.01</td>
<td>0.93</td>
<td/>
</tr>
<tr>
<td>Free lunch (%)</td>
<td>Treatment</td>
<td>191</td>
<td>63</td>
<td>25.64</td>
<td>.41</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>65</td>
<td>24.54</td>
<td/>
</tr>
<tr>
<td>Enrollment</td>
<td>Treatment</td>
<td>191</td>
<td>470</td>
<td>240.78</td>
<td>.25</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>445</td>
<td>179.91</td>
<td/>
</tr>
<tr>
<td>Female (%)</td>
<td>Treatment</td>
<td>191</td>
<td>48</td>
<td>2.98</td>
<td>.88</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>48</td>
<td>4.48</td>
<td/>
</tr>
<tr>
<td>African American (%)</td>
<td>Treatment</td>
<td>191</td>
<td>29</td>
<td>31.82</td>
<td>.83</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>29</td>
<td>31.00</td>
<td/>
</tr>
<tr>
<td>Hispanic (%)</td>
<td>Treatment</td>
<td>191</td>
<td>18</td>
<td>25.13</td>
<td>.38</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>21</td>
<td>31.29</td>
<td/>
</tr>
<tr>
<td>White (%)</td>
<td>Treatment</td>
<td>191</td>
<td>50</td>
<td>35.51</td>
<td>.42</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>206</td>
<td>47</td>
<td>37.46</td>
<td/>
</tr>
<tr>
<td colspan="6"><hr/></td>
</tr>
<tr>
<th align="left" colspan="6">Eighth Grade</th>
</tr>
<tr>
<td colspan="6"><hr/></td>
</tr>
<tr>
<td>Reading pretest (<italic>z</italic>)</td>
<td>Treatment</td>
<td>98</td>
<td>−0.10</td>
<td>0.98</td>
<td>.18</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>0.07</td>
<td>0.99</td>
<td/>
</tr>
<tr>
<td>Math pretest (<italic>z</italic>)</td>
<td>Treatment</td>
<td>98</td>
<td>−0.15</td>
<td>1.00</td>
<td>.05</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>0.12</td>
<td>0.92</td>
<td/>
</tr>
<tr>
<td>Free lunch (%)</td>
<td>Treatment</td>
<td>98</td>
<td>63</td>
<td>26.02</td>
<td>.54</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>65</td>
<td>25.23</td>
<td/>
</tr>
<tr>
<td>Enrollment</td>
<td>Treatment</td>
<td>98</td>
<td>630</td>
<td>319.19</td>
<td>.00</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>510</td>
<td>222.86</td>
<td/>
</tr>
<tr>
<td>Female (%)</td>
<td>Treatment</td>
<td>98</td>
<td>49</td>
<td>2.61</td>
<td>.49</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>49</td>
<td>4.26</td>
<td/>
</tr>
<tr>
<td>African American (%)</td>
<td>Treatment</td>
<td>98</td>
<td>32</td>
<td>34.39</td>
<td>.59</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>30</td>
<td>31.35</td>
<td/>
</tr>
<tr>
<td>Hispanic (%)</td>
<td>Treatment</td>
<td>98</td>
<td>18</td>
<td>25.49</td>
<td>.31</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>22</td>
<td>33.03</td>
<td/>
</tr>
<tr>
<td>White (%)</td>
<td>Treatment</td>
<td>98</td>
<td>48</td>
<td>38.01</td>
<td>.55</td>
</tr>
<tr>
<td/>
<td>Control</td>
<td>127</td>
<td>45</td>
<td>37.54</td>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
<p>Since <italic>n</italic>s of districts and schools were lower in the cohorts that participated for 3 and 4 years, we also examined pretests for the final samples for each cohort. These analyses found no significant differences on pretest scores at any grade level or subject (see <xref ref-type="table" rid="table1-0002831212466909">Table 1</xref>). An intent to treat design was used, so all participating districts remained in the study regardless of what they implemented. Because data were obtained from public sources, there was no attrition of districts or schools.</p>
</sec>
<sec id="section15-0002831212466909">
<title>Measures</title>
<p>The measures for this study were the reading and math assessment scores for each state at the fifth- and eighth-grade levels: the Pennsylvania System of School Assessment (PSSA), the Tennessee Comprehensive Assessment Program (TCAP), the Alabama Reading and Mathematics Test (ARMT), the Arizona Instrument to Measure Standards (AIMS), the Mississippi Curriculum Test 2 (MCT-2), the Indiana Statewide Testing for Educational Progress-Plus (ISTEP+), and the Ohio Achievement Test (OAT). Grades 5 and 8 were reported because these were the only grade levels tested at pretest in some states. Standard school-level scores on these measures were taken from raw scores provided by each state department of education for the year prior to CDDRE implementation through 2009. School-level means were used because individual-level scores were not available from some states.</p>
<p>Prior to analysis, all scores were transformed to <italic>z</italic> scores within states and grade levels, to permit pooling across states and years. Note that this removes year-to-year variations likely to result from variations in test versions within states, which would affect control and experimental schools equally. A <italic>z</italic> score of zero indicates that a school is scoring at the average for its set of matched experimental and control schools in a given year.</p>
</sec>
<sec id="section16-0002831212466909">
<title>Analyses</title>
<p>The evaluation used hierarchical linear modeling (HLM; <xref ref-type="bibr" rid="bibr32-0002831212466909">Raudenbush, 1997</xref>), with schools nested within districts. Pretests from the spring before implementation were used as covariates. Comparisons were made each year using all districts and schools (across cohorts) that were 1 to 4 years beyond random assignment. Because districts and schools joined the CDDRE project in successive waves or cohorts, the number of districts and schools available for comparison at each posttest year diminished over time (see <xref ref-type="table" rid="table2-0002831212466909">Table 2</xref>). That is, while all schools included in the analysis had at least 2 years of posttest data, only the first cohort (<italic>n</italic> = 20 districts) accumulated 4 posttest years, and only the first and second cohorts (<italic>n</italic> = 33 districts) contributed to the third-year posttests.</p>
<table-wrap id="table2-0002831212466909" position="float">
<label>Table 2</label>
<caption>
<p>Phase-In Plan for Districts in Randomized Evaluation of the Center for Data-Driven Reform in Education (CDDRE) Treatment</p>
</caption>
<graphic alternate-form-of="table2-0002831212466909" xlink:href="10.3102_0002831212466909-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="5">Cumulative Number of Districts</th>
</tr>
<tr>
<th align="left" colspan="5"><hr/></th>
</tr>
<tr>
<th align="left">Cohort</th>
<th align="center">2005–2006</th>
<th align="center">2006–2007</th>
<th align="center">2007–2008</th>
<th align="center">2008–2009</th>
</tr>
</thead>
<tbody>
<tr>
<td>1E</td>
<td>9</td>
<td>9</td>
<td>9</td>
<td>9</td>
</tr>
<tr>
<td>1C</td>
<td>11</td>
<td>11</td>
<td>11</td>
<td>11</td>
</tr>
<tr>
<td>2E</td>
<td/>
<td>7</td>
<td>7</td>
<td>7</td>
</tr>
<tr>
<td>2C</td>
<td/>
<td>6</td>
<td>6</td>
<td>6</td>
</tr>
<tr>
<td>3E</td>
<td/>
<td/>
<td>13</td>
<td>13</td>
</tr>
<tr>
<td>3C</td>
<td/>
<td/>
<td>13</td>
<td>13</td>
</tr>
<tr>
<td>Total districts</td>
<td>20</td>
<td>33</td>
<td>59</td>
<td>59</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Effect sizes were computed as the experimental-control difference in adjusted posttest scores divided by the unadjusted school-level pooled standard deviation. School-level effect sizes were considered educationally important if they were at least +.20 (equivalent to individual-level effect sizes in the range of +.07 to +.10, because school-level standards deviations are typically two to three times lower than individual ones).</p>
<p>The main analyses involved a two-level hierarchical linear model to assess the relationship between district-level treatment status (treatment vs. control) and school-level achievement outcomes. For the hierarchical analysis, the Level 1 model is written as</p>
<p><disp-formula id="disp-formula1-0002831212466909">
<mml:math display="block" id="math1-0002831212466909">
<mml:mrow>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mi>o</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>r</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>.</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula1-0002831212466909" xlink:href="10.3102_0002831212466909-eq1.tif"/>
</disp-formula></p>
<p>This represents the posttest achievement for school <italic>i</italic> in district <italic>j</italic> with just an intercept, β<sub><italic>oj</italic></sub>. The term <italic>r<sub>ij</sub></italic> is the Level 1 residual.</p>
<p>At Level 2 of the model, we estimate the cluster-level impact of the CDDRE treatment effects on the mean posttest achievement outcome in district <italic>j</italic>. As suggested by the work of <xref ref-type="bibr" rid="bibr4-0002831212466909">Bloom, Bos, and Lee (1999)</xref> and <xref ref-type="bibr" rid="bibr32-0002831212466909">Raudenbush (1997)</xref>, we included a district-level covariate, district mean pretest score at baseline, to help reduce the unexplained variance in outcomes and to improve the power and precision of our treatment effect estimates. The fully specified Level 2 model is written as</p>
<p><disp-formula id="disp-formula2-0002831212466909">
<mml:math display="block" id="math2-0002831212466909">
<mml:mrow>
<mml:msub>
<mml:mi>β</mml:mi>
<mml:mrow>
<mml:mi>o</mml:mi>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>=</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>00</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>01</mml:mn>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>Mean Pretest</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>γ</mml:mi>
<mml:mrow>
<mml:mn>02</mml:mn>
</mml:mrow>
</mml:msub>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mo>(</mml:mo>
<mml:mrow>
<mml:mtext>CDDRE</mml:mtext>
</mml:mrow>
<mml:mo>)</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo>+</mml:mo>
<mml:msub>
<mml:mi>u</mml:mi>
<mml:mrow>
<mml:mn>0</mml:mn>
<mml:mi>j</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo>,</mml:mo>
</mml:mrow>
</mml:math>
<graphic alternate-form-of="disp-formula2-0002831212466909" xlink:href="10.3102_0002831212466909-eq2.tif"/>
</disp-formula></p>
<p>where the mean posttest intercept for district <italic>j</italic>, β<sub><italic>oj</italic></sub>, is regressed on the district-level mean pretest scores and the treatment indicator, plus a residual, <italic>u<sub>0j</sub></italic>.</p>
</sec>
</sec>
<sec id="section17-0002831212466909" sec-type="results">
<title>Results</title>
<sec id="section18-0002831212466909">
<title>Main Outcomes</title>
<p>The outcomes for the randomized design are summarized in <xref ref-type="table" rid="table3-0002831212466909">Tables 3</xref> through <xref ref-type="table" rid="table6-0002831212466909">6</xref>. <xref ref-type="table" rid="table3-0002831212466909">Table 3</xref> shows that for fifth-grade reading, treatment effects (at the district level) were not significant after 1 year (ES = +.13, <italic>ns</italic>), 2 years (ES = +.09, <italic>ns</italic>), or 3 years (ES = +.24, <italic>p</italic> &lt; .10), but were statistically significant after 4 years (ES = +.49, <italic>p</italic> &lt; .01). Eighth-grade reading (<xref ref-type="table" rid="table4-0002831212466909">Table 4</xref>) showed significant positive effects in Year 1 (ES = +.26, <italic>p</italic> &lt; .05) and Year 2 (ES = +.23, <italic>p</italic> &lt; .05), but not Year 3 (ES = +.05, <italic>ns</italic>) or Year 4 (ES = +025, <italic>ns</italic>).</p>
<table-wrap id="table3-0002831212466909" position="float">
<label>Table 3</label>
<caption>
<p>Multilevel Models Predicting District-Level Fifth-Grade Reading Outcomes</p>
</caption>
<graphic alternate-form-of="table3-0002831212466909" xlink:href="10.3102_0002831212466909-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Fifth-Grade Reading Outcomes<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="3">Year 1 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 2 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 3 (<italic>N</italic> = 33)<hr/></th>
<th align="center" colspan="3">Year 4 (<italic>N</italic> = 20)<hr/></th>
</tr>
<tr>
<th align="left">Fixed effect</th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="13">School mean achievement</td>
</tr>
<tr>
<td>Intercept</td>
<td>−0.04</td>
<td>0.07</td>
<td>−0.57</td>
<td>−0.03</td>
<td>0.04</td>
<td>−0.71</td>
<td>−0.06</td>
<td>0.07</td>
<td>−0.91</td>
<td>0.06</td>
<td>0.06</td>
<td>0.88</td>
</tr>
<tr>
<td>Mean pretest</td>
<td>0.67<xref ref-type="table-fn" rid="table-fn2-0002831212466909">***</xref></td>
<td>0.12</td>
<td>5.40</td>
<td>0.80<xref ref-type="table-fn" rid="table-fn2-0002831212466909">***</xref></td>
<td>0.10</td>
<td>8.32</td>
<td>0.75<xref ref-type="table-fn" rid="table-fn2-0002831212466909">***</xref></td>
<td>0.11</td>
<td>6.68</td>
<td>0.81<xref ref-type="table-fn" rid="table-fn2-0002831212466909">***</xref></td>
<td>0.13</td>
<td>6.45</td>
</tr>
<tr>
<td>Treatment</td>
<td>+0.13</td>
<td>0.14</td>
<td>−1.99</td>
<td>+0.09</td>
<td>0.09</td>
<td>0.38</td>
<td>+0.24<xref ref-type="table-fn" rid="table-fn2-0002831212466909">*</xref></td>
<td>0.12</td>
<td>2.00</td>
<td>+0.49<xref ref-type="table-fn" rid="table-fn2-0002831212466909">***</xref></td>
<td>0.10</td>
<td>5.13</td>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<th align="left">Random effect</th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<td>District mean achievement</td>
<td>0.11</td>
<td>114.9<xref ref-type="table-fn" rid="table-fn2-0002831212466909">***</xref></td>
<td align="center">56</td>
<td>0.01</td>
<td>58.92</td>
<td align="center">56</td>
<td>0.04</td>
<td>40.97<xref ref-type="table-fn" rid="table-fn2-0002831212466909">*</xref></td>
<td align="center">30</td>
<td>0.02</td>
<td>14.72</td>
<td align="center">17</td>
</tr>
<tr>
<td>Within-district variation</td>
<td>0.71</td>
<td/>
<td/>
<td>0.72</td>
<td/>
<td/>
<td>0.68</td>
<td/>
<td/>
<td>0.84</td>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0002831212466909">
<p><italic>Note</italic>. Multilevel models predicting reading outcomes. Level 1 model: <italic>Y<sub>ij</sub></italic> = β<sub>0<italic>j</italic></sub> + <italic>r<sub>ij</sub></italic>. Level 2 model: β<sub>0<italic>j</italic></sub> = γ<sub>00</sub> + γ<sub>01</sub>(Mean Pretest)<sub><italic>j</italic></sub> + γ<sub>02</sub>(CDDRE)<sub>
<italic>j</italic></sub> + <italic>u</italic><sub>0<italic>j</italic></sub>. <italic>N</italic> = 59 (397 schools).</p>
</fn>
<fn id="table-fn2-0002831212466909">
<label>*</label>
<p><italic>p</italic> &lt; .10. **<italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table4-0002831212466909" position="float">
<label>Table 4</label>
<caption>
<p>Multilevel Models Predicting District-Level Eighth-Grade Reading Outcomes</p>
</caption>
<graphic alternate-form-of="table4-0002831212466909" xlink:href="10.3102_0002831212466909-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Eighth-Grade Reading Outcomes<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="3">Year 1 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 2 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 3 (<italic>N</italic> = 33)<hr/></th>
<th align="center" colspan="3">Year 4 (<italic>N</italic> = 20)<hr/></th>
</tr>
<tr>
<th align="left">Fixed effect</th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="13">School mean achievement</td>
</tr>
<tr>
<td>Intercept</td>
<td>−0.02</td>
<td>0.06</td>
<td>−0.43</td>
<td>−0.05</td>
<td>0.05</td>
<td>−0.96</td>
<td>−0.15</td>
<td>0.06</td>
<td>−2.50</td>
<td>−0.15</td>
<td>0.11</td>
<td>−1.31</td>
</tr>
<tr>
<td>Mean pretest</td>
<td>0.76<xref ref-type="table-fn" rid="table-fn4-0002831212466909">***</xref></td>
<td>0.09</td>
<td>8.72</td>
<td>0.64<xref ref-type="table-fn" rid="table-fn4-0002831212466909">***</xref></td>
<td>0.08</td>
<td>7.88</td>
<td>0.66<xref ref-type="table-fn" rid="table-fn4-0002831212466909">***</xref></td>
<td>0.08</td>
<td>8.43</td>
<td>0. 80<xref ref-type="table-fn" rid="table-fn4-0002831212466909">***</xref></td>
<td>0.12</td>
<td>7.45</td>
</tr>
<tr>
<td>Treatment</td>
<td>+0.26<xref ref-type="table-fn" rid="table-fn4-0002831212466909">**</xref></td>
<td>0.11</td>
<td>2.34</td>
<td>+0.23<xref ref-type="table-fn" rid="table-fn4-0002831212466909">**</xref></td>
<td>0.10</td>
<td>2.32</td>
<td>+0.05</td>
<td>0.14</td>
<td>0.37</td>
<td>+0.25</td>
<td>0.21</td>
<td>1.23</td>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<th align="left">Random effect</th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<td>District mean achievement</td>
<td>0.03</td>
<td>45.19</td>
<td align="center">56</td>
<td>0.01</td>
<td>42.52</td>
<td align="center">56</td>
<td>0.01</td>
<td>22.37</td>
<td align="center">30</td>
<td>0.04</td>
<td>17.15</td>
<td align="center">17</td>
</tr>
<tr>
<td>Within-district variation</td>
<td>0.68</td>
<td/>
<td/>
<td>0.76</td>
<td/>
<td/>
<td>0.68</td>
<td/>
<td/>
<td>0.74</td>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn3-0002831212466909">
<p><italic>Note</italic>. Multilevel models predicting reading outcomes. Level 1 model: <italic>Y<sub>ij</sub></italic> = β<sub>0<italic>j</italic></sub> + <italic>r<sub>ij</sub></italic>. Level 2 model: β<sub>0<italic>j</italic></sub> = γ<sub>00</sub> + γ<sub>01</sub>(Mean Pretest)<sub><italic>j</italic></sub> + γ<sub>02</sub>(CDDRE)<sub><italic>j</italic></sub> + <italic>u</italic><sub>0<italic>j</italic></sub>. <italic>N</italic> = 59 (225 schools).</p>
</fn>
<fn id="table-fn4-0002831212466909">
<label>*</label>
<p><italic>p</italic> &lt; .10. **<italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table5-0002831212466909" position="float">
<label>Table 5</label>
<caption>
<p>Multilevel Models Predicting District-Level Fifth-Grade Math Outcomes</p>
</caption>
<graphic alternate-form-of="table5-0002831212466909" xlink:href="10.3102_0002831212466909-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Fifth-Grade Math Outcomes<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="3">Year 1 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 2 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 3 (<italic>N</italic> = 33)<hr/></th>
<th align="center" colspan="3">Year 4 (<italic>N</italic> = 20)<hr/></th>
</tr>
<tr>
<th align="left">Fixed effect</th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="13">School mean achievement</td>
</tr>
<tr>
<td>Intercept</td>
<td>−0.02</td>
<td>0.08</td>
<td>−0.29</td>
<td>−0.01</td>
<td>0.05</td>
<td>−0.23</td>
<td>−0.05</td>
<td>0.07</td>
<td>−0.61</td>
<td>0.08</td>
<td>0.11</td>
<td>0.65</td>
</tr>
<tr>
<td>Mean pretest</td>
<td>0.64<xref ref-type="table-fn" rid="table-fn6-0002831212466909">***</xref></td>
<td>0.14</td>
<td>4.71</td>
<td>0.71<xref ref-type="table-fn" rid="table-fn6-0002831212466909">***</xref></td>
<td>0.12</td>
<td>5.99</td>
<td>0.74<xref ref-type="table-fn" rid="table-fn6-0002831212466909">***</xref></td>
<td>0.13</td>
<td>5.43</td>
<td>0.61<xref ref-type="table-fn" rid="table-fn6-0002831212466909">***</xref></td>
<td>0.14</td>
<td>4.01</td>
</tr>
<tr>
<td>Treatment</td>
<td>+0.25</td>
<td>0.16</td>
<td>1.55</td>
<td>+0.07</td>
<td>0.10</td>
<td>0.70</td>
<td>+0.24<xref ref-type="table-fn" rid="table-fn6-0002831212466909">*</xref></td>
<td>0.10</td>
<td>1.82</td>
<td>+0.33<xref ref-type="table-fn" rid="table-fn6-0002831212466909">**</xref></td>
<td>0.17</td>
<td>1.76</td>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<th align="left">Random effect</th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<td>District mean achievement</td>
<td>0.22</td>
<td>190.9<xref ref-type="table-fn" rid="table-fn6-0002831212466909">***</xref></td>
<td align="center">56</td>
<td>0.04</td>
<td>69.88<xref ref-type="table-fn" rid="table-fn6-0002831212466909">*</xref></td>
<td align="center">56</td>
<td>0.06</td>
<td>45.48<xref ref-type="table-fn" rid="table-fn6-0002831212466909">**</xref></td>
<td align="center">30</td>
<td>0.07</td>
<td>27.12<xref ref-type="table-fn" rid="table-fn6-0002831212466909">**</xref></td>
<td align="center">17</td>
</tr>
<tr>
<td>Within-district variation</td>
<td>0.62</td>
<td/>
<td/>
<td>0.74</td>
<td/>
<td/>
<td>0.66</td>
<td/>
<td/>
<td>0.81</td>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-5-0002831212466909">
<p><italic>Note</italic>. Multilevel models predicting math outcomes. Level 1 model: <italic>Y<sub>ij</sub></italic> = β<sub>0<italic>j</italic></sub> + <italic>r<sub>ij</sub></italic>. Level 2 model: β<sub>0<italic>j</italic></sub> = γ<sub>00</sub> + γ<sub>01</sub>(Mean Pretest)<sub><italic>j</italic></sub> + γ<sub>02</sub>(CDDRE)<sub><italic>j</italic></sub> + <italic>u</italic><sub>0<italic>j</italic></sub>. <italic>N</italic> = 59 (397 schools).</p>
</fn>
<fn id="table-fn6-0002831212466909">
<label>*</label>
<p><italic>p</italic> &lt; .10. **<italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<table-wrap id="table6-0002831212466909" position="float">
<label>Table 6</label>
<caption>
<p>Multilevel Models Predicting District-Level Eighth-Grade Math Outcomes</p>
</caption>
<graphic alternate-form-of="table6-0002831212466909" xlink:href="10.3102_0002831212466909-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="12">Eighth-Grade Math Outcomes<hr/></th>
</tr>
<tr>
<th/>
<th align="center" colspan="3">Year 1 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 2 (<italic>N</italic> = 59)<hr/></th>
<th align="center" colspan="3">Year 3 (<italic>N</italic> = 33)<hr/></th>
<th align="center" colspan="3">Year 4 (<italic>N</italic> = 20)<hr/></th>
</tr>
<tr>
<th align="left">Fixed effect</th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center">Effect</th>
<th align="center"><italic>SE</italic></th>
<th align="center"><italic>t</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="13">School mean achievement</td>
</tr>
<tr>
<td>Intercept</td>
<td>0.03</td>
<td>0.04</td>
<td>0.75</td>
<td>0.03</td>
<td>0.04</td>
<td>0.64</td>
<td>−0.03</td>
<td>0.07</td>
<td>−0.48</td>
<td>−0.04</td>
<td>0.07</td>
<td>−0.69</td>
</tr>
<tr>
<td>Mean pretest</td>
<td>0.79<xref ref-type="table-fn" rid="table-fn8-0002831212466909">***</xref></td>
<td>0.06</td>
<td>13.89</td>
<td>0.76<xref ref-type="table-fn" rid="table-fn8-0002831212466909">***</xref></td>
<td>0.06</td>
<td>13.20</td>
<td>0.75<xref ref-type="table-fn" rid="table-fn8-0002831212466909">***</xref></td>
<td>0.08</td>
<td>8.78</td>
<td>0.83<xref ref-type="table-fn" rid="table-fn8-0002831212466909">***</xref></td>
<td>0.16</td>
<td>6.12</td>
</tr>
<tr>
<td>Treatment</td>
<td>+0.17<xref ref-type="table-fn" rid="table-fn8-0002831212466909">***</xref></td>
<td>0.08</td>
<td>2.09</td>
<td>+0.08</td>
<td>0.08</td>
<td>1.11</td>
<td>+0.01</td>
<td>0.14</td>
<td>0.08</td>
<td>+0.31<xref ref-type="table-fn" rid="table-fn8-0002831212466909">*</xref></td>
<td>0.15</td>
<td>1.98</td>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<th align="left">Random effect</th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
<th align="center">Estimate</th>
<th align="center">χ<sup>2</sup></th>
<th align="center"><italic>df</italic></th>
</tr>
<tr>
<td colspan="13"><hr/></td>
</tr>
<tr>
<td>District mean achievement</td>
<td>0.01</td>
<td>36.97</td>
<td align="center">56</td>
<td>0.01</td>
<td>33.83</td>
<td align="center">56</td>
<td>0.01</td>
<td>29.38</td>
<td align="center">30</td>
<td>0.02</td>
<td>14.21</td>
<td align="center">17</td>
</tr>
<tr>
<td>Within-district variation</td>
<td>0.64</td>
<td/>
<td/>
<td>0.64</td>
<td/>
<td/>
<td>0.61</td>
<td/>
<td/>
<td>0.75</td>
<td/>
<td/>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn7-0002831212466909">
<p><italic>Note</italic>. Multilevel models predicting math outcomes. Level 1 model: <italic>Y<sub>ij</sub></italic> = β<sub>0<italic>j</italic></sub> + <italic>r<sub>ij</sub></italic>. Level 2 model: β<sub>0<italic>j</italic></sub> = γ<sub>00</sub> + γ<sub>01</sub>(Mean Pretest)<sub><italic>j</italic></sub> + γ<sub>02</sub>(CDDRE)<sub><italic>j</italic></sub> + <italic>u</italic><sub>0<italic>j</italic></sub>. <italic>N</italic> = 59 (225 schools).</p>
</fn>
<fn id="table-fn8-0002831212466909">
<label>*</label>
<p><italic>p</italic> &lt; .10. **<italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Mathematics effects for fifth grade (<xref ref-type="table" rid="table5-0002831212466909">Table 5</xref>) were nonsignificant in Year 1 (ES = +.25, <italic>ns</italic>), Year 2 (ES = +.07, <italic>ns</italic>), Year 3 (ES = +.24, <italic>p</italic> &lt; .10), and Year 4 (ES = +.33, <italic>p</italic> &lt; .09). <xref ref-type="table" rid="table6-0002831212466909">Table 6</xref> shows positive effects for eighth-grade math in Year 1 (ES = +.17, <italic>p</italic> &lt; .01) but not Year 2 (ES = +.08, <italic>ns</italic>) or Year 3 (ES = +.01, <italic>ns</italic>). Math effects approached conventional statistical significance in Year 4 (ES = +.31, <italic>p</italic> &lt; .06).</p>
<p>Due to the phasing in of the participating districts over a 3-year period, it is possible that the trend toward more positive outcomes in Years 3 and 4 than in Years 1 and 2 were due to differences among the cohorts rather than to treatment effects over time. In order to rule out this alternative explanation, we carried out analyses including only the cohorts that reached the third and fourth years (i.e., Cohorts 1 and 2). The patterns were very similar to those shown in <xref ref-type="table" rid="table3-0002831212466909">Tables 3</xref> through <xref ref-type="table" rid="table6-0002831212466909">6</xref>. Effect sizes for this longitudinal sample are shown in <xref ref-type="table" rid="table7-0002831212466909">Table 7</xref>.</p>
<table-wrap id="table7-0002831212466909" position="float">
<label>Table 7</label>
<caption>
<p>Effect Sizes for Cohorts 1 and 2</p>
</caption>
<graphic alternate-form-of="table7-0002831212466909" xlink:href="10.3102_0002831212466909-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Year 1 <italic>N</italic> = 33</th>
<th align="center">Year 2 <italic>N</italic> = 33</th>
<th align="center">Year 3 <italic>N</italic> = 33</th>
<th align="center">Year 4 <italic>N</italic> = 20</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fifth-grade reading</td>
<td>+.17</td>
<td>+.11</td>
<td>+.24<xref ref-type="table-fn" rid="table-fn9-0002831212466909">*</xref></td>
<td>+.50<xref ref-type="table-fn" rid="table-fn9-0002831212466909">***</xref></td>
</tr>
<tr>
<td>Eighth-grade reading</td>
<td>+.17</td>
<td>+.31<xref ref-type="table-fn" rid="table-fn9-0002831212466909">***</xref></td>
<td>+.05</td>
<td>+.25</td>
</tr>
<tr>
<td>Fifth-grade math</td>
<td>+.30<xref ref-type="table-fn" rid="table-fn9-0002831212466909">**</xref></td>
<td>+.10</td>
<td>+.24<xref ref-type="table-fn" rid="table-fn9-0002831212466909">*</xref></td>
<td>+.32<xref ref-type="table-fn" rid="table-fn9-0002831212466909">*</xref></td>
</tr>
<tr>
<td>Eighth-grade math</td>
<td>+.09</td>
<td>+.16</td>
<td>+.01</td>
<td>+.31<xref ref-type="table-fn" rid="table-fn9-0002831212466909">*</xref></td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn9-0002831212466909">
<label>*</label>
<p><italic>p</italic> &lt; .10. **<italic>p</italic> &lt; .05. ***<italic>p</italic> &lt; .01.</p>
</fn>
</table-wrap-foot>
</table-wrap>
<sec id="section19-0002831212466909">
<title>Exploratory analyses of effects of adopting reading programs</title>
<p>An exploratory analysis of schools that did or did not adopt proven reading programs supports the idea that the data-driven reform model produced educationally significant effects only when schools actually implemented an evidence-based program. Overall, 30% of Year 3 (<italic>n</italic> = 68) and 42% of Year 4 (<italic>n</italic> = 50) schools adopted a reading program at the elementary level, and 28% of third year (<italic>n</italic> = 34) and 33% of fourth-year (<italic>n</italic> = 20) middle schools had adopted a reading program. For the exploratory analyses, schools outside of the randomized sample but in the same state were selected to serve as matched controls for the experimental schools, matching on prior state test scores, percentage free lunch, and percentage minority. Schools that did or did not adopt reading programs by Years 3 or 4 were compared on their reading test scores, controlling for baseline scores from before Year 1, to their matched counterparts (see <xref ref-type="bibr" rid="bibr41-0002831212466909">Slavin &amp; Madden, 2011</xref>, for details of these analyses). For fifth-grade reading, schools that adopted reading programs outscored matched controls at Year 3 (ES = +.49, <italic>p</italic> &lt; .01) and Year 4 (ES = +.39, <italic>p</italic> &lt; .05), while those that had not implemented a reading program did not score better than their controls at Year 3 (ES = +.19, <italic>p</italic> &lt; .07) or Year 4 (ES = +.06, <italic>ns</italic>). In eighth grade, similar analyses showed positive effects for schools that adopted reading programs in Year 3 (ES = +.37, <italic>p</italic> &lt; .05) and approached significance in Year 4 (ES = +.45, <italic>p</italic> &lt; .07), while those that did not adopt a program scored significantly better than controls at Year 3 (ES = +.33, <italic>p</italic> &lt; .05) but not Year 4 (ES = +.13, <italic>ns</italic>). It is important to note that these are correlational analyses, so it may be that factors other than adoption of research-supported programs account for the findings (e.g., the schools that adopted programs may have had more motivated or reform-oriented staffs). However, any such school-inherent factors did not lead to greater improvements compared to matched controls in the first 2 study years, so the timing of the gains supports the possibility that they are due to program adoption.</p>
<p>Similar exploratory analyses were not attempted for mathematics, where a much smaller percentage of schools implemented proven programs (8% at both grade levels).</p>
</sec>
</sec>
</sec>
<sec id="section20-0002831212466909" sec-type="discussion">
<title>Discussion</title>
<p>The findings of the district-level randomized evaluation of CDDRE show directionally positive effects on reading and math measures and at both grade levels (5 and 8). Effect sizes generally increased in Years 3 and 4, but due to smaller numbers of districts in cohorts in that reached Years 3 and 4 by the end of the study (i.e., the earliest to start), not all of these effects were significantly positive (the minimum detectable effect size for Year 4 was estimated at .40 in the HLM analysis). An earlier analysis of first-year data from the present study by <xref ref-type="bibr" rid="bibr6-0002831212466909">Carlson et al. (2011)</xref> found small (though significant) positive effects of CDDRE on math but no effects on reading.</p>
<p>Based on the finding that there were few important district-level first-year or second-year effects in either subject or grade level, it appears that the provision of workshops and implementation of benchmarks were not sufficient to bring about educationally significant changes in student performance. These findings are in accord with the outcomes of previous large-scale evaluations of benchmark assessment plans by <xref ref-type="bibr" rid="bibr31-0002831212466909">Quint et al. (2008)</xref> and <xref ref-type="bibr" rid="bibr19-0002831212466909">Henderson et al. (2007)</xref>, who also found small effects. They are also in accord with the CDDRE program’s theory of action; first- and second-year interventions were analogous to taking a patient’s temperature, not providing a treatment.</p>
<p>By the third implementation year, school and district leaders were, in many locations, beginning to take action based on the data, especially in reading. There were positive, educationally significant effects in both subjects and both grade levels by Year 4.</p>
<p>What the findings imply is that helping school leaders understand student data is helpful but in itself does not produce educationally important gains in achievement. Schools must actually take action to change teaching and learning. The exploratory analyses comparing reading gains in schools that did or did not implement reading programs provides additional evidence for this commonsense conclusion. The findings support a model of change in which initial consultation and implementation of benchmark assessments motivate school leaders to adopt programs with good evidence of effectiveness, and it is these programs that lead to achievement gains, not the consultation or benchmarks in themselves. In order to bring about substantial improvements in student achievement, teachers need to be using effective practices every day, and consultations, benchmark data, and policies encouraging the use of proven replicable strategies may serve as a means of encouraging large numbers of schools and teachers to adopt such programs.</p>
<p>An important finding of the CDDRE experiment was that it was possible to get many schools to adopt programs with good evidence of effectiveness, at least in reading, by engaging them in a process of self-examination, benchmark assessments, and exposure to information on research-based alternatives. Very few schools had adopted any such programs before their involvement with CDDRE, and by the fourth year, 42% of elementary schools and 33% of middle schools had implemented reading programs that had been successfully evaluated in rigorous experiments. However, the process was slow and uneven, as has been reported in similar programs in areas other than education (e.g., <xref ref-type="bibr" rid="bibr15-0002831212466909">Fagan et al., 2009</xref>). Fewer than half of all eligible schools eventually adopted reading programs, and only 8% adopted math programs. The same programs chosen by the CDDRE schools are typically adopted outside the CDDRE experiment in a period of months, especially if funding is available to help defray the costs (as was true when Obey-Porter comprehensive school reform funding was available in the late 1990s, for example; see <xref ref-type="bibr" rid="bibr36-0002831212466909">Slavin, 2008</xref>). Something like the CDDRE process may be necessary for schools and districts otherwise unlikely to implement programs with good evidence of effectiveness, but schools that are already aware of their needs may benefit from faster (but perhaps less customized) methods of informing them about the effective alternatives available to them.</p>
<p>Where CDDRE appeared to make its largest differences, in the fourth year in elementary schools, the magnitude of the effects was surprisingly large, averaging school-level effect sizes of +.49 in reading (equivalent to a student-level effect size of about +.20). The ability to make this much difference on such a large scale is important. If outcomes of similar magnitude are seen in replications, these findings may point to a relatively inexpensive, readily scalable strategy for making a difference in the performance of high-poverty schools.</p>
<sec id="section21-0002831212466909">
<title>Limitations</title>
<p>There are several limitations of this research worth noting. First, the use of a delayed-treatment design means that instead of comparing districts randomly assigned to implement or not implement the CDDRE design, it compared districts randomly assigned to implement the design for a longer or shorter period of time. The fact that the control districts had been implementing CDDRE (though for a shorter time period) may have reduced the observed impacts.</p>
<p>Second, it would have been useful to include a qualitative component in the research to better understand the findings. What did district leaders, principals, and teachers actually do in CDDRE districts that they did less of in control districts? We have reported our informal observations, but more systematic qualitative work would have been very instructive.</p>
</sec>
<sec id="section22-0002831212466909">
<title>Suggestions for Future Research</title>
<p>The findings of the CDDRE study suggest several directions for future research. First, research should address the limitations identified previously. It may be useful to compare districts randomly assigned to implement a CDDRE-like intervention to those assigned to a business-as-usual condition instead of using a delayed treatment design. It would certainly be beneficial to collect qualitative or survey data to investigate more deeply how behaviors change among teachers, principals, and central office leaders in participating districts and contrast these changes with whatever is happening in control districts.</p>
<p>Further, CDDRE is a complex, multifaceted intervention. It would be useful to carry out component analyses to learn more about which aspects of the intervention account for any differences in outcomes. In particular, further research might try to more distinctly separate the effects of use of benchmark assessments from the effects of considering and adopting programs with good evidence of effectiveness.</p>
</sec>
<sec id="section23-0002831212466909">
<title>Implications for Policy</title>
<p>The size and nature of the CDDRE experiment was intended to allow the findings to have implications for policy. An intervention that can produce meaningful positive effects across 59 high-poverty districts needs to be taken seriously by policymakers.</p>
<p>There are several important policy implications of the research on CDDRE. The most important, noted previously, is that it does not seem to be sufficient to implement quarterly benchmark assessments tied to state assessments. Instead, the findings from assessments need to be used to identify areas in need of intervention and then replicable interventions with evidence of positive outcomes from rigorous experiments should be made available to schools that need them.</p>
<p>The CDDRE design offers a means of introducing evidence-based programs to schools in a way that is neither entirely top-down nor entirely bottom-up. Because of accountability pressures, as well as their own concern about their students’ success, school and district leaders are motivated to do something to improve outcomes. The CDDRE design offers district and school leaders an array of programs with good evidence of effectiveness and lets school staffs choose among them. Schools are not forced to choose from the menu of options, but if they do make such a choice they are likely to be choosing a program with good evidence of effectiveness that their district (and CDDRE staff) is able to support. The choice process gives school staffs a sense of ownership and buy-in, without mandating a solution, so staff members may be more likely to put their energies into implementing the programs they chose with enthusiasm. Schools can make other choices, but they are all ultimately accountable for the outcomes. Strategies of this kind may be particularly useful in programs targeted at failing schools, such as School Improvement Grants (SIG). Such schools are not well placed to invent entirely new approaches to their serious problems and may be better served by selecting among well-structured interventions already known to be effective.</p>
<p>The greatest limitation to widespread adoption of programs with good evidence of effectiveness is the relative paucity of such programs that are capable of working at scale. To this end, the Obama administration’s Investing in Innovation (i3) program is providing substantial funding to help a broad range of developers create, evaluate, and disseminate programs with strong evidence of effectiveness. Further efforts along these lines will be needed to provide school leaders with a broad array of proven solutions to their core, enduring problems.</p>
<p>The CDDRE model, or policies of this kind, could be readily adopted by Title I or other federal or state programs intent on improving student outcomes at a broad scale (see Cohen &amp; Moffitt, 2011; <xref ref-type="bibr" rid="bibr30-0002831212466909">Peurach, 2011</xref>). That is, federal policies could support the creation of a wide range of solutions known to effectively address enduring problems of education, as the i3 program is currently doing. They could then provide information and incentives to schools to enable them to make informed choices of programs to meet their needs. Such policies could profoundly change the conversation about school reform, moving from top-down control to incentivizing adoption of evidence-based approaches.</p>
</sec>
</sec>
</body>
<back>
<notes>
<fn-group>
<fn fn-type="supported-by">
<p>This research was supported by a grant from the Institute for Education Science, U.S. Department of Education (No. R-305A040082). However, any opinions expressed are those of the authors and do not represent the positions or policies of the U.S. Department of Education.</p>
</fn>
</fn-group>
</notes>
<bio>
<p>R<sc>obert</sc> E. S<sc>lavin</sc> is director of the Center for Research and Reform in Education at Johns Hopkins University, 200 W. Towsontown Blvd., Baltimore, MD 21204; e-mail: <italic><email>rslavin@jhu.edu</email></italic>, and professor at the Institute for Effective Education, University of York, England. His research interests include evidence-based education, whole-school reform, cooperative learning, and review of research.</p>
<p>A<sc>lan</sc> C<sc>heung</sc> is an associate professor in the Department of Educational Administration and Policy at The Chinese University of Hong Kong and associate professor at the Center for Research and Reform in Education at Johns Hopkins University. His research interests include research review and statistical analysis.</p>
<p>G<sc>wen</sc>C<sc>arol</sc> H<sc>olmes</sc> is the chief academic officer of the Alexandria City Public Schools in Virginia, and the former chief operating officer of the Success for All Foundation. She assisted in implementing the program to use data to guide classroom instruction.</p>
<p>N<sc>ancy</sc> A. M<sc>adden</sc> is the president and CEO of the Success for All Foundation. Her research interests include cooperative learning, reading, reading disabilities, and whole-school reform.</p>
<p>A<sc>nne</sc> C<sc>hamberlain</sc> is the current director of program evaluation at Social Dynamics and is a former manager of research and evaluation at the Success for All Foundation. Her research interests include qualitative research and evaluation.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Adams</surname><given-names>G. L.</given-names></name>
<name><surname>Engelmann</surname><given-names>S.</given-names></name>
</person-group> (<year>1996</year>). <source>Research on Direct Instruction: 25 years beyond DISTAR</source>. <publisher-loc>Seattle, WA</publisher-loc>: <publisher-name>Educational Achievement Systems</publisher-name>.</citation>
</ref>
<ref id="bibr2-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bangert-Drowns</surname><given-names>R. L.</given-names></name>
<name><surname>Kulik</surname><given-names>C. C.</given-names></name>
<name><surname>Kulik</surname><given-names>J. A.</given-names></name>
<name><surname>Morgan</surname><given-names>M.</given-names></name>
</person-group> (<year>1991</year>). <article-title>The instructional effect of feedback in test-like events</article-title>. <source>Review of Educational Research</source>, <volume>61</volume>(<issue>2</issue>), <fpage>213</fpage>–<lpage>238</lpage>.</citation>
</ref>
<ref id="bibr3-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bernhardt</surname><given-names>V. L.</given-names></name>
</person-group> (<year>2003</year>). <article-title>No schools left behind</article-title>. <source>Educational Leadership</source>, <volume>60</volume>(<issue>5</issue>), <fpage>26</fpage>–<lpage>30</lpage>.</citation>
</ref>
<ref id="bibr4-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bloom</surname><given-names>H. S.</given-names></name>
<name><surname>Bos</surname><given-names>J. M.</given-names></name>
<name><surname>Lee</surname><given-names>S. W.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Using cluster random assignment to measure program impacts: Statistical implications for the evaluation of education programs</article-title>. <source>Evaluation Review</source>, <volume>23</volume>, <fpage>445</fpage>–<lpage>469</lpage>.</citation>
</ref>
<ref id="bibr5-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bulkley</surname><given-names>K. E.</given-names></name>
<name><surname>Christman</surname><given-names>J. B.</given-names></name>
<name><surname>Goertz</surname><given-names>M. E.</given-names></name>
<name><surname>Lawrence</surname><given-names>N. R.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Building with benchmarks: The role of the district in Philadelphia’s benchmark assessment system</article-title>. <source>Peabody Journal of Education</source>, <volume>85</volume>(<issue>2</issue>), <fpage>186</fpage>–<lpage>204</lpage>.</citation>
</ref>
<ref id="bibr6-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carlson</surname><given-names>D.</given-names></name>
<name><surname>Borman</surname><given-names>G. D.</given-names></name>
<name><surname>Robinson</surname><given-names>M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A multi-state district-level cluster randomized trial of the impact of data-driven reform on reading and mathematics achievement</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>33</volume>(<issue>3</issue>), <fpage>378</fpage>–<lpage>398</lpage>.</citation>
</ref>
<ref id="bibr7-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cohen</surname><given-names>D. K.</given-names></name>
<name><surname>Moffitt</surname><given-names>S. L.</given-names></name>
</person-group> (<year>2009</year>). <source>The ordeal of equality: Did federal regulation fix the schools?</source> <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>.</citation>
</ref>
<ref id="bibr8-0002831212466909">
<citation citation-type="book">
<collab>Comprehensive School Reform Quality Center</collab>. (<year>2006a</year>). <source>CSRQ Center report on elementary school comprehensive school reform models</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Institutes for Research</publisher-name>.</citation>
</ref>
<ref id="bibr9-0002831212466909">
<citation citation-type="book">
<collab>Comprehensive School Reform Quality Center</collab>. (<year>2006b</year>). <source>CSRQ Center report on secondary school comprehensive school reform models</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>American Institutes for Research</publisher-name>.</citation>
</ref>
<ref id="bibr10-0002831212466909">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Conrad</surname><given-names>W. H.</given-names></name>
<name><surname>Eller</surname><given-names>B.</given-names></name>
</person-group> (<year>2003</year>, <month>April</month>). <source>District data-informed decision making</source>. <conf-name>Paper presented at the annual meetings of the American Educational Research Association</conf-name>, <conf-loc>Chicago, IL</conf-loc>.</citation>
</ref>
<ref id="bibr11-0002831212466909">
<citation citation-type="book">
<collab>Council of Chief State School Officers</collab>. (<year>2002</year>). <source>Beating the odds II</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr12-0002831212466909">
<citation citation-type="book">
<collab>Council of the Great City Schools</collab>. (<year>2002</year>). <source>Beating the odds II</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr13-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Datnow</surname><given-names>A.</given-names></name>
<name><surname>Park</surname><given-names>V.</given-names></name>
<name><surname>Wohlstetter</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <bold>Achieving with data.</bold> <source>How high-performing school systems use data to improve instruction for elementary students</source>. <publisher-loc>Los Angeles, CA</publisher-loc>: <publisher-name>Center on Educational Governance, University of Southern California</publisher-name>.</citation>
</ref>
<ref id="bibr14-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dempster</surname><given-names>F. N.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Synthesis of research on reviews and tests</article-title>. <source>Educational Leadership</source>, <volume>72</volume>(<issue>8</issue>), <fpage>71</fpage>–<lpage>76</lpage>.</citation>
</ref>
<ref id="bibr15-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fagan</surname><given-names>A. A.</given-names></name>
<name><surname>Brooke-Weiss</surname><given-names>B. L.</given-names></name>
<name><surname>Cady</surname><given-names>R.</given-names></name>
<name><surname>Hawkins</surname><given-names>J. D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>If at first you don’t succeed . . . keep trying: Strategies to enhance coalition/school partnerships to implement school-based prevention programming</article-title>. <source>Australian and New Zealand Journal of Criminology</source>, <volume>42</volume>(<issue>3</issue>), <fpage>387</fpage>–<lpage>405</lpage>.</citation>
</ref>
<ref id="bibr16-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fagan</surname><given-names>A. A.</given-names></name>
<name><surname>Hawkins</surname><given-names>J. D.</given-names></name>
<name><surname>Catalano</surname><given-names>R. F.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Using community epidemiologic data to improve social settings: The Communities That Care Prevention System</article-title>. In <person-group person-group-type="editor">
<name><surname>Shinn</surname><given-names>M.</given-names></name>
<name><surname>Yoshikawa</surname><given-names>H.</given-names></name>
</person-group> (Eds.), <source>Toward positive youth development: Transforming schools and community programs</source> (pp. <fpage>292</fpage>–<lpage>312</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Grissmer</surname><given-names>D.</given-names></name>
<name><surname>Flanagan</surname><given-names>A.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Searching for indirect evidence for the effects of statewide reforms</article-title>. In <person-group person-group-type="editor">
<name><surname>Ravitch</surname><given-names>D.</given-names></name>
</person-group> (Ed.), <source>Brookings papers on education policy</source> (pp. <fpage>131</fpage>–<lpage>179</lpage>). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Brookings Institution</publisher-name>.</citation>
</ref>
<ref id="bibr18-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hawkins</surname><given-names>J. D.</given-names></name>
<name><surname>Oesterle</surname><given-names>S.</given-names></name>
<name><surname>Brown</surname><given-names>E. C.</given-names></name>
<name><surname>Arthur</surname><given-names>M. W.</given-names></name>
<name><surname>Abbott</surname><given-names>R. D.</given-names></name>
<name><surname>Fagan</surname><given-names>A. A.</given-names></name>
<name><surname>Catalano</surname><given-names>R. F.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Results of a type 2 translational research trial to prevent adolescent drug use and delinquency: A test of Communities That Care</article-title>. <source>Archives of Pediatrics and Adolescent Medicine</source>, <volume>163</volume>(<issue>9</issue>), <fpage>789</fpage>–<lpage>798</lpage>.</citation>
</ref>
<ref id="bibr19-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Henderson</surname><given-names>S.</given-names></name>
<name><surname>Petrosino</surname><given-names>A.</given-names></name>
<name><surname>Guckenburg</surname><given-names>S.</given-names></name>
<name><surname>Hamilton</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <source>Measuring how benchmark assessments affect student achievement</source> (Issues &amp; Answers Report, REL 2007–No. 039). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>U.S. Department of Education, Institute of Education Sciences, National Center for Education Evaluation and Regional Assistance, Regional Educational Laboratory Northeast and Islands</publisher-name>.</citation>
</ref>
<ref id="bibr20-0002831212466909">
<citation citation-type="gov">
<person-group person-group-type="author">
<name><surname>Herman</surname><given-names>R.</given-names></name>
<name><surname>Dawson</surname><given-names>P.</given-names></name>
<name><surname>Dee</surname><given-names>T.</given-names></name>
<name><surname>Greene</surname><given-names>J.</given-names></name>
<name><surname>Maynard</surname><given-names>R.</given-names></name>
<name><surname>Redding</surname><given-names>S.</given-names></name>
<name><surname>Darwin</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <source>Turning around chronically low-performing schools: A practice guide</source> (NCEE #2008-4020). <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Institute of Education Sciences, U.S. Department of Education</publisher-name>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/wwc/practiceguides">http://ies.ed.gov/ncee/wwc/practiceguides</ext-link>
</citation>
</ref>
<ref id="bibr21-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Honig</surname><given-names>M. I.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Street-level bureaucracy revisited: Frontline district central office administrators as boundary spanners in education policy implementation</article-title>. <source>Educational Evaluation and Policy Analysis</source>, <volume>28</volume>(<issue>4</issue>), <fpage>357</fpage>–<lpage>383</lpage>.</citation>
</ref>
<ref id="bibr22-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Honig</surname><given-names>M. I.</given-names></name>
<name><surname>Coburn</surname><given-names>C. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Evidence-based decision-making in school district central offices: Toward a research agenda</article-title>. <source>Educational Policy</source>, <volume>22</volume>(<issue>4</issue>), <fpage>578</fpage>–<lpage>608</lpage>.</citation>
</ref>
<ref id="bibr23-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kennedy</surname><given-names>E.</given-names></name>
</person-group> (<year>2003</year>). <source>Raising test scores for all students: An administrator’s guide to improving standardized test performance</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin Press</publisher-name>.</citation>
</ref>
<ref id="bibr24-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>May</surname><given-names>H.</given-names></name>
<name><surname>Robinson</surname><given-names>M. A.</given-names></name>
</person-group> (<year>2007</year>). <source>A randomized evaluation of Ohio’s Personalized Assessment Reporting System (PARS)</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>University of Pennsylvania Consortium for Policy Research in Education</publisher-name>.</citation>
</ref>
<ref id="bibr25-0002831212466909">
<citation citation-type="book">
<collab>Modern Red Schoolhouse</collab>. (<year>2002</year>). <source>School improvement: Research results for Modern Red Schoolhouse</source>. <publisher-loc>Nashville, TN</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr26-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mullis</surname><given-names>I. V. S.</given-names></name>
<name><surname>Martin</surname><given-names>M. O.</given-names></name>
<name><surname>Foy</surname><given-names>P.</given-names></name>
</person-group> (with <person-group person-group-type="author">
<name><surname>Olson</surname><given-names>J. F.</given-names></name>
<name><surname>Preuschoff</surname><given-names>C.</given-names></name>
<name><surname>Erberber</surname><given-names>E.</given-names></name>
<name><surname>Arora</surname><given-names>A.</given-names></name>
<name><surname>Galia</surname><given-names>J.</given-names></name>
</person-group>). (<year>2008</year>). <source>TIMSS 2007 international mathematics report: Findings from IEA’s Trends in International Mathematics and Science Study at the fourth and eighth grades</source>. <publisher-loc>Chestnut Hill, MA</publisher-loc>: <publisher-name>TIMSS &amp; PIRLS International Study Center, Boston College</publisher-name>.</citation>
</ref>
<ref id="bibr27-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mullis</surname><given-names>I.</given-names></name>
<name><surname>Martin</surname><given-names>M.</given-names></name>
<name><surname>Kennedy</surname><given-names>A.</given-names></name>
<name><surname>Foy</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <source>IEA’s Progress in International Reading Literacy Study in primary school in 40 countries</source>. <publisher-loc>Chestnut Hill, MA</publisher-loc>: <publisher-name>TIMSS &amp; PIRLS International Study Center, Boston College</publisher-name>.</citation>
</ref>
<ref id="bibr28-0002831212466909">
<citation citation-type="book">
<collab>National Center for Education Statistics</collab>. (<year>2010</year>). <source>The nation’s report card</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Author</publisher-name>.</citation>
</ref>
<ref id="bibr29-0002831212466909">
<citation citation-type="web">
<collab>Organization for Economic Cooperation and Development</collab> (<year>2006</year>). <source>Program for international student assessment, 2006</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.oecd.org/edu/preschoolandschool/programmeforinternationalstudentassessmentpisa/pisa2006results.htm#News_releases">http://www.oecd.org/edu/preschoolandschool/programmeforinternationalstudentassessmentpisa/pisa2006results.htm#News_releases</ext-link>
</citation>
</ref>
<ref id="bibr30-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Peurach</surname><given-names>D.</given-names></name>
</person-group> (<year>2011</year>). <source>Seeing complexity in public education</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr31-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Quint</surname><given-names>J.</given-names></name>
<name><surname>Sepanik</surname><given-names>S.</given-names></name>
<name><surname>Smith</surname><given-names>J.</given-names></name>
</person-group>, &amp; <collab>MDRC</collab>. (<year>2008</year>). <source>Using student data to improve teaching and learning: Findings from an evaluation of the Formative Assessments of Students Thinking in Reading (FAST-R) Program in Boston elementary schools</source>. Retrieved from ERIC database. (ED503919)</citation>
</ref>
<ref id="bibr32-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Raudenbush</surname><given-names>S. W.</given-names></name>
</person-group> (<year>1997</year>). <article-title>Statistical analysis and optimal design for cluster randomized trials</article-title>. <source>Psychological Methods</source>, <volume>2</volume>, <fpage>173</fpage>–<lpage>185</lpage>.</citation>
</ref>
<ref id="bibr33-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>M.</given-names></name>
<name><surname>Robinson</surname><given-names>R.</given-names></name>
</person-group> (<year>2000</year>). <source>Co-nect retrospective outcomes study</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Boston College, Center for the Study of Testing, Evaluation, and Educational Policy</publisher-name>.</citation>
</ref>
<ref id="bibr34-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Schmoker</surname><given-names>M.</given-names></name>
</person-group> (<year>1999</year>). <source>Results: The key to continuous school improvement</source> (<edition>2nd ed.</edition>). <publisher-loc>Alexandria, VA</publisher-loc>: <publisher-name>ASCD</publisher-name>.</citation>
</ref>
<ref id="bibr35-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schmoker</surname><given-names>M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>First things first: Demystifying data analysis</article-title>. <source>Educational Leadership</source>, <volume>60</volume>(<issue>5</issue>), <fpage>22</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr36-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Comprehensive school reform</article-title>. In <person-group person-group-type="editor">
<name><surname>Ames</surname><given-names>C.</given-names></name>
<name><surname>Berliner</surname><given-names>D.</given-names></name>
<name><surname>Brophy</surname><given-names>J.</given-names></name>
<name><surname>Corno</surname><given-names>L.</given-names></name>
<name><surname>McCaslin</surname><given-names>M.</given-names></name>
</person-group> (Eds.), <source>21st century education: A reference handbook</source> (pp. <fpage>259</fpage>–<lpage>266</lpage>). <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr37-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
<name><surname>Cheung</surname><given-names>A.</given-names></name>
<name><surname>Groff</surname><given-names>C.</given-names></name>
<name><surname>Lake</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Effective reading programs for middle and high schools: A best evidence synthesis</article-title>. <source>Reading Research Quarterly</source>, <volume>43</volume>(<issue>3</issue>), <fpage>290</fpage>–<lpage>322</lpage>.</citation>
</ref>
<ref id="bibr38-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
<name><surname>Lake</surname><given-names>C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Effective programs in elementary math: A best evidence synthesis</article-title>. <source>Review of Educational Research</source>, <volume>78</volume>(<issue>3</issue>), <fpage>427</fpage>–<lpage>515</lpage>.</citation>
</ref>
<ref id="bibr39-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
<name><surname>Lake</surname><given-names>C.</given-names></name>
<name><surname>Chambers</surname><given-names>B.</given-names></name>
<name><surname>Cheung</surname><given-names>A.</given-names></name>
<name><surname>Davis</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Effective reading programs for the elementary grades: A best evidence synthesis</article-title>. <source>Review of Educational Research</source>, <volume>79</volume>(<issue>4</issue>), <fpage>1391</fpage>–<lpage>1465</lpage>.</citation>
</ref>
<ref id="bibr40-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
<name><surname>Lake</surname><given-names>C.</given-names></name>
<name><surname>Groff</surname><given-names>C.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Effective programs in middle and high school mathematics: A best evidence synthesis</article-title>. <source>Review of Educational Research</source>, <volume>79</volume>(<issue>2</issue>), <fpage>839</fpage>–<lpage>911</lpage>.</citation>
</ref>
<ref id="bibr41-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
<name><surname>Madden</surname><given-names>N. A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Measures inherent to treatments in program effectiveness reviews</article-title>. <source>Journal of Research on Educational Effectiveness</source>, <volume>4</volume>(<issue>4</issue>), <fpage>370</fpage>–<lpage>380</lpage>.</citation>
</ref>
<ref id="bibr42-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Slavin</surname><given-names>R. E.</given-names></name>
<name><surname>Madden</surname><given-names>N. A.</given-names></name>
<name><surname>Chambers</surname><given-names>B.</given-names></name>
<name><surname>Haxby</surname><given-names>B.</given-names></name>
</person-group> (<year>2009</year>). <source>Two million children: Success for All</source>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>Corwin</publisher-name>.</citation>
</ref>
<ref id="bibr43-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Snipes</surname><given-names>J.</given-names></name>
<name><surname>Doolittle</surname><given-names>F.</given-names></name>
<name><surname>Herlihy</surname><given-names>C.</given-names></name>
</person-group> (<year>2002</year>). <source>Foundations for success: Case studies of how urban school systems improve student achievement</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Council of the Great City Schools</publisher-name>.</citation>
</ref>
<ref id="bibr44-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Spoth</surname><given-names>R. L.</given-names></name>
<name><surname>Redmond</surname><given-names>C.</given-names></name>
<name><surname>Shin</surname><given-names>C.</given-names></name>
<name><surname>Greenberg</surname><given-names>M.</given-names></name>
<name><surname>Clair</surname><given-names>S.</given-names></name>
<name><surname>Feinberg</surname><given-names>M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Substance abuse outcomes at eighteen months past baseline from the PROSPER community-university partnership trial</article-title>. <source>American Journal of Preventive Medicine</source>, <volume>32</volume>(<issue>5</issue>), <fpage>395</fpage>–<lpage>402</lpage>.</citation>
</ref>
<ref id="bibr45-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Streifer</surname><given-names>P.</given-names></name>
</person-group> (<year>2002</year>). <source>Using data to make better educational decisions</source>. <publisher-loc>Lanham, MD</publisher-loc>: <publisher-name>Scarecrow Press</publisher-name>.</citation>
</ref>
<ref id="bibr46-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Supovitz</surname><given-names>J. A.</given-names></name>
<name><surname>Poglinco</surname><given-names>S. M.</given-names></name>
<name><surname>Snyder</surname><given-names>B. A.</given-names></name>
</person-group> (<year>2001</year>). <source>Moving mountains: Successes and challenges of the America’s Choice comprehensive school reform design</source>. <publisher-loc>Philadelphia, PA</publisher-loc>: <publisher-name>University of Pennsylvania, Consortium for Policy Research in Education</publisher-name>.</citation>
</ref>
<ref id="bibr47-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Symonds</surname><given-names>K. W.</given-names></name>
</person-group> (<year>2003</year>). <source>After the test: How schools are using data to close the achievement gap</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Bay Area School Reform Collaborative</publisher-name>.</citation>
</ref>
<ref id="bibr48-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wayman</surname><given-names>J. C.</given-names></name>
<name><surname>Cho</surname><given-names>V.</given-names></name>
<name><surname>Johnston</surname><given-names>M. T.</given-names></name>
</person-group> (<year>2007</year>). <source>The data-informed district: A district-wide evaluation of data use in the Natrona County School District</source>. <publisher-loc>Austin, TX</publisher-loc>: <publisher-name>The University of Texas</publisher-name>.</citation>
</ref>
<ref id="bibr49-0002831212466909">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Wayman</surname><given-names>J. C.</given-names></name>
<name><surname>Cho</surname><given-names>V.</given-names></name>
<name><surname>Shaw</surname><given-names>S. M.</given-names></name>
</person-group> (<year>2009</year>). <source>First-year results from an efficacy study of the Acuity data system</source>. <publisher-loc>Austin, TX</publisher-loc>: <publisher-name>The University of Texas</publisher-name>.</citation>
</ref>
<ref id="bibr50-0002831212466909">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Wayman</surname><given-names>J. C.</given-names></name>
<name><surname>Jimerson</surname><given-names>J. B.</given-names></name>
<name><surname>Cho</surname><given-names>V.</given-names></name>
</person-group> (<year>2010</year>, <month>October</month>). <source>District policies for the effective use of student data</source>. <conf-name>Paper presented at the 2010 Annual Convention of the University Council for Educational Administration</conf-name>, <conf-loc>New Orleans LA</conf-loc>.</citation>
</ref>
<ref id="bibr51-0002831212466909">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wayman</surname><given-names>J. C.</given-names></name>
<name><surname>Stringfield</surname><given-names>S.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Technology-supported involvement of entire faculties in examination of student data for instructional improvement</article-title>. <source>American Journal of Education</source> <volume>112</volume>(<issue>4</issue>), <fpage>549</fpage>–<lpage>571</lpage>.</citation>
</ref>
<ref id="bibr52-0002831212466909">
<citation citation-type="gov">
<collab>What Works Clearinghouse</collab>. (<year>2011a</year>). <source>Beginning reading topic report</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/wwc/Topic.aspx?sid=8">http://ies.ed.gov/ncee/wwc/Topic.aspx?sid=8</ext-link>
</citation>
</ref>
<ref id="bibr53-0002831212466909">
<citation citation-type="gov">
<collab>What Works Clearinghouse</collab>. (<year>2011b</year>). <source>Elementary mathematics topic report</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/wwc/Topic.aspx?sid=9">http://ies.ed.gov/ncee/wwc/Topic.aspx?sid=9</ext-link>
</citation>
</ref>
<ref id="bibr54-0002831212466909">
<citation citation-type="gov">
<collab>What Works Clearinghouse</collab> (<year>2011c</year>). <article-title>Middle school mathematics topic report</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://ies.ed.gov/ncee/wwc/Topic.aspx?sid=9">http://ies.ed.gov/ncee/wwc/Topic.aspx?sid=9</ext-link>
</citation>
</ref>
</ref-list>
</back>
</article>