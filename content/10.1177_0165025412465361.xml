<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JBD</journal-id>
<journal-id journal-id-type="hwp">spjbd</journal-id>
<journal-title>International Journal of Behavioral Development</journal-title>
<issn pub-type="ppub">0165-0254</issn>
<issn pub-type="epub">1464-0651</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0165025412465361</article-id>
<article-id pub-id-type="publisher-id">10.1177_0165025412465361</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>The foundations of social cognition: Studies on face/voice integration in newborn infants</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Streri</surname>
<given-names>Arlette</given-names>
</name>
<xref ref-type="aff" rid="aff1-0165025412465361">1</xref>
<xref ref-type="aff" rid="aff2-0165025412465361">2</xref>
<xref ref-type="corresp" rid="corresp1-0165025412465361"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Coulon</surname>
<given-names>Marion</given-names>
</name>
<xref ref-type="aff" rid="aff3-0165025412465361">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Guellaï</surname>
<given-names>Bahia</given-names>
</name>
<xref ref-type="aff" rid="aff1-0165025412465361">1</xref>
<xref ref-type="aff" rid="aff2-0165025412465361">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0165025412465361">
<label>1</label>Paris Descartes University, Sorbonne Cité, France</aff>
<aff id="aff2-0165025412465361">
<label>2</label>Laboratory for Psychology of Perception, Centre Biomédical des Saints-Pères, France</aff>
<aff id="aff3-0165025412465361">
<label>3</label>EthoS, Animal and Human Ethology, CNRS, Rennes 1 University, France</aff>
<author-notes>
<corresp id="corresp1-0165025412465361">Arlette Streri, Laboratory for Psychology of Perception, UMR CNRS 8158, Centre Biomédical des Saints-Pères, Paris, France. Email: <email>arlette.streri@parisdescartes.fr</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>3</month>
<year>2013</year>
</pub-date>
<volume>37</volume>
<issue>2</issue>
<issue-title>Special issue: Development of Face Processing: New Evidence on Multi-Modal Contributions, Scanning, and Recognition</issue-title>
<fpage>79</fpage>
<lpage>83</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">International Society for the Study of Behavioural Development</copyright-holder>
</permissions>
<abstract>
<p>A series of studies on newborns’ abilities for recognizing speaking faces has been performed in order to identify the fundamental cues of social cognition. We used audiovisual dynamic faces rather than photographs or patterns of faces. Direct eye gaze and speech addressed to newborns, in interactive situations, appear to be two good candidates for this function. Both enable a correct identification and recognition of an unfamiliar person by the infants. Moreover, a few hours after birth, newborns are also capable of imitating a model (Mouth Opening or Lip Spreading) more quickly if it is associated with the pronunciation of a vowel, /a/ or /i/. Newborns are very sensitive to natural speech compared to filtered speech. These findings reveal the existence of an innate predisposition to social interactions in newborns.</p>
</abstract>
<kwd-group>
<kwd>face recognition</kwd>
<kwd>imitation</kwd>
<kwd>newborns</kwd>
<kwd>social cues</kwd>
<kwd>speech</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0165025412465361">
<title>Introduction</title>
<p>Our faces and voices identify us as human beings. For a long time, faces were studied as visual objects and the voice was restricted to the domain of audition. As a consequence, the two domains were considered separately. How do young infants, from birth, recognize the numerous faces they encounter? How do they learn to produce speech and interact verbally with others? The recognition of self–other equivalences is the foundation of social cognition. Imitation, defined as occurring when individuals are able to reproduce an observed action, or a heard action in the case of speech, is nowadays viewed as playing a fundamental role in this foundation. Imitation is both a cognitive behaviour, as it facilitates learning, and a multimodal behaviour, since it involves different sensory modalities (proprioception, vision, audition, etc.). Moreover, imitation is also a behaviour in which action and perception are intertwined and can influence each other. Although a number of studies have investigated the existence of facial imitation in newborns (<xref ref-type="bibr" rid="bibr24-0165025412465361">Meltzoff &amp; Moore, 1983</xref>; <xref ref-type="bibr" rid="bibr34-0165025412465361">Reissland, 1988</xref>; <xref ref-type="bibr" rid="bibr37-0165025412465361">Vinter, 1986</xref>), few studies have linked face recognition and speech learning via imitation process in very young infants.</p>
<p>Faces and voices are important signals for social cognition, with voices adding additional speech. Imitation of speech by newborns first requires the visual processing of human faces and more precisely the processing of lip movements when someone is speaking. During interactions with their mother, newborns are provided with tactile stimulation, warmth, and exposure to the mother’s voice, odours, taste of milk, etc. Recent studies have provided support for the role of the mother’s voice in the facilitation of recognition of the mother’s face (<xref ref-type="bibr" rid="bibr35-0165025412465361">Sai, 2005</xref>). Maternal verbal interactions with her newborn, while she looks at her baby, would be more efficient in the memorization of the mother’s face than the olfactory and tactile contacts. However, many aspects of multimodal interactions remain to be explored. What are the face cues that are fundamental in this recognition and consequently in the mother–baby or adult–baby interactions? What is the role of the mouth in vision and more precisely of the lips’ movements in imitation of speech by newborns? We ran a series of studies to answer these questions. The main purposes of our studies were: 1) to determine the conditions in which a speaking face is visually processed and memorized by newborns, and 2) to trigger or evoke lip movements in newborns as imitative responses to lip movements of a model. The results may provide a better understanding of the role of imitation in social interaction, and shed light on the conditions of the motor aspect of perception/production of language from birth.</p>
</sec>
<sec id="section2-0165025412465361">
<title>Experimental paradigm</title>
<p>To investigate these questions, we used audiovisual dynamic faces rather than photographs or pattern of faces used in other studies on face perception (<xref ref-type="bibr" rid="bibr30-0165025412465361">Pascalis &amp; de Schonen, 1994</xref>; <xref ref-type="bibr" rid="bibr31-0165025412465361">Pascalis, de Schonen, Morton, Deruelle, &amp; Fabre-Grenet, 1995</xref>). Videos of speaking faces appear as interesting stimuli to newborns. During a familiarization phase, newborns were presented with a dynamic unfamiliar woman’s face speaking to them. In the test phase, newborns were presented twice with photographs of both the familiar and a novel face in alternation. A difference in looking time towards the two photographs was taken as evidence for the recognition of the speaking woman’s face seen during familiarization.</p>
</sec>
<sec id="section3-0165025412465361">
<title>Social cues in the faces</title>
<sec id="section4-0165025412465361">
<title>Speech as a marker to identify the other</title>
<p>The first experiment (<xref ref-type="bibr" rid="bibr7-0165025412465361">Coulon, Guellai, &amp; Streri, 2011</xref>) showed that faces seen on video were very salient stimuli for newborns. The infants paid attention to the displays for a long time. The most important finding was that newborns recognized the woman’s face who was speaking in the familiarization phase when compared with a new face in the test phase: Newborns looked longer at the familiar than at the novel face. In the second experiment, the sound was removed during familiarization and newborns saw a woman who moved her lips without speech sounds. Under these conditions, there were no differences in looking times during the test phase, thus revealing that newborns did not recognize the familiar face. Taken together, the results of both experiments reveal that recognition of unfamiliar speaking faces by newborns is observed only in the condition where lip movements and speech sounds are presented simultaneously. Lip movements without speech sounds are inefficient for correct face recognition by the newborns. However, it is plausible that newborns are only sensitive to the speech sounds and that auditory stimulation enhances their attention to the faces. To test whether newborns were sensitive to the speaking face per se, or to the association between a moving face and a speech soundtrack, a further condition was presented to the newborns (<xref ref-type="bibr" rid="bibr14-0165025412465361">Guellaï, Coulon, &amp; Streri, 2011</xref>). The infants watched a video of a silent moving face with closed lips and a speech soundtrack was added to this video. In this condition, no discrimination between the familiar and the new faces was observed at test. In sum, lip movements without speech sounds or a silent moving face with added speech do not allow newborns to encode and recognize faces.</p>
<p>Overall, these findings support those of <xref ref-type="bibr" rid="bibr35-0165025412465361">Sai (2005</xref>) with the mother’s face and extend them to unfamiliar faces. Soon after birth, newborns recognize and show a preference for someone who previously interacted with them verbally. Newborns encounter many unfamiliar faces. Thus, our findings suggest that the speech component may act as a marker to identify them. These results are congruent with the idea that newborns already integrate multimodal information and are sensitive to redundant audiovisual speech cues (<xref ref-type="bibr" rid="bibr2-0165025412465361">Bahrick, 1988</xref>; <xref ref-type="bibr" rid="bibr21-0165025412465361">Lewkowicz, 2000</xref>).</p>
</sec>
<sec id="section5-0165025412465361">
<title>Two social cues: Speech and eye gaze?</title>
<p>Eye gaze is an important source of information in social interaction in human beings. The social functions of human eye gaze are diverse, including following someone’s gaze to a significant object, gathering feedback on the others’ reactions, and regulating turn-taking in conversation and inferring mental states. Direct gaze constitutes a positive signal engaging its target in a social interaction, and modulates performance in face recognition both at the encoding and retrieval levels, with better performance when facing someone with direct gaze, both in adults and children (<xref ref-type="bibr" rid="bibr17-0165025412465361">Hood, Macrae, Cole-Davies, &amp; Dias, 2003</xref>) as well as in infants (<xref ref-type="bibr" rid="bibr11-0165025412465361">Farroni, Massaccesi, Menon, &amp; Johnson, 2007</xref>).</p>
<p>Newborns are already sensitive to the gaze of others and prefer looking at the photograph of a face with open versus closed eyes (<xref ref-type="bibr" rid="bibr4-0165025412465361">Batki, Baron-Cohen, Wheelwright, Connellan, &amp; Ahluwalia, 2000</xref>). They also prefer looking at a photograph of a face with direct rather than averted gaze (<xref ref-type="bibr" rid="bibr10-0165025412465361">Farroni, Csibra, Simion, &amp; Johnson, 2002</xref>). However, these experiments used photographs of faces. The role of gaze in face recognition at birth, using interactive situations, had not been tested. Nonetheless, it seems that in interactive situations, direct gaze alone (without verbal interaction) is not a sufficient cue in guiding newborns’ identification of previously unfamiliar faces. This finding raises a critical question: Do speech and direct gaze need to appear together for recognition of unfamiliar faces by newborns, or is speech the only effective social cue for newborns tested in social situations? <xref ref-type="bibr" rid="bibr16-0165025412465361">Guellaï and Streri (2011</xref>) investigated this question by testing the role of eye gaze in identification of speaking faces by newborns. In the familiarization phase, newborns saw an unfamiliar woman addressing them either with direct or averted gaze. In the test phase, newborns recognized the familiar face if the woman talked to them with direct gaze, but failed in the case of a speaking face with averted gaze. In short, our findings suggest that newborns are able to identify others with two socially meaningful cues perceived together in a dynamic interactive face: speech and direct gaze.</p>
</sec>
<sec id="section6-0165025412465361">
<title>Face–voice integration and the motor aspects of speech</title>
<p>Our previous studies suggest that newborns focus their attention on both the eyes and the mouth when placed in front of the video of a speaking face. Eyes and mouth can be considered as the two main cues of social interaction at birth, and in fact they will remain predominant until 26 weeks of age (<xref ref-type="bibr" rid="bibr18-0165025412465361">Hunnius &amp; Geuze, 2004</xref>). However, this does not mean that their functions are identical. In fact, it is plausible that during the course of development, their functions will become independent from each other. For example, in some situations one can answer a question or speak to a person without necessarily looking at her/him, even though it could be considered impolite. Similarly, we can follow the eye gaze of another person and understand his or her intention without verbal information. Thus, the person’s eye gaze would eventually be more devoted to some social functions, such as understanding what is on the other’s mind and reacting in accordance with it, sharing a mutual gaze towards an object, etc. The talker’s mouth would be relied on more for other social functions, such as learning speech or interacting verbally and politely with others. The divergence of function might appear at different ages according to the abilities and needs of infants. This idea was proposed on the basis of Lewkowicz and Hansen-Tift’s (<xref ref-type="bibr" rid="bibr22-0165025412465361">2012</xref>) findings. Infants watched and listened to a female speaking to them either in the native or non-native language. The infant’s attention, defined by visual scanning of the face, was measured with an eye-tracker. The results revealed that infants shifted attention from eyes to mouth between 4 and 8 months of age to gain access to redundant audiovisual speech cues that allowed them to learn their native language. Moreover, at 12 months of age, infants shifted back to the eyes when the face talked to them in the native language. Thus, by the end of their first year, speech cues appear to be less necessary for infants.</p>
<p>This research did not study the mechanisms of speech learning at birth, however, and a number of studies have revealed that newborns focus their attention especially towards the mouth in the situation of facial imitation, especially vocal imitation or when they see a face that talks to them. Infants’ attention to the mouth and especially to lip movements is particularly fascinating. When facing speaking faces, young infants detect correspondences between speech sounds and associated mouth articulations. By using a preferential looking procedure, <xref ref-type="bibr" rid="bibr20-0165025412465361">Kuhl and Meltzoff (1984</xref>) first presented 4.5-month-old infants with two filmed images side-by-side of a woman articulating the vowels /i/ and /a/. These two faces were accompanied by only one speech sound ([a] or [i]) that was diffused in synchrony with the mouth movements of the two women’s faces. The results revealed that the infants looked significantly longer at the face that matched the heard speech sound than at the mismatching face. With the same paradigm, Patterson and Werker replicated these results in 4.5-month-olds and extended them to 2-month-olds (<xref ref-type="bibr" rid="bibr32-0165025412465361">1999</xref>, <xref ref-type="bibr" rid="bibr33-0165025412465361">2003</xref>). The authors who conducted these studies asked the same questions: How do young infants know which facial speech gestures match which acoustic vowel? Does this capacity require some visual and auditory experience or learning, or are auditory-visual correspondences of speech already present at birth?</p>
</sec>
<sec id="section7-0165025412465361">
<title>Seeing and hearing vowels and facial neonatal imitation</title>
<p>Research supports the idea of an innate origin of the relationship between the motor patterns of the mouth and their auditory outputs. <xref ref-type="bibr" rid="bibr1-0165025412465361">Aldridge, Braga, Walton, and Bower (1999</xref>) used an operant sucking procedure to present newborns with matching or mismatching audiovisual presentations of a face articulating native and non-native vowels. The results revealed that newborns significantly preferred the matched over mismatched presentations. Thus, newborns know which facial gesture matched the corresponding acoustic vowels. This result suggests that they are able to detect common temporal or amodal properties across sensory modalities (<xref ref-type="bibr" rid="bibr3-0165025412465361">Bahrick, Lickliter, &amp; Flom, 2004</xref>), which is the basis of many types of audiovisual correspondences. However, <xref ref-type="bibr" rid="bibr20-0165025412465361">Kuhl and Meltzoff (1984</xref>) altered the speech sounds of their stimuli by removing the spectral information necessary to identify the vowels, while preserving the temporal characteristics such as amplitude and duration. Their results revealed that infants failed to associate this altered speech sound with an articulating face, consequently ruling out the explanation of a simple association based on audio-visual synchrony.</p>
<p>Classically, studies that have investigated neonatal facial imitation used silent models. To our knowledge, only two studies previously used “talking” models in newborns’ imitation tasks (<xref ref-type="bibr" rid="bibr6-0165025412465361">Chen, Striano, &amp; Rakoczy, 2004</xref>; <xref ref-type="bibr" rid="bibr19-0165025412465361">Kugiumutzakis, 1999</xref>). All these studies found that silent or talking models efficiently elicited newborns’ facial imitations. To date, no study has compared newborns’ productive responses according to different modality conditions of presentation. <xref ref-type="bibr" rid="bibr8-0165025412465361">Coulon, Hemimou, and Streri (2012</xref>) compared the imitative responses of newborn infants (aged 43 hours on average) when facing visual-only, audiovisual congruent, or audiovisual incongruent models. In the congruent audiovisual condition, newborns were presented with a model producing /i/ (as in “lit” in French) and /a/ vowels (as in “chat” in French). In the visual condition, newborns saw the model opening the mouth or spreading her lips. For the audiovisual incongruent condition, the videos were dubbed: The model’s mouth openings were accompanied by the speech sound /i/ and the model’s lip spreadings were accompanied by the speech sound /a/ (the mouth movements and speech sounds were temporally synchronized in terms of onsets/offsets).</p>
<p>We were particularly interested in newborns’ productions in terms of occurrences and reaction times, and expected that these responses would be modulated according to the modality condition of the model’s presentation. If positive results were found, these would support the hypothesis of an existing link between perception and production at birth. Videos were used because they had been effective in our previous studies.</p>
<p>This study revealed several important findings (see <xref ref-type="fig" rid="fig1-0165025412465361">Figure 1</xref>): Under some conditions, 2D models presented on video elicited real facial imitations in newborns. This study thus replicates previous results found with live models and extends these findings to a new experimental setting. The results also showed for the first time neonatal imitation of lip spreading. The imitation of this new gesture had been predicted but never tested previously (<xref ref-type="bibr" rid="bibr25-0165025412465361">Meltzoff &amp; Moore, 1997</xref>). This study therefore extends the range of observed imitated facial gestures by newborn infants. Moreover, Mouth Opening (MO) was higher than Lip Spreading (LS), irrespective of the modality condition of the model’s presentation (visual-only or audiovisual congruent). This result may also be explained by the newborns’ motor capacities: For a neonate, performing LS would be more difficult than performing MO. Indeed, MO simply requires lowering of the jaw, whereas LS requires more muscular control. The /a/ is the first vowel to be pronounced by 2- to 3-month-olds. The /i/ vowel, more difficult to perform since it involves both a broadening of the lips and tongue action, appears at around 8 months of age (<xref ref-type="bibr" rid="bibr28-0165025412465361">Oller, 1980</xref>). Our results also provided evidence that neonatal imitation could be modulated according to the modality condition of the model’s presentation: Imitations appeared significantly more quickly when the model was audiovisual congruent rather than visual-only, even though no significant difference in terms of response frequencies was found. This finding thus highlights the role of sound in facial imitation behaviours. Finally, we observed that a mismatch between auditory stimuli and mouth movements (that is, audiovisual incongruent condition) inhibited newborns’ productions of motor matching. In conclusion, our findings confirm that the link between perceptual and motor systems is present at birth and does not result from associative learning.</p>
<fig id="fig1-0165025412465361" position="float">
<label>Figure 1.</label>
<caption>
<p>Mean frequencies of the MO and the LS in the baseline, the /a/ block, and the /i/ block, in the visual-only, the audiovisual congruent, and the audiovisual incongruent conditions.</p>
</caption>
<graphic xlink:href="10.1177_0165025412465361-fig1.tif"/>
</fig>
</sec>
<sec id="section8-0165025412465361">
<title>The effects of prosody in the case of speaking faces</title>
<p>When addressing infants, adults use speech that is called Infant Directed Speech (IDS), or motherese. This particular speech possesses more exaggerated vowels, higher pitch, and slower tempo than Adult Directed Speech (ADS) (<xref ref-type="bibr" rid="bibr5-0165025412465361">Burnham, Kitamura, &amp; Vollmer-Conna, 2002</xref>; <xref ref-type="bibr" rid="bibr12-0165025412465361">Fernald &amp; Simon, 1984</xref>; <xref ref-type="bibr" rid="bibr29-0165025412465361">Panneton-Cooper &amp; Aslin, 1990</xref>). In other words, the prosody of IDS, which corresponds to the melodic contours of the speech, is more exaggerated than ADS. Newborns are already able to discriminate between inter- and intra-language differences thanks to prosodic cues (Mehler, Bertoncini, Barrière, &amp; Jassik-Gerschenfeld, 1988; <xref ref-type="bibr" rid="bibr27-0165025412465361">Nazzi, Floccia, &amp; Bertoncini, 1998</xref>). They also prefer listening to their native language rather than to non-native languages (<xref ref-type="bibr" rid="bibr26-0165025412465361">Moon, Panneton-Cooper, &amp; Fifer, 1993</xref>), and to their mother’s voice rather than to a stranger’s voice (<xref ref-type="bibr" rid="bibr9-0165025412465361">De Casper &amp; Fifer, 1980</xref>) even when it is low-pass filtered, that is, when the semantic information has been removed (<xref ref-type="bibr" rid="bibr36-0165025412465361">Spence &amp; Freeman, 1996</xref>). Such sensitivity can be linked to the fact that by the last trimester of gestation, human foetuses are able to memorize arbitrary stimuli from the external world with a particular sensitivity to melodic contours in both music and speech (<xref ref-type="bibr" rid="bibr13-0165025412465361">Granier-Deferre, Ribeiro, Jacquet, &amp; Bassereau, 2011</xref>). Thus, the foetus has good access to low-frequency information, especially the prosodic qualities of speech, such as rhythm, stress and intonation. We asked whether newborns process speaking faces in which only prosodic cues remained, similar to speaking faces in which all the linguistic and paralinguistic cues are present (<xref ref-type="bibr" rid="bibr15-0165025412465361">Guellaï, Mersad, &amp; Streri, 2012</xref>). To answer this question, we adopted the same procedure as in Coulon et al.’s (2011) experiments. Newborns were familiarized for 90 seconds with a video of a face speaking to the newborn in French (i.e., “normal speech” condition) or a video in which the speech stream had been low-pass filtered with a 400 Hz cut-off (i.e., “filtered speech” condition). Then, newborns were tested with photographs of a familiar and a new face. Analyses of mean looking times in the test phase revealed striking results. In the “French filtered speech” condition, a preference for the new face (not seen during familiarization) was found. This result was opposed to that observed in the “French normal speech condition”. Moreover, the same pattern of results was observed when the face was presented speaking in English, which presents different rhythmic characteristics relative to French (see <xref ref-type="fig" rid="fig2-0165025412465361">Figure 2</xref>). As a consequence, a face speaking a filtered language has not the same status as a face speaking a normal language. The sonority, a property of speech sounds which indicates the difference of pressure between the interior and exterior of the mouth when pronouncing speech, is toned down in filtered language. In our experiment, this missing cue provoked a mismatch between the visuo-motor aspects and auditory aspect of speech and was perceived by newborns.</p>
<fig id="fig2-0165025412465361" position="float">
<label>Figure 2.</label>
<caption>
<p>Mean looking times (in seconds) to the familiar and new faces at the test phase after familiarization with faces speaking a French or English normal or filtered speech.</p>
<p><italic>Note.</italic> * <italic>p</italic> &lt; .05; *** <italic>p</italic> &lt; .001.</p>
</caption>
<graphic xlink:href="10.1177_0165025412465361-fig2.tif"/>
</fig>
<p>These results, congruent with those of <xref ref-type="bibr" rid="bibr8-0165025412465361">Coulon, Hemimou, and Streri (2012</xref>), support the hypothesis that newborns are already sensitive to the congruence between certain properties of visual speech such as the articulatory movements and speech stream with all its semantic and prosodic components. In other words, the explanation of a simple association based on audio-visual synchrony at birth is not sufficient in the case of interactive face-processing.</p>
</sec>
</sec>
<sec id="section9-0165025412465361">
<title>Conclusions</title>
<p>To our knowledge, this is the first time that auditory-visual videos have been presented to newborns in an identity-recognition task with an adult person different from the mother. These conditions appear to be similar to interactive situations for newborns. This display allowed us to reveal several results that shed light on newborns’ preferences. It was astounding to notice that newborns, a few hours after birth, are fascinated by faces speaking to them and looking at them. They recognize and prefer them to another face. Without speech or direct gaze, faces were not preferred or recognized. Hence, direct gaze and speech are two fundamental cues for newborns, even though several months later this link will become weaker. It is also important to note that the visuo-auditory incongruent situations in which the faces did not speak “naturally” to newborns disrupted the interaction, that is, either the preference for or recognition of this face. In particular, it was surprising to learn that after having only listened for several months to a filtered or even degraded speech during its foetal period, the newborn is very sensitive to this kind of speech when pronounced by a face, and does not prefer it. From imitation conditions and the recognition of self–other equivalences, newborns bring innate predispositions to social interactions with others.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The studies obtained the authorization of the Graduate School of Paris Descartes as well as written consent of the nursing staff of Hôpital Bichat and the parents of the newborns. We thank Julien Blais for custom programming, and Lola De Hevia for her comments. We also thank Pr. Luton, L. de Lorgeril, and the nursing staff at Bichat Maternity in Paris.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure" id="fn1-0165025412465361"><label>Funding</label>
<p>This research was supported by grants from the “Institut Universitaire de France”, the “Agence Nationale de la Recherche”.</p></fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Aldridge</surname>
<given-names>M. A</given-names>
</name>
<name>
<surname>Braga</surname>
<given-names>E. S.</given-names>
</name>
<name>
<surname>Walton</surname>
<given-names>G. E.</given-names>
</name>
<name>
<surname>Bower</surname>
<given-names>T. G. R.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>The intermodal representation of speech in newborns</article-title>. <source>Developmental Science</source>, <volume>2</volume>, <fpage>42</fpage>–<lpage>46</lpage>.</citation>
</ref>
<ref id="bibr2-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bahrick</surname>
<given-names>L. E.</given-names>
</name>
</person-group> (<year>1988</year>). <article-title>Intermodal learning in infancy: Learning on the basis of two kinds of invariant relations in audible and visible events</article-title>. <source>Child Development</source>, <volume>59</volume>, <fpage>197</fpage>–<lpage>209</lpage>.</citation>
</ref>
<ref id="bibr3-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bahrick</surname>
<given-names>L. E.</given-names>
</name>
<name>
<surname>Lickliter</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Flom</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Intersensory redundancy guides the development of selective attention, perception, and cognition in infancy</article-title>. <source>Current Directions in Psychological Science</source>, <volume>13</volume>, <fpage>99</fpage>–<lpage>102</lpage>.</citation>
</ref>
<ref id="bibr4-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Batki</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Baron-Cohen</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Wheelwright</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Connellan</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Ahluwalia</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Is there an innate gaze module? Evidence from human neonates</article-title>. <source>Infant Behavior and Development</source>, <volume>23</volume>, <fpage>223</fpage>–<lpage>229</lpage>.</citation>
</ref>
<ref id="bibr5-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Burnham</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Kitamura</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Vollmer-Conna</surname>
<given-names>U.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>What’s new pussycat? On talking to babies and animals</article-title>. <source>Science</source>, <volume>296</volume>, <fpage>1435</fpage>.</citation>
</ref>
<ref id="bibr6-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>X.</given-names>
</name>
<name>
<surname>Striano</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Rakoczy</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Auditory–oral matching behavior in newborns</article-title>. <source>Developmental Science</source>, <volume>7</volume>, <fpage>42</fpage>–<lpage>47</lpage>.</citation>
</ref>
<ref id="bibr7-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Coulon</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Guellai</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Streri</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Recognition of unfamiliar talking faces at birth</article-title>. <source>International Journal of Behavioral Development</source>, <volume>35</volume>, <fpage>282</fpage>–<lpage>287</lpage>.</citation>
</ref>
<ref id="bibr8-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Coulon</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Hemimou</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Streri</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Effects of seeing and hearing vowels on neonatal facial imitation</article-title>. <source>Infancy</source>. <comment>Advance online publication. doi: 10.1111/infa.12001</comment>
</citation>
</ref>
<ref id="bibr9-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>De Casper</surname>
<given-names>A. J.</given-names>
</name>
<name>
<surname>Fifer</surname>
<given-names>W. P.</given-names>
</name>
</person-group> (<year>1980</year>). <article-title>Of human bonding: Newborns prefer their mother’s voice</article-title>. <source>Science</source>, <volume>208</volume>, <fpage>1174</fpage>–<lpage>1176</lpage>.</citation>
</ref>
<ref id="bibr10-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Farroni</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Csibra</surname>
<given-names>G.</given-names>
</name>
<name>
<surname>Simion</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Johnson</surname>
<given-names>M. H.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Eye contact detection in humans from birth</article-title>. <source>PNAS</source>, <volume>99</volume>, <fpage>9602</fpage>–<lpage>9605</lpage>.</citation>
</ref>
<ref id="bibr11-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Farroni</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Massaccesi</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Menon</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Johnson</surname>
<given-names>M. H.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Direct gaze modulates face recognition in young infants</article-title>. <source>Cognition</source>, <volume>102</volume>, <fpage>396</fpage>–<lpage>404</lpage>.</citation>
</ref>
<ref id="bibr12-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fernald</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Simon</surname>
<given-names>T.</given-names>
</name>
</person-group> (<year>1984</year>). <article-title>Expanded intonation contours in mothers’ speech to newborns</article-title>. <source>Developmental Psychology</source>, <volume>20</volume>, <fpage>104</fpage>–<lpage>113</lpage>.</citation>
</ref>
<ref id="bibr13-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Granier-Deferre</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Ribeiro</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Jacquet</surname>
<given-names>A.-Y.</given-names>
</name>
<name>
<surname>Bassereau</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Near-term fetuses process temporal features of speech</article-title>. <source>Developmental Science</source>, <volume>14</volume>, <fpage>336</fpage>–<lpage>352</lpage>.</citation>
</ref>
<ref id="bibr14-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Guellaï</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Coulon</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Streri</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>The role of motion and speech in face recognition at birth</article-title>. <source>Visual Cognition</source>, <volume>19</volume>, <fpage>1212</fpage>–<lpage>1233</lpage>.</citation>
</ref>
<ref id="bibr15-0165025412465361">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Guellaï</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Mersad</surname>
<given-names>K.</given-names>
</name>
<name>
<surname>Streri</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2012</year>). <source>The role of prosody in recognition of talking faces at birth</source>. <comment>Manuscript submitted for publication</comment>.</citation>
</ref>
<ref id="bibr16-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Guellaï</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Streri</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Cues for early social skills: Direct gaze modulates newborns’ recognition of talking faces</article-title>, <source>PLoS One</source>, <fpage>e18610</fpage>. <comment>doi: 10.1371/journal.pone.0018610</comment>.</citation>
</ref>
<ref id="bibr17-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hood</surname>
<given-names>B. M.</given-names>
</name>
<name>
<surname>Macrae</surname>
<given-names>C. N.</given-names>
</name>
<name>
<surname>Cole-Davies</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Dias</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Eye remember you: The effects of gaze direction on face recognition in children and adults</article-title>. <source>Developmental Science</source>, <volume>6</volume>, <fpage>67</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr18-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hunnius</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Geuze</surname>
<given-names>R. H.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Developmental changes in visual scanning of dynamic faces and abstract stimuli in infants: A longitudinal study</article-title>. <source>Infancy</source>, <volume>6</volume>, <fpage>231</fpage>–<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr19-0165025412465361">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kugiumutzakis</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Genesis and development of early infant mimesis to facial and vocal models</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Nadel</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Butterworth</surname>
<given-names>G.</given-names>
</name>
</person-group> (Eds.), <source>Imitation in infancy</source> (pp. <fpage>36</fpage>–<lpage>59</lpage>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kuhl</surname>
<given-names>P. K.</given-names>
</name>
<name>
<surname>Meltzoff</surname>
<given-names>A. N.</given-names>
</name>
</person-group> (<year>1984</year>). <article-title>The intermodal representation of speech in infants</article-title>. <source>Infant Behavior and Development</source>, <volume>7</volume>, <fpage>361</fpage>–<lpage>381</lpage>.</citation>
</ref>
<ref id="bibr21-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lewkowicz</surname>
<given-names>D.J.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Infants’ perception of the audible, visible, and bimodal attributes of multimodal syllables</article-title>. <source>Child Development</source>, <volume>71</volume>, <fpage>1241</fpage>–<lpage>1257</lpage>.</citation>
</ref>
<ref id="bibr22-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lewkowicz</surname>
<given-names>D. J.</given-names>
</name>
<name>
<surname>Hansen-Tift</surname>
<given-names>A. M.</given-names>
</name>
</person-group> (<year>2012</year>). <article-title>Infants deploy selective attention to the mouth of a talking face when learning speech</article-title>. <source>PNAS</source>, <volume>109</volume>, <fpage>1431</fpage>–<lpage>1436</lpage>.</citation>
</ref>
<ref id="bibr23-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mehler</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Bertoncini</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Barrière</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Jassik-Gerschenfeld</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1978</year>). <article-title>Infant recognition of mother’s voice</article-title>. <source>Perception</source>, <volume>7</volume>, <fpage>491</fpage>–<lpage>497</lpage>.</citation>
</ref>
<ref id="bibr24-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Meltzoff</surname>
<given-names>A. N.</given-names>
</name>
<name>
<surname>Moore</surname>
<given-names>M. K.</given-names>
</name>
</person-group> (<year>1983</year>). <article-title>Newborn infants imitate facial gestures</article-title>. <source>Child Development</source>, <volume>54</volume>, <fpage>702</fpage>–<lpage>709</lpage>.</citation>
</ref>
<ref id="bibr25-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Meltzoff</surname>
<given-names>A. N.</given-names>
</name>
<name>
<surname>Moore</surname>
<given-names>M. K.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Explaining facial imitation: A theoretical model</article-title>. <source>Early Development and Parenting</source>, <volume>6</volume>, <fpage>179</fpage>–<lpage>192</lpage>.</citation>
</ref>
<ref id="bibr26-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Moon</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Panneton-Cooper</surname>
<given-names>R. P.</given-names>
</name>
<name>
<surname>Fifer</surname>
<given-names>W. P.</given-names>
</name>
</person-group> (<year>1993</year>). <article-title>Two-day-olds prefer their native language</article-title>. <source>Infant Behavior and Development</source>, <volume>16</volume>, <fpage>495</fpage>–<lpage>500</lpage>.</citation>
</ref>
<ref id="bibr27-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nazzi</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Floccia</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Bertoncini</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>Discrimination of pitch contours by neonates</article-title>. <source>Infant Behavior and Development</source>, <volume>21</volume>, <fpage>779</fpage>–<lpage>784</lpage>.</citation>
</ref>
<ref id="bibr28-0165025412465361">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Oller</surname>
<given-names>D. K.</given-names>
</name>
</person-group> (<year>1980</year>). <article-title>The emergence of the sounds of speech in infancy</article-title>. In <person-group person-group-type="editor">
<name>
<surname>Yeni-Komshian</surname>
<given-names>G. H.</given-names>
</name>
<name>
<surname>Kavanagh</surname>
<given-names>J. F.</given-names>
</name>
<name>
<surname>Ferguson</surname>
<given-names>C. A.</given-names>
</name>
</person-group> (Eds.), <source>Child phonology, volume 1: Production</source> (pp. <fpage>93</fpage>–<lpage>112</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Academic Press</publisher-name>.</citation>
</ref>
<ref id="bibr29-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Panneton-Cooper</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Aslin</surname>
<given-names>R. N.</given-names>
</name>
</person-group> (<year>1990</year>). <article-title>Preference for infant-directed speech in the first month after birth</article-title>. <source>Child Development</source>, <volume>61</volume>, <fpage>1584</fpage>–<lpage>1595</lpage>.</citation>
</ref>
<ref id="bibr30-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pascalis</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>de Schonen</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>1994</year>). <article-title>Recognition memory in 3–4-day-old human infants</article-title>. <source>Neuroreport</source>, <volume>5</volume>, <fpage>1721</fpage>–<lpage>1724</lpage>.</citation>
</ref>
<ref id="bibr31-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Pascalis</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>de Schonen</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Morton</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Deruelle</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Fabre-Grenet</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Mother’s face recognition by neonates: A replication and an extension</article-title>. <source>Infant Behavior and Development</source>, <volume>18</volume>, <fpage>79</fpage>–<lpage>85</lpage>.</citation>
</ref>
<ref id="bibr32-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Patterson</surname>
<given-names>M. L.</given-names>
</name>
<name>
<surname>Werker</surname>
<given-names>J. F.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Matching phonetic information in lips and voice is robust in 4.5-month-old infants</article-title>. <source>Infant Behavior and Development</source>, <volume>22</volume>, <fpage>237</fpage>–<fpage>247</fpage>.</citation>
</ref>
<ref id="bibr33-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Patterson</surname>
<given-names>M. L.</given-names>
</name>
<name>
<surname>Werker</surname>
<given-names>J. F.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Two-month-old infants match phonetic information in lips and voice</article-title>. <source>Developmental Science</source>, <volume>6</volume>, <fpage>191</fpage>–<lpage>196</lpage>.</citation>
</ref>
<ref id="bibr34-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Reissland</surname>
<given-names>N.</given-names>
</name>
</person-group> (<year>1988</year>). <article-title>Neonatal imitation in the first hour of life: Observations in rural Nepal</article-title>. <source>Developmental Psychology</source>, <volume>24</volume>, <fpage>464</fpage>–<lpage>469</lpage>.</citation>
</ref>
<ref id="bibr35-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Sai</surname>
<given-names>F. Z.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>The role of the mother’s voice in developing mother’s face preference: Evidence for intermodal perception at birth</article-title>. <source>Infant and Child Development</source>, <volume>14</volume>, <fpage>29</fpage>–<lpage>50</lpage>.</citation>
</ref>
<ref id="bibr36-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Spence</surname>
<given-names>M. J.</given-names>
</name>
<name>
<surname>Freeman</surname>
<given-names>M. S.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Newborn infants prefer the maternal low-pass filtered voice, but not the maternal whispered voice</article-title>. <source>Infant Behavior and Development</source>, <volume>19</volume>, <fpage>199</fpage>–<lpage>212</lpage>.</citation>
</ref>
<ref id="bibr37-0165025412465361">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vinter</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>1986</year>). <article-title>The role of movement in eliciting early imitations</article-title>. <source>Child Development</source>, <volume>57</volume>, <fpage>66</fpage>–<lpage>71</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>