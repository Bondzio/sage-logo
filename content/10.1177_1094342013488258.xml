<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342013488258</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342013488258</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Special Issue Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Failure prediction for HPC systems and applications</article-title>
<subtitle>Current situation and open issues</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Gainaru</surname>
<given-names>Ana</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488258">1</xref>
<xref ref-type="aff" rid="aff2-1094342013488258">2</xref>
<xref ref-type="corresp" rid="corresp1-1094342013488258"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cappello</surname>
<given-names>Franck</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342013488258">2</xref>
<xref ref-type="aff" rid="aff3-1094342013488258">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Snir</surname>
<given-names>Marc</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342013488258">2</xref>
<xref ref-type="aff" rid="aff4-1094342013488258">4</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kramer</surname>
<given-names>William</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488258">1</xref>
</contrib>
<bio>
<title>Author biographies</title>
<p>
<italic>Ana Gainaru</italic> is a PhD student in the Computer Science Department of the University of Illinois at Urbana-Champaign. She is holding a research assistantship at the National Centre for Supercomputer Applications where she is part of the reliability and fault-tolerance team for the Blue Waters project. For the past 3 years her work in the context of the INRIA–UIUC Joint Laboratory for Petascale Computing has focused on fault analysis and prediction for large-scale systems. The toolkits developed, HELO and ELSA, have been applied on multiple HPC systems, are currently integrated into the Blue Waters system and are part of the first fault-tolerance hybrid solution that combines a checkpointing protocol with fault prediction. She holds a BS and MS in computer science from the Politehnica University of Bucharest where her work focused on analyzing how different data mining algorithms can be efficiently mapped on GPUs.</p>
<p>
<italic>William Kramer</italic> is responsible for leading the Blue Waters project, a National Science Foundation-funded project at NCSA. Blue Waters is the most powerful general purpose computational and data analytics open science system available. Blue Waters supports a wide and diverse range of science and engineering investigations at unprecedented scale. Previously Kramer was the general manager of the NERSC at Lawrence Berkeley National Laboratory (LBNL) was responsible for all aspects of operations and customer service for NASA’s Numerical Aerodynamic Simulator (NAS) supercomputer center. Blue Waters is the 20th supercomputer Kramer deployed and/or manages, deployed and managed large clusters of workstations, five extremely large data repositories, some of the World’s most intense networks. He has also been involved with the design, creation and commissioning of six “best of class” HPC facilities. He holds a BS and MS in computer science from Purdue University, an ME in electrical engineering from the University of Delaware, a PhD in computer science at UC Berkeley. His research interests include large-scale system performance evaluation, systems management, resource management scheduling, fault detection and resiliency, and cyber protection. He has awards from NASA, Berkeley Laboratory, the Association for Computing Machinery (ACM) and was named one of HPCWire’s “People to Watch” in 2005 and 2013 and Inside HPC first “Rockstar of HPC”. He is the founder of several organizations, including the ACM/IEEE George Michael Memorial HPC Fellowship, the Open Science Grid Executive Committee and the DECUS Seminar Program.</p>
<p>
<italic>Marc Snir</italic> is Director of the Mathematics and Computer Science Division at the Argonne National Laboratory and Michael Faiman and Saburo Muroga Professor in the Department of Computer Science at the University of Illinois at Urbana-Champaign. He currently pursues research in parallel computing. He was head of the Computer Science Department from 2001 to 2007. Until 2001 he was a senior manager at the IBM T.J. Watson Research Center where he led the Scalable Parallel Systems research group that was responsible for major contributions to the IBM SP scalable parallel system and to the IBM Blue Gene system. He received a Ph.D. in Mathematics from the Hebrew University of Jerusalem in 1979, worked at NYU on the NYU Ultracomputer project in 1980–1982, and was at the Hebrew University of Jerusalem in 1982–1986, before joining IBM. He was a major contributor to the design of the Message Passing Interface. He has published numerous papers and given many presentations on computational complexity, parallel algorithms, parallel architectures, interconnection networks, parallel languages and libraries and parallel programming environments. He is an Argonne Distinguished Fellow, AAAS Fellow, ACM Fellow and IEEE Fellow.</p>
<p>
<italic>Franck Cappello</italic> is Program Manager and Senior Computer Scientist at Argonne National Laboratory. Before moving to ANL, he held a joint position at Inria and University of Illinois at Urbana Champaign where he initiated and co-directed from 2009 the INRIA–Illinois Joint Laboratory on Petascale Computing. Until 2008, he led a team at INRIA where he initiated the XtremWeb (Desktop Grid) and MPICH-V (fault-tolerant MPI) projects. From 2003 to 2008, he initiated and directed the Grid5000 project, a nationwide computer science platform for research in large-scale distributed systems. He has authored papers in the domains of fault tolerance, high-performance computing, desktop Grids and Grids and contributed to more than 70 program committees. He is editorial board member of the international <italic>Journal on Grid Computing</italic>, <italic>Journal of Grid and Utility Computing</italic> and <italic>Journal of Cluster Computing</italic>. He is a steering committee member of IEEE/ACM CCGRID. He was the Program program co-chair for ACM HPDC2014, Test of time award chair for IEEE/ACM SC13, Tutorial co-Chair of IEEE/ACM SC12, Technical papers co-chair at IEEE/ACM SC11, Program chair of HiPC2011, program cochair of IEEE CCGRID 2009, Program Area co-chair IEEE/ACM SC’09, General Chair of IEEE HPDC 2006.</p>
</bio>
</contrib-group>
<aff id="aff1-1094342013488258"><label>1</label>National Centre for Supercomputing Applications, Urbana, IL, USA</aff>
<aff id="aff2-1094342013488258"><label>2</label>University of Illinois at Urbana-Champaign, Urbana, IL, USA</aff>
<aff id="aff3-1094342013488258"><label>3</label>INRIA, Rocquencourt, Le Chesnay Cedex, France</aff>
<aff id="aff4-1094342013488258"><label>4</label>Argonne National Laboratory, Argonne, IL, USA</aff>
<author-notes>
<corresp id="corresp1-1094342013488258">Ana Gainaru, University of Illinois at Urbana-Champaign, Office 4017 NCSA, National Center for Supercomputing Aplications, 1205 W. Clark Street, Urbana, IL 61801, USA. Email: <email>againaru@illinois.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2013</year>
</pub-date>
<volume>27</volume>
<issue>3</issue>
<issue-title>Special Issue section on CCDSC 2012 Workshop</issue-title>
<fpage>273</fpage>
<lpage>282</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>As large-scale systems evolve towards post-petascale computing, it is crucial to focus on providing fault-tolerance strategies that aim to minimize fault’s effects on applications. By far the most popular technique is the checkpoint–restart strategy. A complement to this classical approach is failure avoidance, by which the occurrence of a fault is predicted and proactive measures are taken. This requires a reliable prediction system to anticipate failures and their locations. One way of offering prediction is by the analysis of system logs generated during production by large-scale systems. Current research in this field presents a number of limitations that make them unusable for running on real production high-performance computing (HPC) systems. Based on our observations that different failures have different distributions and behaviours, we propose a novel hybrid approach that combines signal analysis with data mining in order to overcome current limitations. We show that by analysing each event according to its specific behaviour, our prediction provides a precision of over 90% and its able to discover about 50% of all failures in a system, result which allows its integration in proactive fault tolerance protocols.</p>
</abstract>
<kwd-group>
<kwd>failure prediction</kwd>
<kwd>fault tolerance</kwd>
<kwd>signal analysis</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342013488258">
<title>1. Introduction</title>
<p>At the scale of today’s large scale systems, fault tolerance is no longer an option, but a necessity. With a system mean time between failures (MTBF) of less than 1 day (<xref ref-type="bibr" rid="bibr16-1094342013488258">Kindratenko, 2011</xref>) and predictions that future systems will experience delays of couple of hours between failures, current fault-tolerance strategies face serious limitations (<xref ref-type="bibr" rid="bibr26-1094342013488258">Snir et al., 2011</xref>). A complement to current approaches is represented by failure avoidance, a technique which is based on an accurate failure predictor. Failure avoidance uses the information received by a failure predictor to facilitate proactive fault-tolerance mechanisms such as proactive job migration or proactive checkpoint.</p>
<p>There are two types of predictions that are possible for high-performance computing (HPC) systems. The first is state prediction where algorithms estimate the state of each node in the system. Current research uses diagrams in order to keep track of the states and the transitions between them in real time (<xref ref-type="bibr" rid="bibr27-1094342013488258">Stearley et al., 2012</xref>; <xref ref-type="bibr" rid="bibr6-1094342013488258">Chen et al., 2004</xref>; <xref ref-type="bibr" rid="bibr24-1094342013488258">Salfner and Malek, 2007</xref>) and in general they use the states to decide whether a job can be scheduled on a specified node. The second type is represented by failure prediction where algorithms focus on providing information about when and where failures will occur in the near future. This paper focuses on the second type.</p>
<p>In general, failure prediction is based on the observation that there is a fault–errors–failure propagation graph (<xref ref-type="bibr" rid="bibr23-1094342013488258">Salfner et al., 2010</xref>). The fault generates a number of errors that could be observable at the system level, which represent either notifications in the log files or changes in performance metrics. The propagation chain ends with the failure which is observed at the application level and usually is represented by an application interruption. However, the error could propagate and generate other effects such as performance degradation.</p>
<p>Our model is based on the observation that different failures have different distributions and create different symptoms in the system. Current state-of-the-art research in the field of failure prediction for HPC systems is based on data-mining approaches and do not differentiate between the behaviour of distinctive failures. In our work, we introduce the concept of signal analysis in the context of event analysis, which allows us to characterize the behaviour of different events and to study how failures affect each of them. This paper highlights the limitations of current fault predictors and proposes ways of overcoming them by combining our signal analysis approach with existing data mining techniques. We show that by analysing each event according to its specific behaviour, our prediction provides a precision of over 90% and its able to discover about 50% of all failures in a system. The paper focuses on a detailed analysis of the prediction method by investigating the characteristics and bottlenecks of each step of the prediction process.</p>
</sec>
<sec id="section2-1094342013488258">
<title>2. Related work</title>
<p>Over the years, approaches on failure prediction have been developed in relation to reliability theory and preventive maintenance (<xref ref-type="bibr" rid="bibr12-1094342013488258">Gertsbakh, 2000</xref>; <xref ref-type="bibr" rid="bibr20-1094342013488258">Nassar and Andrews, 1985</xref>; <xref ref-type="bibr" rid="bibr25-1094342013488258">Schroeder and Gibson, 2006</xref>). Models evolved by trying to incorporate several factors into the distribution, for example the manufacturing process (<xref ref-type="bibr" rid="bibr28-1094342013488258">Vilalta et al., 2002</xref>) or code complexity (<xref ref-type="bibr" rid="bibr8-1094342013488258">Farr, 1996</xref>). However, all of these methods are tailored to long-term predictions and do not work appropriately for online failure prediction.</p>
<p>More recent methods for short-term failure prediction are typically based on runtime monitoring as they take into account a current state of the system. There are two levels of online failure prediction in literature: component-level and system-level failure prediction. The first level assumes methods that observe components (hard drive, mother board, DRAM, etc) with their specific parameters and domain knowledge and define different approaches that give best prediction results for each (<xref ref-type="bibr" rid="bibr15-1094342013488258">Hwang et al., 2012</xref>). One example of this type of approach is to compare the execution of good components with failed ones. A couple of studies from different fields that fit in this category are <xref ref-type="bibr" rid="bibr4-1094342013488258">Bolander et al. (2009</xref>); <xref ref-type="bibr" rid="bibr21-1094342013488258">Patra et al. (2010</xref>). For the HPC community, one example is <xref ref-type="bibr" rid="bibr34-1094342013488258">Zheng et al. (2007</xref>) in which matrices are used to record system performance metrics at every interval. The algorithm afterwards detects outliers by identifying the nodes that are far away from the majority.</p>
<p>The second level is represented by system-level failure prediction, in which monitoring daemons observe different system parameters (system log, scheduler logs, performance metrics, etc.) and investigate the existence of correlations between different events. In the last couple of years, a significant number of papers have been proposed that focus on providing predictions by analysing different HPC systems. However, most predictors are able to use the information extracted in the training phase for only short prediction span after which a new training phase is required. For example, <xref ref-type="bibr" rid="bibr33-1094342013488258">Zheng et al. (2010</xref>) is using almost 3 months of training for predicting only half of month of execution. Another example of a current fault predictor is given by <xref ref-type="bibr" rid="bibr31-1094342013488258">Yu et al. (2011</xref>) where the authors compare two failure prediction approaches and study the influence that the observation window has on the results. <xref ref-type="bibr" rid="bibr13-1094342013488258">Gu et al. (2010)</xref> use a meta-learning predictor to chose between a rule-based method and a statistical method depending on which one gives better predictions for a corresponding state of the system. Another approach for analysing the logs is given by <xref ref-type="bibr" rid="bibr19-1094342013488258">Nakka et al. (2011</xref>), who investigated both usage and failure logs.</p>
<p>The study presented by <xref ref-type="bibr" rid="bibr35-1094342013488258">Zheng and Yu (2011</xref>) makes a difference between system and application failures. The authors use RAS logs and job logs for filtering out the failures that do not have any effect on jobs running in the system. This allows them to make a couple of interesting observations that could help future failure predictors. A more general approach is made by <xref ref-type="bibr" rid="bibr22-1094342013488258">Rajachandrasekar et al. (2012</xref>) where the authors propose a middleware solution between the application and different analysis modules. Failure predictors and other decision-making engines that rely on distributed failure information can benefit from their framework to facilitate proactive fault-tolerance mechanisms such as preemptive job migration.</p>
<p>A different approach is given by <xref ref-type="bibr" rid="bibr18-1094342013488258">Lou et al. (2010</xref>) and <xref ref-type="bibr" rid="bibr30-1094342013488258">Xu (2009</xref>), where the authors investigate parameter correspondence between different application log messages for extracting dependencies among system components. Another approach using time-series analysis is presented by <xref ref-type="bibr" rid="bibr29-1094342013488258">Wang et al. (2010</xref>) where the authors implement different processing methods, such as spike detection and subspace method in order to find outlier patterns which indicate anomalies in monitored systems.</p>
<p>There are a number of ways of building prediction modules for large-scale systems. In this paper we focus on analyzing only log files for the purpose of prediction. Log files give useful information about many components in the system, however, due to their large size and unstructured format they are very complex and cannot be analyzed manually. In general, current state-of-the-art research is using data mining algorithms for automating this process (<xref ref-type="bibr" rid="bibr31-1094342013488258">Yu et al., 2011</xref>; <xref ref-type="bibr" rid="bibr33-1094342013488258">Zheng et al., 2010</xref>; <xref ref-type="bibr" rid="bibr13-1094342013488258">Gu et al., 2010</xref>; <xref ref-type="bibr" rid="bibr19-1094342013488258">Nakka et al., 2011</xref>; <xref ref-type="bibr" rid="bibr17-1094342013488258">Liang, 2006</xref>). Most of these algorithms are using the same workflow: they group the messages in the log file into categories, filter redundant events both in time and space, extract correlations between events based on the small filtered set of log messages and in the end use the correlations to predict future events or failures.</p>
<p>Each step from the workflow introduces imprecision or noise that influences the accuracy of the prediction. In this paper we are analyzing and characterizing this noise. We also propose a novel methodology that decreases the noise and is able to offer predictions that can be used on real production systems.</p>
</sec>
<sec id="section3-1094342013488258">
<title>3. Methodology</title>
<p>Our methodology follows the workflow presented in <xref ref-type="fig" rid="fig1-1094342013488258">Figure 1</xref>. The modules are divided into two major phases: offline characterization and online prediction. The first phase is called training phase and it works on historic event logs by first extracting all event types generated by the system and then by transforming the logs into time series interpreted as signals. Signal analysis allows us to characterize the behaviour of events affecting the system, highlighting the differences between failures. Data mining is an efficient method for extracting patterns in high-dimensionality sets so we use one of these methods to provide accurate correlations between defined behaviours to the online modules. The second phase is responsible with monitoring the incoming stream of events and deciding when to trigger a prediction. Also, modules in this phase are updating the correlations and the characteristics of event’s behaviour to reflect the state of the entire system at different moments. We used the advantages of both data mining and signal analysis by offering a hybrid approach for predicting failures in HPC systems. We also implemented a propagation module that extends the prediction with location information so that the results could be applied for proactive migration or proactive uncoordinated checkpointing. The offline phase is described in more detail by <xref ref-type="bibr" rid="bibr9-1094342013488258">Gainaru et al. (2012a)</xref> and the online by <xref ref-type="bibr" rid="bibr10-1094342013488258">Gainaru et al. (2012b)</xref>.</p>
<fig id="fig1-1094342013488258" position="float">
<label>Figure 1.</label>
<caption>
<p>Failure prediction methodology: (a) offline training; (b) online prediction.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488258-fig1.tif"/>
</fig>
<p>The following subsections follow the workflow of log analysis and prediction methods presented in the related work section. We will highlight the limitations of data mining algorithms and how our model overcomes them. At the end of each section we will discuss the weaknesses of our approach and future directions for optimizing them. Before starting the analysis, we define in this paragraph the main parameters that will be used in the following sections. Precision is seen as a measure of fidelity and represents the proportion of correctly found failures to all identified failures. Recall is the ratio of corrected identifications to all of the existing failures in the log and represents a measure of completeness. The lead time represents the time between when a prediction is triggered and when the failure occurs. The lead time can be used by fault-avoidance techniques to take a proactive action before the failure manifests in the system.</p>
<sec id="section4-1094342013488258">
<title>3.1. Group events of the same type in clusters</title>
<p>In the preprocessing step, we use the Hierarchical Event Log Organizer (HELO) (<xref ref-type="bibr" rid="bibr11-1094342013488258">Gainaru et al., 2011</xref>) on the raw logs, resulting in a list of message templates that represent frequently occurring messages with similar syntactic patterns. Examples of logs and their characteristics can be found in <xref ref-type="table" rid="table1-1094342013488258">Table 1</xref>. These templates represent regular expressions that describe different events in a system. In the online phase, we use HELO online to keep the set of templates updated and consistent to the output of the system.</p>
<table-wrap id="table1-1094342013488258" position="float">
<label>Table 1.</label>
<caption>
<p>Computing platform configuration and template examples.</p>
</caption>
<graphic alternate-form-of="table1-1094342013488258" xlink:href="10.1177_1094342013488258-table1.tif"/>
<table>
<thead>
<tr>
<th>System</th>
<th>Size (MB/day)</th>
<th>Rates (lines/minute)</th>
<th>Template example</th>
</tr>
</thead>
<tbody>
<tr>
<td>BlueGene/L</td>
<td>5.74</td>
<td>15</td>
<td>node card is not fully functional</td>
</tr>
<tr>
<td>Spirit</td>
<td>55.58</td>
<td>339</td>
<td>log_error::is_request bad attempt to connect from * address n+</td>
</tr>
<tr>
<td>Mercury</td>
<td>152.4</td>
<td>868</td>
<td>node_bailout, d+ poll failed from node d+ * job will be killed</td>
</tr>
<tr>
<td>Current system</td>
<td align="center">~ 2 GB/day</td>
<td>~ 5000</td>
<td>vm: killing process %s n+</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Monitoring each event type separately is important since information regarding the events of interest might be hidden when the analysis is made at a lower granularity. For example, when looking at all types of failures at once, the logs show close to no spatial propagation. However, when analyzing only a certain type of filesystem errors, about 20% of failures affect only one node, the rest propagating on a variable number of locations (<xref ref-type="bibr" rid="bibr14-1094342013488258">Heien et al., 2011</xref>).</p>
<p>For each of the event types, we use ELSA (Gainaru et al., 2012a) to map the number of occurrences per time step into the corresponding signal. After analyzing the signals for different HPC systems, we discovered that events are characterized by three types of signals presented in <xref ref-type="fig" rid="fig2-1094342013488258">Figure 2</xref>(a): periodic, noise and silent. Usually, periodic signals represent events generated by monitoring daemons. Silent signals are defined as having a flat line around amplitude zero, and only from time to time presenting burst of messages and are usually characteristic for error messages, for example in case of PBS errors. However, sometimes normal messages behave the same and human knowledge is needed to make the difference between them. This is the case, for example, with “Job starting” messages. Noise are chatty signals that send notifications very often, both during the normal or faulty behaviour of the system. Anomalies in these event types usually represent a precursor to a failure in the system. One example are memory errors that cannot be corrected by ECC that present beforehand an abnormal number of correctable errors.</p>
<fig id="fig2-1094342013488258" position="float">
<label>Figure 2.</label>
<caption>
<p>Signal analysis: (a) correlation methodology for each type of signal; (b) distribution of lead time.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488258-fig2.tif"/>
</fig>
</sec>
</sec>
<sec id="section5-1094342013488258">
<title>Discussion 1</title>
<p>Most data mining techniques rely on human expertise and cannot be used without this input in order to extract the event categories. Manually identifying categories is a time-consuming process and might result in category sets inconsistent across different systems. Moreover manual extraction usually generates wide granularity categories that affect the future analysis. Our focus is on providing an efficient way of identifying the event types that any system generates. In <xref ref-type="bibr" rid="bibr11-1094342013488258">Gainaru et al. (2011</xref>) we showed that our tool can identify the correct templates with over 90% accuracy when compared with system administrators knowledge. Moreover, we analyze BlueGene/P that uses a different error code for each event type (e.g. e10000_ras_link_error for a specific type of link failure). We compared the templates generated by HELO with the error code list.</p>
<p>HELO generates templates that consist of constant words and variables. Variables identify manipulated objects or states for the program and are replaced by wildcards. In case constants are mistakenly replaced by wildcards the template becomes too general and, when variables are identified as constants by HELO, we call the corresponding templates too specific. We plotted the ratio of general and specific templates generated by HELO compared with the error codes of BlueGene/P in <xref ref-type="fig" rid="fig3-1094342013488258">Figure 3</xref>(a) and how these differences affect the final prediction in <xref ref-type="fig" rid="fig3-1094342013488258">Figure 3</xref>(b). Cluster goodness represents the similarity threshold that defines when two messages are part of one single events. Depending on the cluster goodness, there is a 2–30% difference between the template set generated by HELO and the error codes of BlueGene/P. However, this is translated into a much smaller difference when looking at the impact on prediction, the highest impact showing a median difference of only 5% for recall and 3% for precision. Only for extreme values the impact is higher. We argue that this step affects in a small way the final prediction so automatic processes provide great benefits compared with human interaction without significantly affecting the final results.</p>
<fig id="fig3-1094342013488258" position="float">
<label>Figure 3.</label>
<caption>
<p>Preprocessing analysis: (a) percentage of incorrect templates; (b) precision/recall decrease.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488258-fig3.tif"/>
</fig>
</sec>
<sec id="section6-1094342013488258">
<title>Discussion 2</title>
<p>Because data mining is expensive by nature and the size of the log files are posing serious limitations on the analysis step, filtering is used to reduce the size of the analyzed dataset both in time and space without loosing the log’s characteristics. One great advantage of using signal analysis is that it eliminates one of the steps in the workflow used by the pure data mining approaches. Filtering redundant events is no longer necessary since the signals compress the data in the log in a natural way without losing any occurrence. The limitation of filtering methods was analysed by <xref ref-type="bibr" rid="bibr7-1094342013488258">DiMartino (2012</xref>) by creating a log generator that keeps the properties of real logs and that gives a ground truth with which to compare the results of different filtering techniques. Our observations show that the noise from this step filters out 7% to 12% of the events that might be useful to prediction.</p>
<sec id="section7-1094342013488258">
<title>3.2. Extract the correlation between events</title>
<p>In our experiments we observed that different failures show different distributions and behaviours. We use different signal analysis techniques to shape all of these behaviours and to characterize the way failures might affect them. This represents an important step since data mining algorithms apply the same extraction methods on all data entries.</p>
<p>The process for extracting correlations with ELSA is presented in <xref ref-type="fig" rid="fig2-1094342013488258">Figure 1</xref>(a). In the first step, anomalies are extracted from each signal by first using wavelet functions to characterize the normal behaviour of each signal and then by monitoring changes in the signal’s normal frequency and intensity. The right part of the figure shows the transformed signals after filtering out the normal behaviour. What is left for all signals are two values, zero when the messages are generated during the normal behaviour and one when an anomaly occurs. This process ensures that the data mining algorithms for extracting correlations are applied on the same type of data points. The algorithms in detail are described by <xref ref-type="bibr" rid="bibr10-1094342013488258">Gainaru et al. (2012b</xref>)</p>
<p>To test the noise induced by this step we created a log generator that takes into account system parameters that were observed on Inrepid, the BlueGene/P system at ANL (<xref ref-type="bibr" rid="bibr1-1094342013488258">Alam, 2008</xref>). <xref ref-type="table" rid="table2-1094342013488258">Table 2</xref> presents the parameters and their values that were used for the experiments. The log generator first creates failures in the log corresponding to the system parameters and then adds precursors based on a predefine correlation set. The correlations set represents the ground truth and is based on our past experiences with HPC systems. We used the values found after analysing Blue Gene/L from (Gainaru et al., 202b).</p>
<table-wrap id="table2-1094342013488258" position="float">
<label>Table 2.</label>
<caption>
<p>System parameters.</p>
</caption>
<graphic alternate-form-of="table2-1094342013488258" xlink:href="10.1177_1094342013488258-table2.tif"/>
<table>
<thead>
<tr>
<th>MTBF</th>
<th>Failure distribution</th>
<th>Nodes</th>
<th>System lifespan</th>
<th>Propagation</th>
<th>Lead time</th>
</tr>
</thead>
<tbody>
<tr>
<td>1 day</td>
<td>Weibull</td>
<td>40,960</td>
<td>1 year</td>
<td>Yes</td>
<td>Weibull</td>
</tr>
<tr>
<td>
</td>
<td>Scale=8116.7</td>
<td>
</td>
<td>
</td>
<td>20% of failures</td>
<td>Mean = 50s</td>
</tr>
<tr>
<td>
</td>
<td>Shape=0.387187</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
</sec>
<sec id="section8-1094342013488258">
<title>Discussion 3</title>
<p>The generated log is created so that all failures are predictable allowing the results obtained by using ELSA to represent the noise induced by the analysis process. <xref ref-type="fig" rid="fig4-1094342013488258">Figure 4</xref>(a) shows the percentage of incomplete or completely missing sequences extracted by ELSA or by using a data mining technique, such as that in <xref ref-type="bibr" rid="bibr33-1094342013488258">Zheng et al. (2010</xref>). The data mining algorithm that was used is the one incorporated in ELSA but applied on the raw log, by completely excluding all signal analysis modules from the process. We broke down the results by looking at correlations that incorporate different signal types. The figure shows that ELSA gives better results for correlations between noise signals or between signals that have different behaviours. Owing to this fact, overall ELSA cannot detect approximately 25% of the correlations that were used to construct the logs while the data mining approach has far worse results by losing about 52% of the correlations.</p>
<fig id="fig4-1094342013488258" position="float">
<label>Figure 4.</label>
<caption>
<p>Correlation analysis: (a) percentage of incomplete/missing correlations; (b) precision/recall on BlueGene/L and synthetic logs.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488258-fig4.tif"/>
</fig>
</sec>
<sec id="section9-1094342013488258">
<title>Discussion 4</title>
<p>In order to get a better understanding of the impact of log characteristics on prediction’s results, we computed the precision and recall values by predicting both the synthetic log and also the logs from a real HPC system. Moreover, we analysed at the same time, how data mining algorithms behave compared with ELSA’s results in <xref ref-type="fig" rid="fig4-1094342013488258">Figure 4</xref>(b). Our previous observation is visible here as well, there is a difference of more than 20% of recall between ELSA and when only using data mining. Moreover, the recall value for ELSA is approximately 78% which can be explained by the fact that ELSA is not able to find almost a quarter of the initial correlations. This 22% represents the noise of the correlation extraction method used by ELSA and the noise given by the complexity of the logs. Interestingly only approximately a third of the correlations lost by ELSA are responsible for the high decrease in recall. In the future, we plan to isolate them and further study their properties.</p>
</sec>
<sec id="section10-1094342013488258">
<title>Discussion 5</title>
<p>When looking at the results obtained on the logs generated by Blue Gene/L, we observe that the precision is close to that for synthetic logs, however the recall is only about 45% (<xref ref-type="bibr" rid="bibr10-1094342013488258">Gainaru et al., 2012b</xref>). We consider that this 45% together with the rest of 22% up to the recall value obtained for synthetic logs represents the predictable set of failures from Blue Gene/L. The rest of 33% is represented by unpredictable failures represented by failures that do not offer precursors in the log file. This indicates performance metrics or other precursors detectors might give more information before a failure and could help in increasing the results of our predictor.</p>
<sec id="section11-1094342013488258">
<title>3.3. Failure prediction</title>
<p>
<xref ref-type="fig" rid="fig1-1094342013488258">Figure 1</xref>(b) shows the overview of the prediction process. The observation window is used to decide whether the current event is an anomaly. The analysis time represents the overhead of our method in making a prediction: the execution time for detecting the outlier, triggering a correlation sequence, and finding the corresponding locations. The time delay until the predicted event will occur in the system is defining the prediction window, which starts right after the observation point but is visible only at the end of the analysis time. The visible prediction window represents the lead time that a fault-avoidance technique might use. The values used for the lead time given by the correlations are presented in <xref ref-type="fig" rid="fig2-1094342013488258">Figure 2</xref>(b). After the analysis using the log generator, we observed the lead time distribution shifts to the left which means we obtain shorter lead times. This is due to the correlations loss seen when using ELSA. The two problems are related and have the highest impact on prediction’s result so we plan to analyse different correlation extraction methods into more detail in the future.</p>
</sec>
</sec>
<sec id="section12-1094342013488258">
<title>Discussion 6</title>
<p>All modules implemented in ELSA have online phases where they update the data generated in the training phase. In general, current research is not updating the correlations found offline and thus has limitations when using a short training set. We believe this limitation makes the prediction unrealistic when used on real production systems. To study the impact of not adapting the correlation set on the prediction’s result, we used the data collected by ELSA during the training phase to predict the next 9 months of BlueGene/L and plotted the recall for each month in <xref ref-type="fig" rid="fig5-1094342013488258">Figure 5</xref>(a). We have similar results when using the synthetic logs, however due to space limitation we did not present this figures. It is visible that the prediction keeps a high recall value only for the first couple of months and then decreases dramatically. By adapting the correlations and signal characterization over time we were able to keep the recall value almost constant throughout the entire studied life cycle of the system. The recall value presents a slight increases at a 3-month interval when ELSA is redoing the offline analysis. In the rest of the time, ELSA is relying only on the updates of existing correlations.</p>
<fig id="fig5-1094342013488258" position="float">
<label>Figure 5.</label>
<caption>
<p>Prediction analysis: (a) recall on different months; (b) recall/precision tradeoff; (c) recall breakdown on components.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488258-fig5.tif"/>
</fig>
</sec>
<sec id="section13-1094342013488258">
<title>Discussion 7</title>
<p>For some fault-avoidance techniques the cost of predicting a failure that does not appear in the system is low compared with experiencing an unpredicted failure. This is, for example, the case of object migration with Charm++ (<xref ref-type="bibr" rid="bibr32-1094342013488258">Zheng et al., 2004</xref>). Therefore, we did a study of the recall/precision trade-off in <xref ref-type="fig" rid="fig5-1094342013488258">Figure 5</xref>(b). In general, the recall increases when using low threshold values for deciding when a correlation is strong. Interestingly, the maximum recall value reached by ELSA is 63% which is close to what we observed in the previous section as being the amount of predictable failures. However, the cost in precision is really high, making more than 70% of ELSA’s predictions wrong. It is also noticeable that the precision decreases at a higher rate than the increase in recall. Depending on the fault-avoidance technique different values might be the best option.</p>
</sec>
<sec id="section14-1094342013488258">
<title>Discussion 8</title>
<p>In a more detailed analysis, we break down the predicted events into different categories. The results are presented in <xref ref-type="fig" rid="fig5-1094342013488258">Figure 5</xref>(c), where each bar represents how often a certain type of error appears in the log as a percentage reported to all errors in the system. The dark portion of every bar represents the correctly predicted cases out of the total occurrences. We observed that the node card errors were the type that our system detected with a high rate; more than 80% of the occurrences were predicted. Overall, we observed an uneven distribution between different components in the system. For example the results obtained for network and cache failures are almost one order of magnitude lower than the results for network card. For the future, we plan to focus on analysing each component individually and try to understand what influences the prediction process for each of them.</p>
</sec>
<sec id="section15-1094342013488258">
<title>4. Discussion</title>
<p>Accurate predictions are necessary for proactive fault-tolerance solutions. These solutions have the benefit of reducing the overhead due to fault-tolerance actions and the amount of lost work due to predicted failures. However, an extra overhead is added due to wrong predictions. The trade-off between this overhead and the benefit is highly influenced by the predictors recall and precision. We believe that understanding current prediction methods and their limitations is crucial in designing failure-avoidance techniques for exascale systems.</p>
<p>The correlation extraction method has the highest impact on prediction results. Therefore, the choice of the methodology is the utmost part of the prediction. We plan in the future to analyse various algorithms and study their results on large-scale production systems to get a better understanding of their limitations. Data mining algorithms have particularly poor results on noise and periodic signals. Our observation that failures do not affect the same way the system and are represented in different ways in system logs allowed us to analysed failures differently and, in the end, offer more accurate predictions.</p>
<p>Adapting the set of correlations and behaviour characterization is a necessity when working on real systems. Correlations using the last couple of months become unusable after less than 1 month of predictions. The pace of change is becoming increasingly fast for current and future HPC systems, so it is no longer viable for system administrators to give their input in any of the online analysis steps.</p>
<p>With the implementation of more accurate failures predictors there have been developed a number of mathematical models (<xref ref-type="bibr" rid="bibr2-1094342013488258">Aupy et al., 2012</xref>; <xref ref-type="bibr" rid="bibr10-1094342013488258">Gainaru et al., 2012b</xref>) that deal with characterizing the benefit of merging predictors with current checkpointing protocols. We combined ELSA with FTI (<xref ref-type="bibr" rid="bibr3-1094342013488258">Bautista-Gomez et al., 2011</xref>), a fast multi-level checkpointing protocol and observed the overhead induced by the predictor is between 2% and 6%. When using the mathematical models with our predictor parameters and overhead, we observe that the benefit of this fault-avoidance technique can exceed 20% (<xref ref-type="bibr" rid="bibr5-1094342013488258">Bouguerra et al., 2013</xref>). Another direction of our future work focuses on providing real implementations for different fault-avoidance protocols and computing their actual benefit when running large-scale applications in production.</p>
</sec>
<sec id="section16-1094342013488258">
<title>5. Conclusions</title>
<p>Failure prediction has made outstanding progress in the last 5 years and for future HPC systems this technique could give benefits of over 20% when associated with different fault-avoidance techniques compared with the classical fault-tolerance approaches. Understanding the properties of a prediction module and exploiting them for enhancing fault-tolerance approaches and scheduling decisions is crucial for providing scalable solutions for dealing with failures on future HPC systems. In this paper, we described the problems and limitations faced in developing a accurate failure predictor. We show that a good solution is obtained by combining two different analysis techniques: signal analysis for shaping the normal behaviour of each event type and of the whole system and characterizing the way faults affect them and data mining for analysing the correlations between these behaviours. We analysed the deficiencies of our method and potential solutions in order to evolve our predictor into a viable solution for failure avoidance approaches. For the future, we plan to improve the results of our predictor by inspecting different precursor detectors to include in ELSA and by analysing in detail different error types for which our system has a low recall. Also, we will study to a wider extent, the practical way the prediction system influences current fault-tolerance mechanisms.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure" id="fn1-1094342013488258">
<label>Funding</label>
<p>This research is part of the Blue Waters sustained-petascale computing project, which is supported by the National Science Foundation (award number OCI 07-25070) and the state of Illinois. Blue Waters is a joint effort of the University of Illinois at Urbana-Champaign and its National Center for Supercomputing Applications. This research was done in the context of the INRIA-Illinois Joint Laboratory for Petascale Computing. This work was also supported by the U.S. Department of Energy, Office of Science, under Contract No. DE-AC02-06CH11357.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Alam</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Early evaluation of IBM BlueGene/P</article-title>. In: <source>Proceedings of 2008 International Conference for High Performance Computing, Networking, Storage and Analysis</source>, <fpage>p</fpage>. 23.</citation>
</ref>
<ref id="bibr2-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Aupy</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Robert</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Vivien</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Zaidouni</surname>
<given-names>D</given-names>
</name>
</person-group> (<year>2012</year>) <source>Impact of Fault Prediction on Checkpointing Strategies</source>. <comment>Rapport de Recherche RR-8023, INRIA</comment>.</citation>
</ref>
<ref id="bibr3-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bautista-Gomez</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Tsuboi</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Komatitsch</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Cappello</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Maruyama</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Matsuoka</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>FTI: high performance fault tolerance interface for hybrid systems</article-title>. In: <source>Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis (SC ‘11)</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, pp. 32:<fpage>1</fpage>–<lpage>32</lpage>:32. <comment>DOI: 10.1145/2063384.2063427</comment>.</citation>
</ref>
<ref id="bibr4-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bolander</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Qiu</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Eklund</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Hindle</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Rosenfeld</surname>
<given-names>T</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>Physics-based remaining useful life predictions for aircraft engine bearing prognosis</article-title>. In: <source>Conference of the Prognostics and Health Management Society</source>.</citation>
</ref>
<ref id="bibr5-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Bouguerra</surname>
<given-names>MS</given-names>
</name>
<name>
<surname>Gainaru</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Cappello</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Gomez</surname>
<given-names>LB</given-names>
</name>
<name>
<surname>Maruyama</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Matsuoka</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2013</year>) <article-title>Improving the computing efficiency of hpc systems using a combination of proactive and preventive checkpointing</article-title>. In: <source>Proceedings of IEEE IPDPS 2013</source>. <publisher-name>IEEE Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>MY</given-names>
</name>
<name>
<surname>Accardi</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Kýcýman</surname>
<given-names>E</given-names>
</name>
<etal/>
</person-group> (<year>2004</year>) <article-title>Path-based failure and evolution management</article-title>. In: <source>Symposium on Networked Systems Design and Implementation</source>, vol. <volume>1</volume>, p. <fpage>23</fpage>.</citation>
</ref>
<ref id="bibr7-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>DiMartino</surname>
<given-names>C</given-names>
</name>
</person-group> (<year>2012</year>) <article-title>Assessing time coalescence techniques for the analysis of supercomputer logs</article-title>. In: <source>International Conference on Dependable System and Networks</source>, pp. <fpage>1</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr8-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Farr</surname>
<given-names>W</given-names>
</name>
</person-group> (<year>1996</year>) <article-title>Software reliability modeling survey</article-title>. <source>Handbook of Software Reliability Engineering</source>. <publisher-loc>Highstown, NJ</publisher-loc>: <publisher-name>McGraw-Hill</publisher-name>, pp. <fpage>71</fpage>–<lpage>117</lpage>.</citation>
</ref>
<ref id="bibr9-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gainaru</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Cappello</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Kramer</surname>
<given-names>W</given-names>
</name>
</person-group> (<year>2012 a</year>) <article-title>Taming of the shrew: modeling the normal and faulty behavior of large-scale hpc systems</article-title>. In: <source>Proceedings of IEEE IPDPS 2012</source>. <publisher-name>IEEE Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gainaru</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Cappello</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Snir</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Kramer</surname>
<given-names>W</given-names>
</name>
</person-group> (<year>2012b</year>) <article-title>Fault prediction under the microscope: a closer look into HPC systems</article-title>. In: <source>Proceedings of 2012 International Conference for High Performance Computing, Networking, Storage and Analysis</source>. <publisher-name>IEEE Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gainaru</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Cappello</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Trausan-Matu</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Kramer</surname>
<given-names>B</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Event log mining tool for large scale HPC systems</article-title>. In: <source>Proceedings of the 17th International Conference on Parallel Processing - Part I (Euro-Par’11)</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>, pp. <fpage>52</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr12-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gertsbakh</surname>
<given-names>I</given-names>
</name>
</person-group> (<year>2000</year>) <source>Reliability Theory: With Applications to Preventive Maintenance</source>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr13-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gu</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Zheng</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Lan</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>White</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Hocks</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Park</surname>
<given-names>B-H</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Dynamic meta-learning for failure prediction in large-scale systems: A case study</article-title>. <source>Journal of Parallel and Distributed Computing</source> <volume>6</volume>: <fpage>630</fpage>–<lpage>643</lpage>.</citation>
</ref>
<ref id="bibr14-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Heien</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Kondo</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Gainaru</surname>
<given-names>A</given-names>
</name>
<name>
<surname>LaPine</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Kramer</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Cappello</surname>
<given-names>F</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Modeling and tolerating heterogeneous failures in large parallel systems</article-title>. In: <source>Proceedings of 2011 International Conference for High Performance Computing, Networking, Storage and Analysis</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, p. <fpage>45</fpage>.</citation>
</ref>
<ref id="bibr15-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hwang</surname>
<given-names>AA</given-names>
</name>
<name>
<surname>Stefanovici</surname>
<given-names>IA</given-names>
</name>
<name>
<surname>Schroeder</surname>
<given-names>B</given-names>
</name>
</person-group> (<year>2012</year>) <article-title>Cosmic rays don’t strike twice: understanding the nature of dram errors and the implications for system design</article-title>. <source>SIGARCH Computer Architecture News</source> <volume>40</volume>(<issue>1</issue>): <fpage>111</fpage>–<lpage>122</lpage>. <comment>DOI: 10.1145/2189750.2150989</comment>.</citation>
</ref>
<ref id="bibr16-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kindratenko</surname>
<given-names>V</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Trends in high-performance computing</article-title>. <source>Computing in Science and Engineering</source> <volume>3</volume>: <fpage>92</fpage>–<lpage>95</lpage>.</citation>
</ref>
<ref id="bibr17-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Liang</surname>
<given-names>Y</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>BlueGene/L failure analysis and prediction models</article-title>. <source>Proceedings of the International Conference on Dependable Systems and Networks</source>, pp. <fpage>425</fpage>–<lpage>434</lpage>.</citation>
</ref>
<ref id="bibr18-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lou</surname>
<given-names>J-G</given-names>
</name>
<name>
<surname>Fu</surname>
<given-names>Q</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>J</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Mining dependency in distributed systems through unstructured logs analysis</article-title>. <source>ACM SIGOPS Operating Systems Review</source> <volume>44</volume>(<issue>1</issue>): <fpage>91</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr19-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Nakka</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Agrawal</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Choudhary</surname>
<given-names>A</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Predicting node failure in high performance computing systems from failure and usage logs</article-title>. In: <source>IEEE Workshop on Dependable Parallel, Distributed and Network-Centric Systems</source>. <publisher-name>IEEE Press</publisher-name>.</citation>
</ref>
<ref id="bibr20-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nassar</surname>
<given-names>FA</given-names>
</name>
<name>
<surname>Andrews</surname>
<given-names>DM</given-names>
</name>
</person-group> (<year>1985</year>) <article-title>A methodology for analysis of failure prediction data</article-title>. In: <source>IEEE Real-Time Systems Symposium</source>, pp. <fpage>160</fpage>–<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr21-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Patra</surname>
<given-names>AP</given-names>
</name>
<name>
<surname>Bidhar</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Kumar</surname>
<given-names>U</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Failure prediction of rail considering rolling contact fatigue</article-title>. <source>International Journal of Reliability, Quality and Safety Engineering</source> <volume>17</volume>: <fpage>167</fpage>.</citation>
</ref>
<ref id="bibr22-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Rajachandrasekar</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Besseron</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Panda</surname>
<given-names>DK</given-names>
</name>
</person-group> (<year>2012</year>) <article-title>Monitoring and predicting hardware failures in HPC clusters with FTB-IPMI</article-title>. In: <source>International Parallel and Distributed Processing Symposium Workshops</source>.</citation>
</ref>
<ref id="bibr23-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Salfner</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Lenk</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Malek</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>A survey of online failure prediction methods</article-title>. <source>Computing Surveys</source> <volume>42</volume>: <fpage>1</fpage>–<lpage>42</lpage>. <comment>DOI: 10.1145/1670679.1670680</comment>.</citation>
</ref>
<ref id="bibr24-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Salfner</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Malek</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Using hidden semi-Markov models for effective online failure prediction</article-title>. In: <source>Symposium on Reliable Distributed Systems</source>, pp. <fpage>161</fpage>–<lpage>174</lpage>.</citation>
</ref>
<ref id="bibr25-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Schroeder</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Gibson</surname>
<given-names>G</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>A large-scale study of failures in high-performance computing systems</article-title>. <source>IEEE Transactions on Dependable and Secure Computing</source> <volume>7</volume>(<issue>4</issue>): <fpage>337</fpage>–<lpage>351</lpage>.</citation>
</ref>
<ref id="bibr26-1094342013488258">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Snir</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Gropp</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Kogge</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Exascale research: preparing for the post Moore era</article-title>. <source>Computer Science Whitepapers</source>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="https://www.ideals.illinois.edu/bitstream/handle/2142/25469/Exascale%20Research.pdf?sequence=2">https://www.ideals.illinois.edu/bitstream/handle/2142/25469/Exascale%20Research.pdf?sequence=2</ext-link>
</citation>
</ref>
<ref id="bibr27-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Stearley</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Ballance</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Bauman</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>2012</year>) <article-title>A state-machine approach to disambiguating supercomputer event logs</article-title>. In: <source>Workshop on Managing System Automatically and Dynamically</source>, vol. <volume>2</volume>, pp. <fpage>155</fpage>–<lpage>192</lpage>.</citation>
</ref>
<ref id="bibr28-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vilalta</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Apte</surname>
<given-names>CV</given-names>
</name>
<name>
<surname>Hellerstein</surname>
<given-names>JL</given-names>
</name>
<name>
<surname>Ma</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Weiss</surname>
<given-names>SM</given-names>
</name>
</person-group> (<year>2002</year>) <article-title>Predictive algorithms in the management of computer systems</article-title>. <source>IBM Systems Journal</source> <volume>41</volume>: <fpage>461</fpage>–<lpage>474</lpage>.</citation>
</ref>
<ref id="bibr29-1094342013488258">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Talwar</surname>
<given-names>V</given-names>
</name>
<name>
<surname>Schwan</surname>
<given-names>K</given-names>
</name>
<name>
<surname>Ranganathan</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Online detection of utility cloud anomalies using metric distributions</article-title>. <source>Network Operations and Management Symposium</source>, pp. <fpage>96</fpage>–<lpage>103</lpage>.</citation>
</ref>
<ref id="bibr30-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Xu</surname>
<given-names>W</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>Online system problem detection by mining patterns of console logs</article-title>. In: <source>IEEE International Conference on Data Mining</source>, pp. <fpage>588</fpage>–<lpage>597</lpage>.</citation>
</ref>
<ref id="bibr31-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Yu</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Zheng</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Lan</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Coghlan</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Practical online failure prediction for BlueGene/P: period-based vs event-driven</article-title>. In: <source>IEEE Conference on Dependable Systems and Networks Workshops</source>, pp. <fpage>259</fpage>–<lpage>264</lpage>.</citation>
</ref>
<ref id="bibr32-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Zheng</surname>
<given-names>GB</given-names>
</name>
<name>
<surname>Shi</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Kale</surname>
<given-names>LV</given-names>
</name>
</person-group> (<year>2004</year>) <article-title>FTC-Charm++: An in-memory checkpoint-based fault tolerant runtime for Charm++ and MPI</article-title>. In: <source>International Conference on Cluster Computing CLUSTER</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>, pp. <fpage>93</fpage>–<lpage>103</lpage>.</citation>
</ref>
<ref id="bibr33-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Zheng</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Lan</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Gupta</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Coghlan</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Beckman</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>A practical failure prediction with location and lead time for Blue Gene/P</article-title>. In: <source>IEEE Conference on Dependable Systems and Networks Workshops</source>, pp. <fpage>15</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr34-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Zheng</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Li</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Lan</surname>
<given-names>Z</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Anomaly localization in large-scale clusters</article-title>. In: <source>IEEE International Conference on Cluster Computing</source>, pp. <fpage>322</fpage>–<lpage>330</lpage>.</citation>
</ref>
<ref id="bibr35-1094342013488258">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Zheng</surname>
<given-names>Z</given-names>
</name>
<name>
<surname>Yu</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>Co-analysis of RAS log and job log on Blue Gene/P</article-title>. In: <source>Proceedings of the 2011 IEEE International Parallel and Distributed Processing Symposium</source>, pp. <fpage>840</fpage>–<lpage>851</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>