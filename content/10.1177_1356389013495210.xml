<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EVI</journal-id>
<journal-id journal-id-type="hwp">spevi</journal-id>
<journal-title>Evaluation</journal-title>
<issn pub-type="ppub">1356-3890</issn>
<issn pub-type="epub">1461-7153</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1356389013495210</article-id>
<article-id pub-id-type="publisher-id">10.1177_1356389013495210</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Using case studies to explore the external validity of ‘complex’ development interventions</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Woolcock</surname><given-names>Michael</given-names></name>
</contrib>
<aff id="aff1-1356389013495210">World Bank, USA</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-1356389013495210">Michael Woolcock, Mailstop MC3-306, The World Bank, 1818 H Street NW, Washington, DC 20433, USA. Email: <email>mwoolcock@worldbank.org</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2013</year>
</pub-date>
<volume>19</volume>
<issue>3</issue>
<issue-title>Special Issue: What can case studies do?</issue-title>
<fpage>229</fpage>
<lpage>248</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>Rising standards for accurately inferring the impact of development projects has not been matched by equivalently rigorous procedures for guiding decisions about whether and how similar results might be expected elsewhere. These ‘external validity’ concerns are especially pressing for ‘complex’ development interventions, in which the explicit purpose is often to adapt projects to local contextual realities and where high quality implementation is paramount to success. A basic analytical framework is provided for assessing the external validity of complex development interventions. It argues for deploying case studies to better identify the conditions under which diverse outcomes are observed, focusing in particular on the salience of contextual idiosyncrasies, implementation capabilities and trajectories of change. Upholding the canonical methodological principle that questions should guide methods, not vice versa, is required if a truly rigorous basis for generalizing claims about likely impact across time, groups, contexts and scales of operation is to be discerned for different kinds of development interventions.</p>
</abstract>
<kwd-group>
<kwd>case studies</kwd>
<kwd>complexity</kwd>
<kwd>development</kwd>
<kwd>evaluation</kwd>
<kwd>external validity</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p><disp-quote>
<p>[T]he bulk of the literature presently recommended for policy decisions . . . cannot be used to identify ‘what works here’. And this is not because it may fail to deliver in some particular cases [; it] is not because its advice fails to deliver what it can be expected to deliver . . . The failing is rather that it is not designed to deliver the bulk of the key facts required to conclude that it will work here. (<xref ref-type="bibr" rid="bibr16-1356389013495210">Cartwright and Hardie, 2012</xref>: 137)</p>
</disp-quote></p>
<sec id="section1-1356389013495210" sec-type="intro">
<title>Introduction</title>
<p>Over the last 15 years or so, researchers have worked tirelessly to enhance the precision of claims made about the impact of development projects, seeking to formally verify ‘what works’ as part of a broader campaign for ‘evidence-based policy making’ conducted on the basis of ‘rigorous evaluations’. Though most development projects for most of the last 50 years have, upon completion, been subjected to some form of review, by the late 1990s the standards typically deployed in doing so were increasingly deemed inadequate: in an age of heightened public scrutiny of aid budgets and policy effectiveness, and of rising calls by development agencies themselves for greater accountability and transparency, it was no longer acceptable to claim ‘success’ for a project if selected beneficiaries or officials expressed satisfaction, if necessary administrative requirements had been upheld, or if large sums had been dispersed without undue controversy. For their part, researchers seeking publications in elite empirical journals, where the primary criteria for acceptance was (and remains) the integrity of one’s ‘identification strategy’ – i.e. the methods deployed to verify a causal relationship – faced powerful incentives to actively promote not merely more and better impact evaluations, but methods squarely focused on isolating the singular effects of particular variables, such as randomized control trials (RCTs) or quasi-experimental designs (QEDs). Moreover, by claiming to be adopting (or at least approximating) the ‘gold standard’ methodological procedures of biomedical science, champions of RCTs in particular imputed to themselves the moral and epistemological high ground as ‘the white lab coat guys’ of development research.</p>
<p>The heightened focus on RCTs as the privileged basis on which to impute causal claims in development research and project evaluation has been subjected to increasingly trenchant critique in recent years,<sup><xref ref-type="fn" rid="fn1-1356389013495210">1</xref></sup> but for present purposes my objective is not to rehearse, summarize or contribute to these debates per se but rather to assert that these preoccupations have drained attention from an equally important issue, namely our basis for generalizing any claims about impact across time, contexts, groups and scales of operation. If identification and causality are debates about ‘internal validity’, then generalization and extrapolation are concerns about ‘external validity’.<sup><xref ref-type="fn" rid="fn2-1356389013495210">2</xref></sup> It surely matters for the latter that we first have a good handle on the former, but even the cleanest estimation of a given project’s impact does not axiomatically provide warrant for confidently inferring that similar results can be expected if that project is scaled-up or replicated elsewhere.<sup><xref ref-type="fn" rid="fn3-1356389013495210">3</xref></sup> Yet too often this is precisely what happens: having expended enormous effort and resources in procuring a clean estimate of a project’s impact, and having successfully defended the finding under vigorous questioning at professional seminars and review sessions, the standards for inferring that similar results can be expected elsewhere or when ‘scaled up’ suddenly drop away markedly. The ‘rigorous result’, if ‘significantly positive’, translates all too quickly into implicit or explicit claims that the intervention now has the status of a veritable ‘best practice’, the very ‘rigor’ of ‘the evidence’ invoked to promote or defend the project’s introduction into a novel (perhaps highly uncertain) context, wherein it is confidently assumed that it will also now ‘work’.</p>
<p>These tendencies are reflected in and reinforced by the logic of claim-making surrounding ‘systematic reviews’ (e.g. the Cochrane and Campbell Collaborations), in which only a tiny fraction of the studies conducted on a particular intervention (i.e. those conducted using an RCT or perhaps a QED) are deemed sufficiently rigorous for determining the ‘true’ impact of a class of interventions.<sup><xref ref-type="fn" rid="fn4-1356389013495210">4</xref></sup> The very rationale of systematic reviews is to ensure that ‘busy policymakers’ tasked with making difficult choices under hard time and budget constraints have access to ‘warehouses’ of verified ‘instruments’ from which they can prudently chose. In development policy deliberations, especially those premised on identifying interventions most likely to meet predetermined targets (such as the Millennium Development Goals), asking whether and how expectations and project design characteristics might need to be modified for qualitatively different times, places and circumstances is at best a third order consideration; everyone might claim to agree that ‘context matters’ and that ‘one size doesn’t fit all’, but the prestige and power in most development agencies, large and small, remain squarely with project designers, funders and those granting the project’s initial approval. In recent years this august group has been joined by those given (or assuming) the mantle of determining whether that project – or, more ambitiously, the broader class of interventions (‘microfinance’, ‘agricultural extension’) of which the project is a member – actually ‘works’.</p>
<p>Even if concerns about the weak external validity of RCTs/QEDs – or for that matter any methodology – are acknowledged by most researchers, development professionals still lack a useable framework by which to engage in the vexing deliberations surrounding whether and when it is at least plausible to infer that a given impact result (positive or negative) ‘there’ is likely to obtain ‘here’. Equally importantly, we lack a coherent system-level imperative requiring decision-makers to take these concerns seriously, not only so that we avoid intractable, non-resolvable debates about the effectiveness of entire portfolios of activity (‘community health’, ‘justice reform’) or abstractions (‘do women’s empowerment programs work?’<sup><xref ref-type="fn" rid="fn5-1356389013495210">5</xref></sup>) but, more positively and constructively, so that we can enter into context-specific discussions about the relative merits of (and priority that should be accorded to) roads, irrigation, cash transfers, immunization, legal reform etc with some degree of grounded confidence – i.e. on the basis of appropriate metrics, theory, experience and (as we shall see) trajectories of change.</p>
<p>Though the external validity problem is widespread and vastly consequential for lives, resources and careers, my modest goal in this article is not to provide a ‘tool kit’ for ‘resolving it’ but rather to promote a broader conversation about how external validity concerns might be more adequately addressed in the practice of development. (Given that the bar, at present, is very low, facilitating any such conversations will be a non-trivial achievement.) As such, this is an article to think with. Assessing the extent to which empirical claims about a given project’s impact can be generalized is only partly a technical endeavour; it is equally a political, organizational and philosophical issue, and as such useable and legitimate responses will inherently require extended deliberation in each instance. To this end, the article is structured in five sections. Following this introduction, section two provides a general summary of selected contributions to the issue of external validity from a range of disciplines. Section three outlines three domains of inquiry (‘causal density’, ‘implementation capabilities’, ‘reasoned expectations’) that for present purposes constitute the key elements of an applied framework for assessing the external validity of development interventions generally, and ‘complex’ projects in particular. Section four considers the role analytic case studies can play in responding constructively to these concerns. Section five concludes.</p>
</sec>
<sec id="section2-1356389013495210">
<title>External validity concerns across the disciplines: A short tour</title>
<p>Development professionals are far from the only social scientists, or scientists of any kind, who are confronting the challenges posed by external validity concerns. Consider first the field of psychology. It is safe to say that many readers of this article, in their undergraduate days, participated in various psychology research studies. The general purpose of those studies, of course, was (and continues to be) to test various hypotheses about how and when individuals engage in strategic decision-making, display prejudice towards certain groups, perceive ambiguous stimuli, respond to peer pressure, and the like. But how generalizable are these findings? In a detailed and fascinating paper, <xref ref-type="bibr" rid="bibr32-1356389013495210">Henrich et al. (2010a)</xref> reviewed hundreds of such studies, most of which had been conducted on college students in North American and European universities. Despite the limited geographical scope of this sample, most of the studies they reviewed readily inferred (implicitly or explicitly) that their findings were indicative of ‘humanity’ or reflected something fundamental about ‘human nature’. Subjecting these broad claims of generalizability to critical scrutiny (e.g. by examining the results from studies where particular ‘games’ and experiments had been applied to populations elsewhere in the world), Henrich et al. concluded that the participants in the original psychological studies were in fact rather WEIRD – western, educated, industrialized, rich and democratic – since few of the findings of the original studies could be replicated in ‘non-WEIRD’ contexts (see also <xref ref-type="bibr" rid="bibr33-1356389013495210">Henrich et al., 2010b</xref>).</p>
<p>Consider next the field of biomedicine, whose methods development researchers are so often invoked to adopt. In the early stages of designing a new pharmaceutical drug, it is common to test prototypes on mice, doing so on the presumption that mice physiology is sufficiently close to human physiology to enable results on the former to be inferred for the latter. Indeed, over the last several decades a particular mouse – known as ‘Black 6’ – has been genetically engineered so that biomedical researchers around the world are able to work on mice that are literally genetically identical. This sounds ideal for inferring causal results: biomedical researchers in Norway and New Zealand know they are effectively working on clones, and thus can accurately compare findings. Except that it turns out that in certain key respects mice physiology is different enough from human physiology to have compromised ‘years and billions of dollars’ (<xref ref-type="bibr" rid="bibr35-1356389013495210">Kolata, 2013</xref>: A19) of biomedical research on drugs for treating burns, trauma and sepsis, as reported in a <italic>New York Times</italic> summary of a major (39 co-authors) paper published recently in the prestigious <italic>Proceedings of the National Academy of Sciences</italic> (see <xref ref-type="bibr" rid="bibr58-1356389013495210">Seok et al., 2013</xref>). In an award-winning science journalism article, <xref ref-type="bibr" rid="bibr24-1356389013495210">Engber (2012)</xref> summarized research showing that Black 6 was not even representative of mice – indeed, upon closer inspection Black 6 turns out to be ‘a teenaged, alcoholic couch potato with a weakened immune system, and he might be a little hard of hearing’. An earlier study published in the <italic>Lancet</italic> (<xref ref-type="bibr" rid="bibr56-1356389013495210">Rothwell, 2005</xref>) reviewed nearly 200 RCTs in biomedical and clinical research in search of answers to the important question: ‘To whom do the results of this trial apply?’ and concluded, rather ominously, that the methodological quality of many of the published studies was such that even their internal validity, let alone their external validity, was questionable. Needless to say, it is more than a little disquieting to learn that even the people who do actually wear white lab coats for a living have their own serious struggles with external validity.<sup><xref ref-type="fn" rid="fn6-1356389013495210">6</xref></sup></p>
<p>Consider next a wonderful simulation paper in health research, which explores the efficacy of two different strategies for identifying the optimal solution to a given clinical problem, a process the authors refer to as ‘searching the fitness landscape’ (<xref ref-type="bibr" rid="bibr25-1356389013495210">Eppstein et al., 2012</xref>).<sup><xref ref-type="fn" rid="fn7-1356389013495210">7</xref></sup> Strategy one entails adopting a verified ‘best practice’ solution: you attempt to solve the problem, in effect, by doing what experts elsewhere have determined is the best approach. Strategy two effectively entails making it up as you go along: you work with others and learn from collective experience to iterate your way to a customized ‘best fit’<sup><xref ref-type="fn" rid="fn8-1356389013495210">8</xref></sup> solution in response to the particular circumstances you encounter. The problem these two strategies confront is then itself varied. Initially the problem is quite straight forward, exhibiting what is called a ‘smooth fitness landscape’ – think of being asked to climb an Egyptian pyramid, with its familiar symmetrical sides. Over time the problem being confronted is made more complex, its fitness landscape becoming increasingly rugged – think of being asked to ascend a steep mountain, with craggy, idiosyncratic features. Which strategy is best for which problem? It turns out the ‘best practice’ approach is best – but only as long as you are climbing a pyramid (i.e. facing a problem with a smooth fitness landscape). As soon as you tweak the fitness landscape just a little, however, making it even slightly ‘rugged’, the efficacy of ‘best practice’ solutions fall away precipitously, and the ‘best fit’ approach surges to the lead. One can over-interpret these results, of course, but given the powerful imperatives in development to identify ‘best practices’ (as verified by an RCT/QED) and replicate ‘what works’, it is worth pondering the implications of the fact that the ‘fitness landscapes’ we face in development are probably far more likely to be rugged than smooth, and that compelling experimental evidence (supporting a long tradition in the history of science) now suggests that promulgating best practice solutions is a demonstrably inferior strategy for resolving them.</p>
<p>Two final studies demonstrate the crucial importance of implementation and context for understanding external validity concerns in development. <xref ref-type="bibr" rid="bibr9-1356389013495210">Bold et al. (2013)</xref> deploy the novel technique of subjecting RCT results themselves to an RCT test of their generalizability using different types of implementing agencies. Earlier studies from India (e.g. <xref ref-type="bibr" rid="bibr6-1356389013495210">Banerjee et al., 2007</xref>; <xref ref-type="bibr" rid="bibr23-1356389013495210">Duflo et al., 2012</xref>; <xref ref-type="bibr" rid="bibr42-1356389013495210">Muralidharan and Sundararaman, 2010</xref>) famously found that, on the basis of an RCT, contract teachers were demonstrably ‘better’ (i.e. both more effective and less costly) than regular teachers in terms of helping children to learn. A similar result had been found in Kenya, but as with the India finding, the implementing agent was an NGO. Bold et al., took essentially the identical project design but deployed an evaluation procedure in which 192 schools in Kenya were randomly allocated either to a control group, an NGO-implemented group, or a Ministry-of-Education-implemented group. The findings were highly diverse: the NGO-implemented group did quite well relative to the control group (as expected), but the Ministry of Education group actually performed <italic>worse</italic> than the control group. In short, the impact of ‘the project’ was a function not only of its design but, crucially and inextricably, its implementation and context. As the authors aptly conclude, ‘the effects of this intervention appear highly fragile to the involvement of carefully-selected non-governmental organizations. Ongoing initiatives to produce a fixed, evidence-based menu of effective development interventions will be potentially misleading if interventions are defined at the school, clinic, or village level without reference to their institutional context’ (p. 7).<sup><xref ref-type="fn" rid="fn9-1356389013495210">9</xref></sup></p>
<p>A similar conclusion, this time with implications for the basis on which policy interventions might be ‘scaled up’, emerges from an evaluation of a small business registration programme in Brazil (see <xref ref-type="bibr" rid="bibr11-1356389013495210">Bruhn and McKenzie, 2013</xref>). Intuition and some previous research suggests that a barrier to growth faced by small unregistered firms is that their very informality denies them access to legal protection and financial resources; if ways could be found to lower the barriers to registration – e.g. by reducing fees, expanding information campaigns promoting the virtues of registration, etc. – many otherwise unregistered firms would surely avail themselves of the opportunity to register, with both the firms themselves and the economy more generally enjoying the fruits. This was the basis on which the state of Minas Gerais in Brazil sought to expand a business start-up simplification programme into rural areas: a pilot programme that had been reasonably successful in urban areas now sought to ‘scale up’ into more rural and remote districts, the initial impacts extrapolated by its promoters to the new levels and places of operation. At face value this was an entirely sensible expectation, one that could also be justified on intrinsic grounds – one could argue that all small firms, irrespective of location, should as a matter of principle be able to register. Deploying an innovative evaluation strategy centered on the use of existing administrative data, Bruhn and McKenzie found that despite faithful implementation the effects of the expanded programme on firm registration were net <italic>negative</italic>; isolated villagers, it seems, were so deeply wary of the state that heightened information campaigns on the virtues of small business registration only confirmed their suspicions that the government’s real purpose was probably sinister and predatory, and so even those owners that once might have registered their business now did not. If only with the benefit of hindsight, ‘what worked’ in one place and at one scale of operation was clearly inadequate grounds for inferring what could be expected elsewhere at a much larger one.<sup><xref ref-type="fn" rid="fn10-1356389013495210">10</xref></sup></p>
<p>In this brief tour<sup><xref ref-type="fn" rid="fn11-1356389013495210">11</xref></sup> of fields ranging from psychology, biomedicine and clinical health to education, regulation and criminology we have compelling empirical evidence that inferring external validity to given empirical results – i.e. generalizing findings from one group, place, implementation modality or scale of operation to another – is a highly fraught exercise. As the opening epigraph wisely intones, evidence supporting claims of a significant impact ‘there’, <italic>even (or especially) when that evidence is a product of a putatively rigorous research design</italic>, does not ‘deliver the bulk of the key facts required to conclude that it will work here.’ What might those missing ‘key facts’ be? In the next section, I propose three categories of issues that can be used to interrogate given development interventions and the basis of the claims made regarding their effectiveness; I argue that these categories can yield potentially useful and useable ‘key facts’ to better inform pragmatic decision-making regarding the likelihood that results obtained ‘there’ can be expected ‘here’. In section 4 I argue that analytic case studies can be a particularly fruitful empirical resource informing the tone and terms of this interrogation, especially for complex development interventions; indeed, I will argue that this fruitfulness rises in proportion to the ‘complexity’ of the intervention: in short, the higher the complexity the more salient (even necessary) analytic case studies become.</p>
</sec>
<sec id="section3-1356389013495210">
<title>Elements of an applied framework for identifying ‘key facts’</title>
<p>Heightened sensitivity to external validity concerns does not axiomatically solve the problem of how exactly to make difficult decisions regarding whether, when and how to replicate and/or scale-up (or for that matter cancel) interventions on the basis of an initial empirical result, a challenge that becomes incrementally harder as interventions themselves (or constituent elements of them) become more ‘complex’ (see below). Even if we have eminently reasonable grounds for accepting a claim about a given project’s impact ‘there’ (with ‘that group’, at this ‘size’, implemented by ‘those guys’ using ‘that approach’), under what conditions can we confidently infer that the project will generate similar results ‘here’ (or with ‘this group’, or if it is ‘scaled up’, or if implemented by ‘these guys’ deploying ‘this approach’)? We surely need firmer analytical foundations on which to engage in these deliberations; in short, we need more and better ‘key facts’, and a corresponding theoretical framework able to both generate and accurately interpret those facts.</p>
<p>One could plausibly defend a number of domains in which such ‘key facts’ might reside, but for present purposes I focus on three:<sup><xref ref-type="fn" rid="fn12-1356389013495210">12</xref></sup> ‘causal density’ (the extent to which an intervention or its constituent elements are ‘complex’); ‘implementation capability’ (the extent to which a designated organization in the new context can in fact faithfully implement the type of intervention under consideration); and ‘reasoned expectations’ (the extent to which claims about actual or potential impact are understood within the context of a grounded theory of change specifying what can reasonably be expected to be achieved by when). I address each of these domains in turn.</p>
<sec id="section4-1356389013495210">
<title>‘Causal density’<sup><xref ref-type="fn" rid="fn13-1356389013495210">13</xref></sup></title>
<p>Conducting even the most routine development intervention is difficult, in the sense that considerable effort needs to be expended at all stages over long periods of time, and that doing so may entail carrying out duties in places that are dangerous (‘fragile states’) or require navigating morally wrenching situations (dealing with overt corruption, watching children die). If there is no such thing as a ‘simple’ development project, we need at least a framework for distinguishing between different types and degrees of complexity, since this has a major bearing on the likelihood that a project (indeed a system or intervention of any kind) will function in predictable ways, which in turn shapes the probability that impact claims associated with it can be generalized.</p>
<p>One entry point into analytical discussions of complexity is of course ‘complexity theory’, a field to which social scientists have increasingly begun to contribute and learn (see Byrne, this volume; <xref ref-type="bibr" rid="bibr13-1356389013495210">Byrne and Callaghan; 2013</xref>), but for present purposes I will create some basic distinctions using the concept of ‘causal density’ (see <xref ref-type="bibr" rid="bibr41-1356389013495210">Manzi, 2012</xref>). An entity with low causal density is one whose constituent elements interact in precisely predictable ways; a wrist watch, for example, may be a marvel of craftsmanship and micro-engineering, but its very genius is its relative ‘simplicity’: in the finest watches, the cogs comprising the internal mechanism are connected with a degree of precision such that they keep near perfect time over many years, but this is possible because every single aspect of the process is perfectly understood – the watchmakers have achieved what philosophers call ‘proof of concept’. Development interventions (or aspects of interventions<sup><xref ref-type="fn" rid="fn14-1356389013495210">14</xref></sup>) with low causal density are ideally suited for assessment via techniques such as RCTs because it is reasonable to expect that the impact of a particular element can be isolated and discerned, and the corresponding adjustments or policy decisions made. Indeed, the most celebrated RCTs in the development literature – assessing de-worming pills, textbooks, malaria nets, classroom size, cameras in classrooms to reduce teacher absenteeism – have largely been undertaken with interventions (or aspect of interventions) with relatively low causal density. If we are even close to reaching ‘proof of concept’ with interventions such as immunization and iodized salt it is largely because the underlying physiology and biochemistry <italic>has come to be</italic> perfectly understood, and their implementation (while still challenging logistically) requires only basic, routinized behaviour – see baby, insert needle – on the part of front-line agents (see <xref ref-type="bibr" rid="bibr47-1356389013495210">Pritchett and Woolcock, 2004</xref>). In short, when we have ‘proof of concept’ we have essentially eliminated the proverbial ‘black box’ – everything going on inside the ‘box’ (i.e. every mechanism connecting inputs and outcomes) is known or knowable.</p>
<p>Entities with high causal density, on the other hand, are characterized by high uncertainty, which is a function of the numerous pathways and feedback loops connecting inputs, actions and outcomes, the entity’s openness to exogenous influences, and the capacity of constituent elements (most notably people) to exercise discretion (i.e. to act independently of or in accordance with rules, expectations, precedent, passions, professional norms or self-interest). Parenting is perhaps the most familiar example of a high causal density activity. Humans have literally been raising children forever, but as every parent knows, there are often many factors (known and unknown) intervening between their actions and the behaviour of their offspring, who are intensely subject to peer pressure and willfully act in accordance with their own (often fluctuating) wishes. Despite millions of years and billions of ‘trials’, we have not produced anything remotely like ‘proof of concept’ with parenting, even if there are certainly useful rules of thumb. Each generation produces its own best-selling ‘manual’ based on what it regards as the prevailing scientific and collective wisdom, but even if a given parent dutifully internalizes and enacts the latest manual’s every word it is far from certain that his/her child will emerge as a minimally functional and independent young adult; conversely, a parent may know nothing of the book or unwittingly engage in seemingly contrarian practices and yet happily preside over the emergence of a perfectly normal young adult.<sup><xref ref-type="fn" rid="fn15-1356389013495210">15</xref></sup></p>
<p>Assessing the veracity of development interventions (or aspects of them) with high causal density – e.g. women’s empowerment projects, programmes to change adolescent sexual behaviour in the face of the HIV/AIDS epidemic – requires evaluation strategies tailored to accommodate this reality. Precisely because the ‘impact’ (wholly or in part) of these interventions often cannot be truly isolated, and is highly contingent on the quality of implementation, any observed impact is very likely to change over time, across contexts and at different scales of implementation; as such, we need evaluation strategies able to capture these dynamics and provide correspondingly useable recommendations. Crucially, strategies used to assess high causal density interventions are not ‘less rigorous’ than those used to assess their low causal density counterpart; any evaluation strategy, like any tool, is ‘rigorous’ to the extent it deftly and ably responds to the questions being asked of it.<sup><xref ref-type="fn" rid="fn16-1356389013495210">16</xref></sup></p>
<p>To operationalize causal density we need a basic analytical framework for distinguishing more carefully between these ‘low’ and ‘high’ extremes: we can agree that a lawn mower and a family are qualitatively different ‘systems’ but how can we array the spaces in between?<sup><xref ref-type="fn" rid="fn17-1356389013495210">17</xref></sup> Four questions can be proposed to distinguish between different types of problems in development.<sup><xref ref-type="fn" rid="fn18-1356389013495210">18</xref></sup> First, how many person-to-person transactions are required?<sup><xref ref-type="fn" rid="fn19-1356389013495210">19</xref></sup> Second, how much discretion is required of front-line implementing agents?<sup><xref ref-type="fn" rid="fn20-1356389013495210">20</xref></sup> Third, how much pressure do implementing agents face to do something other than respond constructively to the problem?<sup><xref ref-type="fn" rid="fn21-1356389013495210">21</xref></sup> Fourth, to what extent are implementing agents required to deploy solutions from a known menu or to innovate in situ?<sup><xref ref-type="fn" rid="fn22-1356389013495210">22</xref></sup> These questions are most useful when applied to specific operational challenges; rather than asserting that (or trying to determine whether) one ‘sector’ in development is more or less ‘complex’ than another (e.g. ‘health’ versus ‘infrastructure’) it is more instructive to begin with a locally nominated and prioritized problem (e.g. how can workers in this factory be afforded adequate working conditions and wages?) and asking of it the four questions posed above to interrogate its component elements. An example of an array of such problems within ‘health’ is provided in <xref ref-type="fig" rid="fig1-1356389013495210">Figure 1</xref>; by providing straightforward yes/no answers to these four questions we can arrive at five coherent kinds of problems in development: technocratic, logistical, implementation intensive ‘downstream’, implementation intensive ‘upstream’, and complex.</p>
<fig id="fig1-1356389013495210" position="float">
<label>Figure 1.</label>
<caption>
<p>Classification of activities in ‘health’.</p>
<p>Adapted from <xref ref-type="bibr" rid="bibr46-1356389013495210">Pritchett (2013)</xref>.</p>
</caption>
<graphic xlink:href="10.1177_1356389013495210-fig1.tif"/>
</fig>
<p>So understood, problems are truly ‘complex’ that are: highly transaction intensive, require considerable discretion by implementing agents, yield powerful pressures for those agents to do something other than implement a solution, and have no known (ex ante) solution.<sup><xref ref-type="fn" rid="fn23-1356389013495210">23</xref></sup> Solutions to these <italic>kinds</italic> of problems are likely to be highly idiosyncratic and context specific; as such, and irrespective of the quality of the evaluation strategy used to discern their ‘impact’, the default assumption regarding their external validity, I argue, should be zero. Put differently, in such instances the burden of proof should lie with those claiming that the result <italic>is</italic> in fact generalizable. (This burden might be slightly eased for ‘implementation intensive’ problems, but some considerable burden remains nonetheless.) I hasten to add, however, that this does not mean others facing similarly ‘complex’ (or ‘implementation intensive’) challenges elsewhere have little to learn from a successful (or failed) intervention’s experiences; on the contrary, it can be highly instructive, but its ‘lessons’ reside less in the quality of its final ‘design’ characteristics than the processes of exploration and incremental understanding by which a solution was proposed, refined, supported, funded, implemented, refined again, and assessed – i.e. in the ideas, principles and inspiration from which a solution was crafted and enacted. This is the point at which analytic case studies can demonstrate their true utility, as I discuss below.</p>
</sec>
<sec id="section5-1356389013495210">
<title>‘Implementation capability’</title>
<p>Another danger stemming from a single-minded focus on a project’s ‘design’ as the causal agent determining observed outcomes is that implementation dynamics are largely overlooked, or at least assumed to be non-problematic. If, as a result of an RCT (or series of RCTs), a given conditional cash transfer (CCT) programme is deemed to have ‘worked’,<sup><xref ref-type="fn" rid="fn24-1356389013495210">24</xref></sup> we all too quickly presume that it can and should be introduced elsewhere, in effect ascribing to it ‘proof of concept’ status. Again, we can be properly convinced of the veracity of a given evaluation’s empirical findings and yet have grave concerns about its generalizability. If from a ‘causal density’ perspective our four questions would likely reveal that in fact any given CCT comprises numerous elements, some of which are ‘complex’, from an ‘implementation capability’ perspective the concern is more prosaic: how confident can we be that any designated implementing agency in the new country or context would in fact have the capability to do so?</p>
<p>Recent research (<xref ref-type="bibr" rid="bibr49-1356389013495210">Pritchett et al., 2013</xref>) and everyday experience suggests, again, that the burden of proof should lie with those claiming or presuming that the designated implementing agency in the proposed context is indeed up to the task. Consider the delivery of mail. It is hard to think of a less contentious and ‘less complex’ task: everybody wants their mail to be delivered accurately and on time, and doing so is almost entirely a logistical exercise<sup><xref ref-type="fn" rid="fn25-1356389013495210">25</xref></sup> – the procedures to be followed are unambiguous, universally recognized (by international agreement) and entail little discretion on the part of implementing agents (sorters, deliverers). A recent empirical test of the capability of mail delivery systems around the world, however, yielded sobering results. <xref ref-type="bibr" rid="bibr17-1356389013495210">Chong et al. (2012)</xref> sent letters to ten deliberately non-existent addresses in 159 countries, all of which were signatories to an international convention requiring them simply to return such letters to the country of origin (in this case the USA) within 90 days. How many countries were actually able to perform this most routine of tasks? In 25 countries <italic>none</italic> of the 10 letters came back within the designated timeframe; of countries in the bottom half of the world’s education distribution the average return rate was 21 percent of the letters. Working with a broader dataset, <xref ref-type="bibr" rid="bibr46-1356389013495210">Pritchett (2013)</xref> calculates that these countries will take roughly 160 years to have post offices with the capability of countries such as Finland and Colombia (which returned 90% of the letters).<sup><xref ref-type="fn" rid="fn26-1356389013495210">26</xref></sup></p>
<p>The general point is that in many developing countries, especially the poorest, implementation capability is demonstrably low for ‘logistical’ tasks, let alone for ‘complex’ ones. ‘Fragile states’ such as Haiti, almost by definition, cannot readily be assumed to be able to undertake complex tasks (such as disaster relief) even if such tasks are most needed there. And even if they are in fact able to undertake some complex projects (such as regulatory or tax reform), which would be admirable, yet again the burden of proof in these instances should reside with those arguing that such capability to implement does indeed exist (or can readily be acquired). For complex interventions as here defined, high quality implementation is inherently and inseparably a constituent element of any success they may enjoy; the presence in novel contexts of implementing organizations with the requisite capability thus should be demonstrated rather than assumed by those seeking to replicate or expand ‘complex’ interventions.</p>
</sec>
<sec id="section6-1356389013495210">
<title>‘Reasoned expectations’</title>
<p>The final domain of consideration, which I call ‘reasoned expectations’, focuses attention on an intervention’s known or imputed trajectory of change. By this I mean that any empirical claims about a project’s putative impact, <italic>independently of the method(s) by which the claims were determined</italic>, should be understood in the light of where we should reasonably expect a project to be by when. As I have documented elsewhere (<xref ref-type="bibr" rid="bibr62-1356389013495210">Woolcock, 2009</xref>), the default assumption in the vast majority of impact evaluations is that change over time is monotonically linear: baseline data is collected (perhaps on both a ‘treatment’ and a ‘control’ group) and after a specified time follow-up data is also obtained; following necessary steps to factor out the effects of selection and confounding variables a claim is then made about the net impact of the intervention, and if presented graphically is done by connecting a straight line from the baseline scores to the net follow-up scores. The presumption of a straight-line impact trajectory is an enormous one, however, which become readily apparent when one alters the shape of the trajectory (to, say, a step function or a J-curve) and recognizes that the period between the baseline and follow-up data collection is mostly arbitrary; with variable time frames and non-linear impact trajectories, vastly different accounts can be provided of whether a given project is ‘working’ or not.</p>
<p>Consider <xref ref-type="fig" rid="fig2-1356389013495210">Figure 2</xref>. If one was ignorant of a project impact’s underlying functional form, and the net impact of four projects was evaluated ‘rigorously’ at point C, then remarkably similar stories would be told about these projects’ positive impact, and the conclusion would be that they all unambiguously ‘worked’. But what if the impact trajectory of these four interventions actually differs markedly, as represented by the four different lines? And what if the evaluation was conducted not at point C but rather at points A or B? At point A one tells four qualitatively different stories about which projects are ‘working’; indeed, if one had the misfortune to be working on the J-curve project during its evaluation by an RCT at point A, one may well face disciplinary sanction for not merely having ‘no impact’ but for making things worse, as verified by ‘rigorous evidence’! If one then extrapolates into the future, to point D, it is only the linear trajectory that turns out to yield continued gains; the rest either remain stagnant or decline markedly.</p>
<fig id="fig2-1356389013495210" position="float">
<label>Figure 2.</label>
<caption>
<p>Understanding impact trajectories.</p>
</caption>
<graphic xlink:href="10.1177_1356389013495210-fig2.tif"/>
</fig>
<p>A recent paper by <xref ref-type="bibr" rid="bibr15-1356389013495210">Casey et al. (2012)</xref> embodies these concerns. Using an innovative RCT design to assess the efficacy of a ‘community driven development’ project in Sierra Leone, the authors sought to jointly determine the impact of the project on participants’ incomes and the quality of their local institutions. They found ‘positive short-run effects on local public goods and economic outcomes, but no evidence for sustained impacts on collective action, decision making, or the involvement of marginalized groups, suggesting that the intervention did not durably reshape local institutions.’ This may well be true empirically, but such a conclusion presumes that incomes and institutions change at the same pace and along the same trajectory; most of what we know from political and social history would suggest that institutional change in fact follows a trajectory (if it has one at all) more like a step-function or a J-curve than a straight line (see <xref ref-type="bibr" rid="bibr63-1356389013495210">Woolcock et al., 2011</xref>), and that our ‘reasoned expectations’ against which to assess the effects of an intervention trying to change ‘local institutions’ should thus be guided accordingly. Perhaps it is entirely within historical experience to see no measureable change on institutions for a decade; perhaps, in fact, one needs to toil in obscurity for two or more decades as the necessary price to pay for any ‘change’ to be subsequently achieved and discerned;<sup><xref ref-type="fn" rid="fn27-1356389013495210">27</xref></sup> perhaps seeking such change is a highly ‘complex’ endeavour, and as such has no consistent functional form (or has one that is apparent only with the benefit of hindsight, and is an idiosyncratic product of a series of historically contingent moments and processes). In any event, the interpretation and implications of ‘the evidence’ from any evaluation of any intervention is never self-evident; it must be discerned in the light of theory, and benchmarked against reasoned expectations, especially when that intervention exhibits high causal density and necessarily requires robust implementation capability.<sup><xref ref-type="fn" rid="fn28-1356389013495210">28</xref></sup></p>
<p>In the first instance this has important implications for internal validity, but it also matters for external validity, since one dimension of external validity is extrapolation over time. As <xref ref-type="fig" rid="fig2-1356389013495210">Figure 2</xref> shows, the trajectory of change between the baseline and follow-up points bears not only on the claims made about ‘impact’ but on the claims made about the likely impact of this intervention in the future. These extrapolations only become more fraught once we add the dimensions of scale and context, as the Braun and McKenzie (2013) and <xref ref-type="bibr" rid="bibr9-1356389013495210">Bold et al. (2013)</xref> papers reviewed earlier show. The abiding point for external validity concerns is that decision-makers need a coherent theory of change against which to accurately assess claims about a project’s impact ‘to date’ and its likely impact ‘in the future’; crucially, claims made on the basis of a ‘rigorous methodology’ alone do not solve this problem.</p>
<sec id="section7-1356389013495210">
<title>Integrating these domains into a single framework</title>
<p>The three domains considered in this analysis – causal density, implementation capability, reasoned expectations – comprise a basis for pragmatic and informed deliberations regarding the external validity of development interventions in general and ‘complex’ interventions in particular. While data in various forms and from various sources can be vital inputs into these deliberations (see <xref ref-type="bibr" rid="bibr5-1356389013495210">Bamberger et al., 2010</xref>), when the three domains are considered as part of a single integrated framework for engaging with ‘complex’ interventions, it is extended deliberations on the basis of analytic case studies, I argue, that have a particular comparative advantage for delivering the ‘key facts’ necessary for making hard decisions about the generalizability of those interventions (or their constituent elements).</p>
<p>Considered together (see <xref ref-type="fig" rid="fig3-1356389013495210">Figure 3</xref>), it should now be apparent that generalizing about projects that exhibit high causal density, require high implementation capability and generate impacts along an unknown (perhaps even unknowable, ex ante) trajectory is a decidedly high-uncertainty undertaking. These three domains are often interrelated – highly complex projects, by their nature, are likely to exhibit different impact trajectories in different contexts and/or when implemented by different agencies – but for decision-making purposes they can be considered discrete realms of deliberation. As the next section shows, carefully assembled analytic case studies – in conjunction with mixed method research designs (<xref ref-type="bibr" rid="bibr5-1356389013495210">Bamberger et al., 2010</xref>) and realist evaluation strategies (<xref ref-type="bibr" rid="bibr43-1356389013495210">Pawson, 2006</xref>) – can be an informed basis on which these deliberations are conducted.</p>
<fig id="fig3-1356389013495210" position="float">
<label>Figure 3.</label>
<caption>
<p>An integrated framework for assessing external validity.</p>
</caption>
<graphic xlink:href="10.1177_1356389013495210-fig3.tif"/>
</fig>
</sec>
</sec>
</sec>
<sec id="section8-1356389013495210">
<title>Harnessing the distinctive contribution of analytic case studies</title>
<p>When carefully compiled and conveyed, case studies can be instructive for policy deliberations across the analytic space set out in <xref ref-type="fig" rid="fig3-1356389013495210">Figure 3</xref>. Our focus here is on development problems that are highly complex, require robust implementation capability and that unfold along non-linear context-specific trajectories, but this is only where the comparative advantage of case studies is strongest (and where, by extension, the comparative advantage of RCTs is weakest). It is obviously beyond the scope of this article to provide a comprehensive summary of the theory and strategies underpinning case study analysis,<sup><xref ref-type="fn" rid="fn29-1356389013495210">29</xref></sup> but three key points bear some discussion (which I provide below): the distinctiveness of case studies as a method of analysis in social science beyond the familiar qualitative/quantitative divide; the capacity of case studies to elicit causal claims and generate testable hypotheses; and (related) the focus of case studies on exploring and explaining mechanisms (i.e. identifying how, for whom and under what conditions outcomes are observed – or ‘getting inside the black box’).</p>
<p>The rising quality of the analytic foundations of case study research has been one of the underappreciated (at least in mainstream social science) methodological advances of the last twenty years (<xref ref-type="bibr" rid="bibr39-1356389013495210">Mahoney, 2007</xref>). Where everyday discourse in development research typically presumes a rigid and binary ‘qualitative’ or ‘quantitative’ divide, this is a distinction many contemporary social scientists (especially historians, historical sociologists and comparative political scientists) feel does not aptly accommodates their work, if ‘qualitative’ is understood to mean ‘ethnography’, ‘participant observation’ and ‘interviews’. These researchers see themselves as occupying a distinctive epistemological space, using case studies (across varying units of analysis: countries to firms to events) to interrogate instances of phenomena – with an ‘N’ of, say, 30, such as revolutions – that are ‘too large’ for orthodox qualitative approaches and ‘too small’ for orthodox quantitative analysis. (There is no inherent reason, they argue, why the problems of the world should array themselves in accordance with the bi-modal methodological distribution social scientists otherwise impose on them.)</p>
<p>More ambitiously perhaps, case study researchers also claim to be able to draw causal inferences (see <xref ref-type="bibr" rid="bibr38-1356389013495210">Mahoney, 2000</xref>). Defending this claim in detail requires engagement with philosophical issues beyond the scope of this article,<sup><xref ref-type="fn" rid="fn30-1356389013495210">30</xref></sup> but a pragmatic application can be seen in the law (<xref ref-type="bibr" rid="bibr34-1356389013495210">Honoré, 2010</xref>), where it is the task of investigators to assemble various forms and sources of evidence (inherently of highly variable quality) as part of the process of building a ‘case’ for or against a charge, which must then pass the scrutiny of a judge or jury: whether a threshold of causality is reached in this instance has very real (in the real world) consequences. Good case study research in effect engages in its own internal dialogue with the ‘prosecution’ and ‘defense’, posing alternative hypotheses to account for observed outcomes and seeking to test their veracity on the basis of the best available evidence. As in civil law, a ‘preponderance of the evidence’ standard<sup><xref ref-type="fn" rid="fn31-1356389013495210">31</xref></sup> is used to determine whether a causal relationship has been established. This is the basis on which causal claims (and, needless to say, highly ‘complex’ causal claims) affecting the fates of individuals, firms and governments are determined in courts every day, and deploying a variant on it is what good case study research entails.</p>
<p>Finally, by exploring ‘cases within cases’ (thereby raising or lowering the instances of phenomena they are exploring), and by overtly tracing the evolution of given cases over time within the context(s) in which they occur, case study researchers seek to document and explain the processes by which, and the conditions under which, certain outcomes are obtained. (This technique is sometimes referred to as process tracing, or assessing the ‘causes of effects’ as opposed to the ‘effects of causes’ approach characteristic of most econometric research.) Case study research finds its most prominent place in development research and programme assessment in the literature on ‘realist evaluation’ (the foundational text is <xref ref-type="bibr" rid="bibr44-1356389013495210">Pawson and Tilly, 1997</xref>), where the abiding focus is exploiting, exploring and explaining variance (or standard deviations): i.e. on identifying what works for whom, when, where and why.<sup><xref ref-type="fn" rid="fn32-1356389013495210">32</xref></sup> This is the signature role that case studies can play for understanding ‘complex’ development interventions in particular on their own terms, as has been the central plea of this article.</p>
</sec>
<sec id="section9-1356389013495210" sec-type="conclusions">
<title>Conclusion</title>
<p>The energy and exactitude with which development researchers debate the veracity of claims about ‘causality’ and ‘impact’ (internal validity) has yet to inspire corresponding firepower in the domain of concerns about whether and how to ‘replicate’ and ‘scale up’ interventions (external validity). Indeed, as manifest in everyday policy debates in contemporary development, the gulf between these modes of analysis is wide, palpable and consequential: the fate of billions of dollars, millions of lives and thousands of careers turn on how external validity concerns are addressed, and yet too often the basis for these deliberations is decidedly shallow.</p>
<p>It does not have to be this way. The social sciences, broadly defined, contain within them an array of theories and methods for addressing both internal and external validity concerns; they are there to be deployed if invited to the table (see <xref ref-type="bibr" rid="bibr61-1356389013495210">Stern et al., 2012</xref>). This article has sought to show that ‘complex’ development interventions require evaluation strategies tailored to accommodate that reality; such interventions are square pegs which when forced into methodological round holes yield confused, even erroneous, verdicts regarding their effectiveness ‘there’ and likely effectiveness ‘here’. History is now demanding that development professionals engage with issues of increasing ‘complexity’: consolidating democratic transitions, reforming legal systems, promoting social inclusion, enhancing public sector management. These types of issues are decidedly (wickedly) ‘complex’, and responses to them need to be prioritized, designed, implemented and assessed accordingly. Beyond evaluating such interventions on their own terms, however, it is as important to be able to advise front-line staff, senior management and colleagues working elsewhere about when and how the ‘lessons’ from these diverse experiences can be applied. Deliberations centered on causal density, implementation capability and reasoned expectations have the potential to elicit, inform and consolidate this process.</p>
</sec>
</body>
<back>
<ack>
<p>The views expressed in this article are those of the author alone, and should not be attributed to the World Bank, its executive directors or the countries they represent. This article is part of a larger project on identifying practical strategies for assessing the conditions under which the impacts of complex development interventions can be assessed and generalized. I am grateful to Elliot Stern for inviting me to turn some initial thoughts into a more substantial article, to Arathi Rao for diligent research assistance, to April Harding, Heather Lanthorn, Massoud Moussavi, and Lant Pritchett for constructive written comments on an early draft, and to seminar participants at the University of Copenhagen, the World Bank, InterAction, USAID, DFID, SIDA (Sweden), Unite for Sight, the American Sociological Association (Economic Development section meeting) and the American Evaluation Association for helpful feedback, questions and suggestions. Scott Guggenheim, April Harding, Christopher Nelson, Lant Pritchett, Vijayendra Rao and Sanjeev Sridharan have been valued interlocutors on these issues for many years (though of course the usual disclaimers apply).</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-1356389013495210">
<label>1.</label>
<p>See, among others, <xref ref-type="bibr" rid="bibr14-1356389013495210">Cartwright (2007)</xref>, <xref ref-type="bibr" rid="bibr21-1356389013495210">Deaton (2010)</xref>, <xref ref-type="bibr" rid="bibr45-1356389013495210">Picciotto (2012)</xref>, <xref ref-type="bibr" rid="bibr55-1356389013495210">Ravallion (2009)</xref> and <xref ref-type="bibr" rid="bibr59-1356389013495210">Shaffer (2011)</xref>. Nobel laureate James Heckman has been making related critiques of ‘randomization bias’ in the evaluation of social policy experiments for over 20 years.</p>
</fn>
<fn fn-type="other" id="fn2-1356389013495210">
<label>2.</label>
<p>The distinctions between construct, internal and external validity form, along with replication, the four core elements of the classic quasi-experimental methodological framework of <xref ref-type="bibr" rid="bibr20-1356389013495210">Cook and Campbell (1979)</xref>. In more recent work, <xref ref-type="bibr" rid="bibr19-1356389013495210">Cook (2001)</xref> is decidedly more circumspect about the extent to which social scientists (of any kind) can draw empirical generalizations.</p>
</fn>
<fn fn-type="other" id="fn3-1356389013495210">
<label>3.</label>
<p>The veracity of extrapolating given findings to a broader population in large part turns on sampling quality; the present concern is with enhancing the analytical bases for making comparisons about likely impact between different populations, scales of operation (e.g. pilot projects to national programmes) and across time.</p>
</fn>
<fn fn-type="other" id="fn4-1356389013495210">
<label>4.</label>
<p>In a recent systematic review to which I contributed, our team assessed the effectiveness of conditional and unconditional cash transfer programmes, applying (by rule) the RCT-only criteria. This meant that fully 97% of the published literature – more than 4000 studies, many of them published in leading peer-reviewed journals by seasoned practitioners and researchers – had to be declared inadmissible, as essentially having nothing of substance to contribute. Note that this is not a criticism of systematic reviews (or RCTs/QEDs) per se – they are what they are; rather, my concern is the broader apparatus of institutional decision-making that has created, in effect, a monopoly on what counts as a question and what counts as an answer in the assessment of social interventions (with, I would argue, all the attendant inefficiencies one characteristically associates with monopolies).</p>
</fn>
<fn fn-type="other" id="fn5-1356389013495210">
<label>5.</label>
<p>The insightful and instructive review of ‘community driven development’ programmes by <xref ref-type="bibr" rid="bibr40-1356389013495210">Mansuri and Rao (2012)</xref> emphasizes the importance of understanding context when making claims about the effectiveness of such programmes (and their generalizability), though it has not always been read this way.</p>
</fn>
<fn fn-type="other" id="fn6-1356389013495210">
<label>6.</label>
<p>It is worth pointing out that the actual ‘gold standard’ in clinical trials requires not merely the random assignment of subjects to treatment and control groups, but that the allocation be ‘triple blind’ (i.e. neither the subjects themselves, the front-line researchers nor the principal investigators knows who has been assigned to which group until after the study is complete), that control groups receive a placebo treatment (i.e. a treatment that looks and feels like a real treatment, but is in fact not one at all) and that subjects cross over between groups mid-way through the study (i.e. the control group becomes the treatment group, and the treatment group becomes the control group) – all to deal with well-understood sources of bias (e.g. Hawthorn effects) that could otherwise compromise the integrity of the study. Needless to say, it is hard to imagine any policy intervention, let alone a development project, could come remotely close to upholding these standards.</p>
</fn>
<fn fn-type="other" id="fn7-1356389013495210">
<label>7.</label>
<p>In a more applied version of this idea, <xref ref-type="bibr" rid="bibr48-1356389013495210">Pritchett et al. (2012)</xref> argue for ‘crawling the design space’ as the strategy of choice for navigating rugged fitness environments.</p>
</fn>
<fn fn-type="other" id="fn8-1356389013495210">
<label>8.</label>
<p>The concept of ‘best fit’ comes to development primarily through the work of <xref ref-type="bibr" rid="bibr10-1356389013495210">David Booth (2011)</xref>; in the <xref ref-type="bibr" rid="bibr25-1356389013495210">Eppstein et al. (2012)</xref> formulation, the equivalent concept for determining optimal solutions to novel problems in different contexts emerges through what they refer to as ‘quality improvement collaboratives’ (QICs). Their study effectively sets up an empirical showdown between RCTs and QICs as rival strategies for complex problem solving.</p>
</fn>
<fn fn-type="other" id="fn9-1356389013495210">
<label>9.</label>
<p>See also the important work of <xref ref-type="bibr" rid="bibr22-1356389013495210">Denizer et al. (2012)</xref>, who assess the performance of more than 6000 World Bank projects from inception to completion, a central finding of which is the key role played by high quality task team leaders (i.e. those responsible for the project’s management and implementation on a day-to-day basis) in projects that are not only consistently rated ‘satisfactory’ but manage to become ‘satisfactory’ after a mid-term review deeming their project ‘unsatisfactory’.</p>
</fn>
<fn fn-type="other" id="fn10-1356389013495210">
<label>10.</label>
<p>See also the insightful discussion of the criminology impact evaluation literature in <xref ref-type="bibr" rid="bibr57-1356389013495210">Sampson (2013)</xref>, who argues strongly for exploring the notion of ‘contextual causality’ as a basis for inferring what might work elsewhere. <xref ref-type="bibr" rid="bibr36-1356389013495210">Lamont (2012)</xref> also provides a thoughtful overview of evaluation issues from a sociological perspective.</p>
</fn>
<fn fn-type="other" id="fn11-1356389013495210">
<label>11.</label>
<p><xref ref-type="bibr" rid="bibr53-1356389013495210">Rao and Woolcock (forthcoming)</xref> provide a more extensive review of the literature on external validity and its significance for development policy. Econometricians have recently begun to focus more concertedly on external validity concerns (e.g. <xref ref-type="bibr" rid="bibr1-1356389013495210">Allcott and Mullainathan, 2012</xref>; <xref ref-type="bibr" rid="bibr3-1356389013495210">Angrist and Fernandez-Val, 2010</xref>), though their contributions to date have largely focused on technical problems emergent within evaluations of large social programmes in OECD countries (most notably the USA) rather than identifying pragmatic guidelines for replicating or expanding different types of projects in different types of (developing) country contexts.</p>
</fn>
<fn fn-type="other" id="fn12-1356389013495210">
<label>12.</label>
<p>These three domains are derived from my reading of the literature, numerous discussions with senior operational colleagues, and my hard-won experience both assessing complex development interventions (e.g. <xref ref-type="bibr" rid="bibr7-1356389013495210">Barron et al., 2011</xref>) and advising others considering their expansion/replication elsewhere.</p>
</fn>
<fn fn-type="other" id="fn13-1356389013495210">
<label>13.</label>
<p>The idea of causal density comes from neuroscience, computing and physics, and can be succinctly defined as ‘the number of independent significant interactions among a system’s components’ (<xref ref-type="bibr" rid="bibr60-1356389013495210">Shanahan, 2008</xref>: 041924). More formally, and within economics, it is an extension of the notion of ‘Granger causality’, in which data from one time-series is used to make predictions about another.</p>
</fn>
<fn fn-type="other" id="fn14-1356389013495210">
<label>14.</label>
<p>See <xref ref-type="bibr" rid="bibr37-1356389013495210">Klinger et al. (2011)</xref> for a discussion of the virtues of conducting delineated ‘mechanism experiments’ within otherwise large social policy interventions.</p>
</fn>
<fn fn-type="other" id="fn15-1356389013495210">
<label>15.</label>
<p>Such books are still useful, of course, and diligent parents do well to read them; the point is that at best the books provide general guidance at the margins on particular issues, which is incorporated into the larger storehouse of knowledge the parent has gleaned from their own parents, through experience, common sense and the advice of significant others.</p>
</fn>
<fn fn-type="other" id="fn16-1356389013495210">
<label>16.</label>
<p>That is, hammers, saws and screwdrivers are not ‘rigorous’ tools; they become so to the extent they are correctly deployed in response to the distinctive problem they are designed to solve.</p>
</fn>
<fn fn-type="other" id="fn17-1356389013495210">
<label>17.</label>
<p>In the complexity theory literature, this space is characteristically arrayed according to whether problems are ‘simple’, ‘complicated’, ‘complex’ and ‘chaotic’ (see <xref ref-type="bibr" rid="bibr52-1356389013495210">Ramalingam and Jones, 2009</xref>). There is much overlap in these distinctions with the framework I present below, but my concern (and that of the colleagues with whom I work most closely on this) is primarily with articulating pragmatic questions for arraying development interventions, which leads to slightly different categories.</p>
</fn>
<fn fn-type="other" id="fn18-1356389013495210">
<label>18.</label>
<p>The first two questions (or dimensions) come from <xref ref-type="bibr" rid="bibr47-1356389013495210">Pritchett and Woolcock (2004)</xref>; the latter two from <xref ref-type="bibr" rid="bibr2-1356389013495210">Andrews et al. (forthcoming)</xref>.</p>
</fn>
<fn fn-type="other" id="fn19-1356389013495210">
<label>19.</label>
<p>Producing a minimally educated child, for example, requires countless interactions between teacher and student (and between students) over many years; the raising or lowering of interest rates is determined at periodic meetings by a handful of designated technical professionals.</p>
</fn>
<fn fn-type="other" id="fn20-1356389013495210">
<label>20.</label>
<p>Being an effective social worker requires making wrenching discretionary decisions (e.g. is this family sufficiently dysfunctional that I should withdraw the children and make them wards of the state?); reducing some problems to invariant rules (e.g. the age at which young adults are sufficiently mature to drive, vote, or drink alcohol) should in principle make their implementation relatively straightforward by reducing discretion entirely, but as <xref ref-type="bibr" rid="bibr31-1356389013495210">Gupta (2012)</xref> powerfully shows for India, weak administrative infrastructure (e.g. no birth certificates or land registers) can render even the most basic demographic questions (age, number of children, size of land holding) matters for discretionary interpretation by front-line agents, with all the potential for abuse and arbitrariness that goes with it.</p>
</fn>
<fn fn-type="other" id="fn21-1356389013495210">
<label>21.</label>
<p>Virtually everyone agrees that babies should be immunized, that potholes should be fixed, and that children should be educated; professionals implementing these activities will face little political resistance or ‘temptations’ to do otherwise. Those enforcing border patrols, regulating firms or collecting property taxes, on the other hand, will encounter all manner of resistance and ‘temptations’ (e.g. bribes) to be less than diligent.</p>
</fn>
<fn fn-type="other" id="fn22-1356389013495210">
<label>22.</label>
<p>Even when a problem is clear and well understood – e.g. fatty foods, a sedentary lifestyle and smoking are not good for one’s health – it may or may not map onto a known, universal, readily implementable solution.</p>
</fn>
<fn fn-type="other" id="fn23-1356389013495210">
<label>23.</label>
<p>In more vernacular language we might characterize such problems as ‘wicked’ (after <xref ref-type="bibr" rid="bibr18-1356389013495210">Churchman, 1967</xref>).</p>
</fn>
<fn fn-type="other" id="fn24-1356389013495210">
<label>24.</label>
<p>See, among others, the extensive review of the empirical literature on CCTs provided in <xref ref-type="bibr" rid="bibr26-1356389013495210">Fiszbein and Schady (2009)</xref>; <xref ref-type="bibr" rid="bibr4-1356389013495210">Baird et al. (2013)</xref> provide a more recent ‘systematic review’ of the effect of both conditional and unconditional cash transfer programmes on education outcomes.</p>
</fn>
<fn fn-type="other" id="fn25-1356389013495210">
<label>25.</label>
<p>Indeed, the high-profile advertising slogan of a large, private international parcel service is: ‘We love logistics’.</p>
</fn>
<fn fn-type="other" id="fn26-1356389013495210">
<label>26.</label>
<p>For a broader conceptual and empirical discussion of the evolving organizational capabilities of developing countries see <xref ref-type="bibr" rid="bibr49-1356389013495210">Pritchett et al. (2013)</xref>. An applied strategy for responding to the challenges identified therein is presented in <xref ref-type="bibr" rid="bibr2-1356389013495210">Andrews et al. (forthcoming)</xref>.</p>
</fn>
<fn fn-type="other" id="fn27-1356389013495210">
<label>27.</label>
<p>Any student of the history of issues such as civil liberties, gender equality, the rule of law and human rights surely appreciates this; many changes took centuries to be realized, and many remain unfulfilled.</p>
</fn>
<fn fn-type="other" id="fn28-1356389013495210">
<label>28.</label>
<p>In a blog post I have used a horticultural analogy to demonstrate this point: no one would claim that sunflowers are ‘more effective’ than acorns if we were to test their ‘growth performance’ over a two month period. After this time the sunflowers would be six feet high and the acorns would still be dormant underground, with ‘nothing to show’ for their efforts. But we know the expected impact trajectory of sunflowers and oak trees: it is wildly different, and as such we judge (or benchmark) their growth performance over time accordingly. Unfortunately we have no such theory of change informing most assessments of most development projects at particular points in time; in the absence of such theories – whether grounded in evidence and/or experience – and corresponding trajectories of change, we assume linearity (which for ‘complex’ interventions as defined in this article is almost assuredly inaccurate).</p>
</fn>
<fn fn-type="other" id="fn29-1356389013495210">
<label>29.</label>
<p>Such accounts are provided in the canonical works of <xref ref-type="bibr" rid="bibr51-1356389013495210">Ragin and Becker (1992)</xref>, <xref ref-type="bibr" rid="bibr28-1356389013495210">George and Bennett (2005)</xref>, <xref ref-type="bibr" rid="bibr29-1356389013495210">Gerring (2007)</xref> and <xref ref-type="bibr" rid="bibr64-1356389013495210">Yin (2009)</xref>; see also the earlier work of <xref ref-type="bibr" rid="bibr50-1356389013495210">Ragin (1987)</xref> on ‘qualitative comparative analysis’ and <xref ref-type="bibr" rid="bibr8-1356389013495210">Bates et al. (1998)</xref> on ‘analytic narratives’, and the most recent methodological innovations outlined in <xref ref-type="bibr" rid="bibr30-1356389013495210">Goertz and Mahoney (2012)</xref>.</p>
</fn>
<fn fn-type="other" id="fn30-1356389013495210">
<label>30.</label>
<p>But see the discussion in <xref ref-type="bibr" rid="bibr16-1356389013495210">Cartwright and Hardie (2012)</xref>; <xref ref-type="bibr" rid="bibr27-1356389013495210">Freedman (2008)</xref> and especially <xref ref-type="bibr" rid="bibr30-1356389013495210">Goertz and Mahoney (2012)</xref> are also instructive on this point.</p>
</fn>
<fn fn-type="other" id="fn31-1356389013495210">
<label>31.</label>
<p>In criminal law the standard is higher; the evidence must be ‘beyond a reasonable doubt’.</p>
</fn>
<fn fn-type="other" id="fn32-1356389013495210">
<label>32.</label>
<p>This strand of work can reasonably be understood as a qualitative complement to <xref ref-type="bibr" rid="bibr54-1356389013495210">Ravallion’s (2001)</xref> clarion call for development researchers to ‘look beyond averages’.</p>
</fn>
</fn-group>
</notes>
<bio>
<p><bold>Michael Woolcock</bold> is Lead Social Development Specialist in the World Bank’s Development Research Group, where he has worked since 1998. He is also a (part-time) Lecturer in Public Policy at Harvard University’s Kennedy School of Government. An Australian national, he has an MA and PhD in sociology from Brown University.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Allcott</surname><given-names>H</given-names></name>
<name><surname>Mullainathan</surname><given-names>S</given-names></name>
</person-group> (<year>2012</year>) <source>External validity and partner selection bias</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research Working Paper No. 18373</publisher-name>.</citation>
</ref>
<ref id="bibr2-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Andrews</surname><given-names>M</given-names></name>
<name><surname>Pritchett</surname><given-names>L</given-names></name>
<name><surname>Woolcock</surname><given-names>M</given-names></name>
</person-group> (<year>forthcoming</year>) <article-title>Escaping capability traps through problem-driven iterative adaption (PDIA)</article-title>. <source>World Development</source>.</citation>
</ref>
<ref id="bibr3-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Angrist</surname><given-names>J</given-names></name>
<name><surname>Fernandez-Val</surname><given-names>I</given-names></name>
</person-group> (<year>2010</year>) <source>Extrapolate-ing: external validity and overidentification in the LATE framework</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>National Bureau of Economic Research</publisher-name>. National Bureau of Economic Research Working Paper No. 16566.</citation>
</ref>
<ref id="bibr4-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Baird</surname><given-names>S</given-names></name>
<name><surname>Ferreira</surname><given-names>F</given-names></name>
<name><surname>Özler</surname><given-names>B</given-names></name>
<name><surname>Woolcock</surname><given-names>M</given-names></name>
</person-group> (<year>2013</year>) <source>Relative effectiveness of conditional and unconditional cash transfers for schooling outcomes in developing countries: a systematic review</source>. <publisher-loc>London</publisher-loc>: <publisher-name>3ie</publisher-name>.</citation>
</ref>
<ref id="bibr5-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bamberger</surname><given-names>M</given-names></name>
<name><surname>Rao</surname><given-names>V</given-names></name>
<name><surname>Woolcock</surname><given-names>M</given-names></name>
</person-group> (<year>2010</year>) <article-title>Using mixed methods in monitoring and evaluation: experiences from international development</article-title>. In: <person-group person-group-type="editor">
<name><surname>Tashakkori</surname><given-names>A</given-names></name>
<name><surname>Teddlie</surname><given-names>C</given-names></name>
</person-group> (eds), <source>Handbook of Mixed Methods in Social and Behavioral Research</source>, <edition>2nd revised edition</edition>. <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>, <fpage>613</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr6-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Banerjee</surname><given-names>AV</given-names></name>
<name><surname>Cole</surname><given-names>S</given-names></name>
<name><surname>Duflo</surname><given-names>E</given-names></name>
<name><surname>Linden</surname><given-names>L</given-names></name>
</person-group> (<year>2007</year>) <article-title>Remedying education: evidence from two randomized experiments in India</article-title>. <source>Quarterly Journal of Economics</source> <volume>122</volume>(<issue>3</issue>): <fpage>1235</fpage>–<lpage>64</lpage>.</citation>
</ref>
<ref id="bibr7-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Barron</surname><given-names>P</given-names></name>
<name><surname>Diprose</surname><given-names>R</given-names></name>
<name><surname>Woolcock</surname><given-names>M</given-names></name>
</person-group> (<year>2011</year>) <source>Contesting Development: Participatory Projects and Local Conflict Dynamics in Indonesia</source>. <publisher-loc>New Haven, CT</publisher-loc>: <publisher-name>Yale University Press</publisher-name></citation>
</ref>
<ref id="bibr8-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bates</surname><given-names>R</given-names></name>
<name><surname>Greif</surname><given-names>A</given-names></name>
<name><surname>Levi</surname><given-names>M</given-names></name>
<name><surname>Rosenthal</surname><given-names>J-L</given-names></name>
<name><surname>Weingast</surname><given-names>BR</given-names></name>
</person-group> (<year>1998</year>) <source>Analytic Narratives</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr9-1356389013495210">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Bold</surname><given-names>T</given-names></name>
<name><surname>Kimenyi</surname><given-names>M</given-names></name>
<name><surname>Mwabu</surname><given-names>G</given-names></name>
<name><surname>Ng’ang’a</surname><given-names>A</given-names></name>
<name><surname>Sandefur</surname><given-names>J</given-names></name>
</person-group> (<year>2013</year>) <article-title>Scaling-up what works: experimental evidence on external validity in Kenyan education</article-title>. <source>Washington: Center for Global Development</source>, Working Paper No. 321.</citation>
</ref>
<ref id="bibr10-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Booth</surname><given-names>D</given-names></name>
</person-group> (<year>2011</year>) <source>Aid effectiveness: bring country ownership (and politics) back in</source>. <publisher-loc>London</publisher-loc>: <publisher-name>ODI</publisher-name> Working Paper No. 336.</citation>
</ref>
<ref id="bibr11-1356389013495210">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Bruhn</surname><given-names>M</given-names></name>
<name><surname>McKenzie</surname><given-names>D</given-names></name>
</person-group> (<year>2013</year>) <article-title>Using administrative data to evaluate municipal reforms: an evaluation of the impact of Minas Fácil Expresso</article-title>. <source>Washington: World Bank Policy Research</source> Working Paper No. 6368.</citation>
</ref>
<ref id="bibr12-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Byrne</surname><given-names>D</given-names></name>
</person-group> (<year>2013</year>) <article-title>Evaluating complex social interventions in a complex world</article-title>. <source>Evaluation</source> <volume>19</volume>(<issue>3</issue>).</citation>
</ref>
<ref id="bibr13-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Byrne</surname><given-names>D</given-names></name>
<name><surname>Callaghan</surname><given-names>G</given-names></name>
</person-group> (<year>2013</year>) <source>Complexity Theory and the Social Sciences: The State of the Art</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>.</citation>
</ref>
<ref id="bibr14-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cartwright</surname><given-names>N</given-names></name>
</person-group> (<year>2007</year>) <article-title>Are RCTs the gold standard?</article-title> <source>Biosocieties</source> <volume>2</volume>(<issue>2</issue>): <fpage>11</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr15-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Casey</surname><given-names>K</given-names></name>
<name><surname>Glennerster</surname><given-names>R</given-names></name>
<name><surname>Miguel</surname><given-names>E</given-names></name>
</person-group> (<year>2012</year>) <article-title>Reshaping institutions: evidence on aid impacts using a pre-analysis plan</article-title>. <source>Quarterly Journal of Economics</source> <volume>127</volume>(<issue>4</issue>): <fpage>1755</fpage>–<lpage>812</lpage>.</citation>
</ref>
<ref id="bibr16-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cartwright</surname><given-names>N</given-names></name>
<name><surname>Hardie</surname><given-names>J</given-names></name>
</person-group> (<year>2012</year>) <source>Evidence-Based Policy: A Practical Guide to Doing it Better</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Chong</surname><given-names>A</given-names></name>
<name><surname>La Porta</surname><given-names>R</given-names></name>
<name><surname>Lopez-de-Silanes</surname><given-names>F</given-names></name>
<name><surname>Shleifer</surname><given-names>A</given-names></name>
</person-group> (<year>2012</year>) <source>Letter grading government efficiency</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>NBER</publisher-name> Working Paper No. 18268.</citation>
</ref>
<ref id="bibr18-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Churchman</surname><given-names>CW</given-names></name>
</person-group> (<year>1967</year>) <article-title>Wicked problems</article-title>. <source>Management Science</source> <volume>14</volume>(<issue>4</issue>): <fpage>141</fpage>–<lpage>2</lpage>.</citation>
</ref>
<ref id="bibr19-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cook</surname><given-names>TD</given-names></name>
</person-group> (<year>2001</year>) <article-title>Generalization: conceptions in the social sciences</article-title>. In: <person-group person-group-type="editor">
<name><surname>Smelser</surname><given-names>NJ</given-names></name>
<name><surname>Wright</surname><given-names>J</given-names></name>
<name><surname>Baltes</surname><given-names>PB</given-names></name>
</person-group> (eds), <source>International Encyclopedia of the Social and Behavioral Sciences</source>, <volume>Vol. 9</volume>. <publisher-loc>Amsterdam</publisher-loc>: <publisher-name>Elsevier</publisher-name>, <fpage>6037</fpage>–<lpage>43</lpage>.</citation>
</ref>
<ref id="bibr20-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cook</surname><given-names>TD</given-names></name>
<name><surname>Campbell</surname><given-names>DT</given-names></name>
</person-group> (<year>1979</year>) <source>Quasi-Experimentation: Design and Analysis Issues for Field Settings</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Houghton Mifflin Company</publisher-name>.</citation>
</ref>
<ref id="bibr21-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Deaton</surname><given-names>A</given-names></name>
</person-group> (<year>2010</year>) <article-title>Instruments, randomization, and learning about development</article-title>. <source>Journal of Economic Perspectives</source> <volume>48</volume>(<month>June</month>): <fpage>424</fpage>–5<lpage>5</lpage>.</citation>
</ref>
<ref id="bibr22-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Denizer</surname><given-names>C</given-names></name>
<name><surname>Kaufmann</surname><given-names>D</given-names></name>
<name><surname>Kraay</surname><given-names>A</given-names></name>
</person-group> (<year>2012</year>) <source>Good projects or good countries? Macro and micro correlates of World Bank project performance</source>. <publisher-loc>Washington</publisher-loc>: <publisher-name>World Bank Policy Research</publisher-name> Working Papers No. 5646.</citation>
</ref>
<ref id="bibr23-1356389013495210">
<citation citation-type="other">
<person-group person-group-type="author">
<name><surname>Duflo</surname><given-names>E</given-names></name>
<name><surname>Dupas</surname><given-names>P</given-names></name>
<name><surname>Kremer</surname><given-names>M</given-names></name>
</person-group> (<year>2012</year>) <article-title>School governance, teacher incentives, and pupil-teacher ratios: experimental evidence from Kenyan primary schools</article-title>. <source>NBER</source> Working Paper No. 17939.</citation>
</ref>
<ref id="bibr24-1356389013495210">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Engber</surname><given-names>D</given-names></name>
</person-group> (<year>2011</year>) <article-title>The mouse trap (part I): the dangers of using one lab animal to study every disease</article-title>. <source>Slate</source>, <month>November</month> <day>15</day>. URL: <ext-link ext-link-type="uri" xlink:href="http://www.slate.com/articles/health_and_science/the_mouse_trap/2011/11/the_mouse_trap.html">http://www.slate.com/articles/health_and_science/the_mouse_trap/2011/11/the_mouse_trap.html</ext-link></citation>
</ref>
<ref id="bibr25-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eppstein</surname><given-names>MJ</given-names></name>
<name><surname>Horbar</surname><given-names>JD</given-names></name>
<name><surname>Buzas</surname><given-names>JS</given-names></name>
<name><surname>Kauffman</surname><given-names>S</given-names></name>
</person-group> (<year>2012</year>) <article-title>Searching the clinical fitness landscape</article-title>. <source>PLoS One</source> <volume>7</volume>(<issue>11</issue>): <fpage>e49901</fpage>.</citation>
</ref>
<ref id="bibr26-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Fiszbein</surname><given-names>A</given-names></name>
<name><surname>Schady</surname><given-names>N</given-names></name>
</person-group> (<year>2009</year>) <source>Conditional Cash Transfers: Reducing Present and Future Poverty</source>. <publisher-loc>Washington</publisher-loc>: <publisher-name>World Bank</publisher-name>.</citation>
</ref>
<ref id="bibr27-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Freedman</surname><given-names>DA</given-names></name>
</person-group> (<year>2008</year>) <article-title>On types of scientific enquiry: the role of qualitative reasoning</article-title>. In: <person-group person-group-type="editor">
<name><surname>Box-Steffensmeier</surname><given-names>J</given-names></name>
<name><surname>Brady</surname><given-names>HE</given-names></name>
<name><surname>Collier</surname><given-names>D</given-names></name>
</person-group> (eds), <source>The Oxford Handbook of Political Methodology</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>, <fpage>300</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr28-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>George</surname><given-names>A</given-names></name>
<name><surname>Bennett</surname><given-names>A</given-names></name>
</person-group> (<year>2005</year>) <source>Case Studies and Theory Development in the Social Sciences</source>. <publisher-loc>Cambridge, MA</publisher-loc>: <publisher-name>MIT Press</publisher-name>.</citation>
</ref>
<ref id="bibr29-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gerring</surname><given-names>J</given-names></name>
</person-group> (<year>2007</year>) <source>Case Study Research: Principles and Practices</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr30-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Goertz</surname><given-names>G</given-names></name>
<name><surname>Mahoney</surname><given-names>J</given-names></name>
</person-group> (<year>2012</year>) <source>A Tale of Two Cultures: Qualitative and Quantitative Research in the Social Sciences</source>. <publisher-loc>Princeton, NJ</publisher-loc>: <publisher-name>Princeton University Press</publisher-name>.</citation>
</ref>
<ref id="bibr31-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gupta</surname><given-names>A</given-names></name>
</person-group> (<year>2012</year>) <source>Red Tape: Bureaucracy, Structural Violence and Poverty in India</source>. <publisher-loc>Durham, MD and London</publisher-loc>: <publisher-name>Duke University Press</publisher-name>.</citation>
</ref>
<ref id="bibr32-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Henrich</surname><given-names>J</given-names></name>
<name><surname>Heine</surname><given-names>SJ</given-names></name>
<name><surname>Norenzayan</surname><given-names>A</given-names></name>
</person-group> (<year>2010a</year>) <article-title>The weirdest people in the world?</article-title> <source>Behavioral and Brain Sciences</source> <volume>33</volume>(<issue>2–3</issue>): <fpage>61</fpage>–<lpage>83</lpage>.</citation>
</ref>
<ref id="bibr33-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Henrich</surname><given-names>J</given-names></name>
<name><surname>Heine</surname><given-names>SJ</given-names></name>
<name><surname>Norenzayan</surname><given-names>A</given-names></name>
</person-group> (<year>2010b</year>) <article-title>Beyond WEIRD: towards a broad-based behavioral science</article-title>. <source>Behavioral and Brain Sciences</source> <volume>33</volume>(<issue>2–3</issue>): <fpage>111</fpage>–<lpage>35</lpage>.</citation>
</ref>
<ref id="bibr34-1356389013495210">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Honoré</surname><given-names>A</given-names></name>
</person-group> (<year>2010</year>) <article-title>Causation in the law</article-title>. <source>Stanford Encyclopedia of Philosophy</source>. URL (consulted <day>20</day> <month>March</month> <year>2013</year>): <ext-link ext-link-type="uri" xlink:href="http://stanford.library.usyd.edu.au/entries/causation-law/">http://stanford.library.usyd.edu.au/entries/causation-law/</ext-link></citation>
</ref>
<ref id="bibr35-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kolata</surname><given-names>G</given-names></name>
</person-group> (<year>2013</year>) <article-title>Mice fall short as test subjects for humans’ deadly ills</article-title>. <source>New York Times</source>, <day>11</day> <month>February</month>, p. <fpage>A19</fpage>.</citation>
</ref>
<ref id="bibr36-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lamont</surname><given-names>M</given-names></name>
</person-group> (<year>2012</year>) <article-title>Toward a comparative sociology of valuation and evaluation</article-title>. <source>Annual Review of Sociology</source> <volume>38</volume>: <fpage>201</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr37-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ludwig</surname><given-names>J</given-names></name>
<name><surname>Kling</surname><given-names>JR</given-names></name>
<name><surname>Mullainathan</surname><given-names>S</given-names></name>
</person-group> (<year>2011</year>) <article-title>Mechanism experiments and policy evaluations</article-title>. <source>Journal of Economic Perspectives</source> <volume>25</volume>(<issue>3</issue>): <fpage>17</fpage>–<lpage>38</lpage>.</citation>
</ref>
<ref id="bibr38-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mahoney</surname><given-names>J</given-names></name>
</person-group> (<year>2000</year>) <article-title>Strategies of causal inference in small-N analysis</article-title>. <source>Sociological Methods &amp; Research</source> <volume>28</volume>(<issue>4</issue>): <fpage>387</fpage>–<lpage>424</lpage>.</citation>
</ref>
<ref id="bibr39-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mahoney</surname><given-names>J</given-names></name>
</person-group> (<year>2007</year>) <article-title>Qualitative methodology and comparative politics</article-title>. <source>Comparative Political Studies</source> <volume>40</volume>(<issue>2</issue>): <fpage>122</fpage>–<lpage>44</lpage>.</citation>
</ref>
<ref id="bibr40-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Mansuri</surname><given-names>G</given-names></name>
<name><surname>Rao</surname><given-names>V</given-names></name>
</person-group> (<year>2012</year>) <source>Localizing Development: Does Participation Work?</source> <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>World Bank</publisher-name>.</citation>
</ref>
<ref id="bibr41-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Manzi</surname><given-names>J</given-names></name>
</person-group> (<year>2012</year>) <source>Uncontrolled: The Surprising Payoff of Trial and Error for Business, Politics, and Society</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Basic Books</publisher-name>.</citation>
</ref>
<ref id="bibr42-1356389013495210">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Muralidharan</surname><given-names>K</given-names></name>
<name><surname>Sundararaman</surname><given-names>V</given-names></name>
</person-group> (<year>2010</year>) <source>Contract teachers: experimental evidence from India</source>. <publisher-name>University of California</publisher-name>, <publisher-loc>San Diego: Mimeo</publisher-loc>. URL: <ext-link ext-link-type="uri" xlink:href="http://www.fas.nus.edu.sg/ecs/events/seminar/seminar-papers/31Aug10.pdf">http://www.fas.nus.edu.sg/ecs/events/seminar/seminar-papers/31Aug10.pdf</ext-link></citation>
</ref>
<ref id="bibr43-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pawson</surname><given-names>R</given-names></name>
</person-group> (<year>2006</year>) <source>Evidence-based Policy: A Realist Perspective</source>. <publisher-loc>London</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr44-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pawson</surname><given-names>R</given-names></name>
<name><surname>Tilly</surname><given-names>N</given-names></name>
</person-group> (<year>1997</year>) <source>Realist Evaluation</source> <publisher-loc>London</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
<ref id="bibr45-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Picciotto</surname><given-names>R</given-names></name>
</person-group> (<year>2012</year>) <article-title>Experimentalism and development evaluation: Will the bubble burst?</article-title> <source>Evaluation</source> <volume>18</volume>(<issue>2</issue>): <fpage>213</fpage>–<lpage>29</lpage>.</citation>
</ref>
<ref id="bibr46-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pritchett</surname><given-names>L</given-names></name>
</person-group> (<year>2013</year>) <source>The folk and the formula: fact and fiction in development</source>. <publisher-loc>Helsinki</publisher-loc>: <publisher-name>WIDER Annual Lecture</publisher-name> <fpage>16</fpage>.</citation>
</ref>
<ref id="bibr47-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pritchett</surname><given-names>L</given-names></name>
<name><surname>Woolcock</surname><given-names>M</given-names></name>
</person-group> (<year>2004</year>) <article-title>Solutions when the solution is the problem: arraying the disarray in development</article-title>. <source>World Development</source> <volume>32</volume>(<issue>2</issue>): <fpage>191</fpage>–<lpage>212</lpage>.</citation>
</ref>
<ref id="bibr48-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pritchett</surname><given-names>L</given-names></name>
<name><surname>Samji</surname><given-names>S</given-names></name>
<name><surname>Hammer</surname><given-names>J</given-names></name>
</person-group> (<year>2012</year>) <source>It’s all about MeE: using structured experiential learning (‘e’) to crawl the design space</source>. <publisher-loc>Helsinki</publisher-loc>: <publisher-name>UNU WIDER</publisher-name> Working Paper No. 2012/104.</citation>
</ref>
<ref id="bibr49-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pritchett</surname><given-names>L</given-names></name>
<name><surname>Woolcock</surname><given-names>M</given-names></name>
<name><surname>Andrews</surname><given-names>M</given-names></name>
</person-group> (<year>2013</year>) <article-title>Looking like a state: techniques of persistent failure in state capability for implementation</article-title>. <source>Journal of Development Studies</source> <volume>49</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr50-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ragin</surname><given-names>CC</given-names></name>
</person-group> (<year>1987</year>) <source>The Comparative Method: Moving Beyond Qualitative and Quantitative Strategies</source>. <publisher-loc>Berkeley and Los Angeles</publisher-loc>: <publisher-name>University of California Press</publisher-name>.</citation>
</ref>
<ref id="bibr51-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Ragin</surname><given-names>CC</given-names></name>
<name><surname>Becker</surname><given-names>H</given-names></name>
</person-group> (eds) (<year>1992</year>) <source>What is a Case? Exploring the Foundations of Social Inquiry</source>. <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr52-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ramalingam</surname><given-names>B</given-names></name>
<name><surname>Jones</surname><given-names>H</given-names></name>
</person-group> (with Reba T and Young J) (<year>2009</year>) <source>Exploring the science of complexity: ideas and implications for development and humanitarian efforts</source>. <publisher-loc>London</publisher-loc>: <publisher-name>ODI</publisher-name> Working Paper No. 285.</citation>
</ref>
<ref id="bibr53-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Rao</surname><given-names>A</given-names></name>
<name><surname>Woolcock</surname><given-names>M</given-names></name>
</person-group> (<year>forthcoming</year>) <source>But how generalizable is that? A framework for assessing the external validity of complex development interventions</source>. <publisher-name>World Bank</publisher-name>, <publisher-loc>mimeo</publisher-loc>.</citation>
</ref>
<ref id="bibr54-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ravallion</surname><given-names>M</given-names></name>
</person-group> (<year>2001</year>) <article-title>Growth, inequality and poverty: looking beyond averages</article-title>. <source>World Development</source> <volume>29</volume>(<issue>11</issue>): <fpage>1803</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr55-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ravallion</surname><given-names>M</given-names></name>
</person-group> (<year>2009</year>) <article-title>Should the randomistas rule?</article-title> <source>Economists’ Voice</source> <volume>6</volume>(<issue>2</issue>): <fpage>1</fpage>–<lpage>5</lpage>.</citation>
</ref>
<ref id="bibr56-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rothwell</surname><given-names>PM</given-names></name>
</person-group> (<year>2005</year>) <article-title>External validity of randomized controlled trials: ‘To whom do the results of this trial apply?’</article-title> <source>The Lancet</source> <volume>365</volume>: <fpage>82</fpage>–<lpage>93</lpage>.</citation>
</ref>
<ref id="bibr57-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sampson</surname><given-names>R</given-names></name>
</person-group> (<year>2013</year>) <article-title>The place of context: a theory and strategy for criminology’s hard problems</article-title>. <source>Criminology</source> <volume>51</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>31</lpage>.</citation>
</ref>
<ref id="bibr58-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Seok</surname><given-names>J</given-names></name>
<name><surname>Warren</surname><given-names>HS</given-names></name>
<name><surname>Cuenca</surname><given-names>AG</given-names></name>
<etal/>
</person-group>. (<year>2013</year>) <article-title>Genomic responses in mouse models poorly mimic human inflammatory diseases</article-title>. <source>Proceedings of the National Academy of Sciences</source> <volume>110</volume>(<issue>9</issue>): <fpage>3507</fpage>–<lpage>12</lpage>.</citation>
</ref>
<ref id="bibr59-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shaffer</surname><given-names>P</given-names></name>
</person-group> (<year>2011</year>) <article-title>Against excessive rhetoric in impact assessment: overstating the case for randomised controlled experiments</article-title>. <source>Journal of Development Studies</source> <volume>47</volume>(<issue>11</issue>): <fpage>1619</fpage>–<lpage>35</lpage>.</citation>
</ref>
<ref id="bibr60-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shanahan</surname><given-names>M</given-names></name>
</person-group> (<year>2008</year>) <article-title>Dynamical complexity in small-world networks of spiking neurons</article-title>. <source>Physical Review E</source> <volume>78</volume>(<issue>4</issue>): <fpage>041924</fpage>.</citation>
</ref>
<ref id="bibr61-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Stern</surname><given-names>E</given-names></name>
<name><surname>Stame</surname><given-names>N</given-names></name>
<name><surname>Mayne</surname><given-names>J</given-names></name>
<name><surname>Forss</surname><given-names>K</given-names></name>
<name><surname>Davies</surname><given-names>R</given-names></name>
<name><surname>Befani</surname><given-names>B</given-names></name>
</person-group> (<year>2012</year>) <source>Broadening the range of designs and methods for impact evaluation</source>. <publisher-loc>London</publisher-loc>: <publisher-name>DFID</publisher-name> Working Paper No. 38.</citation>
</ref>
<ref id="bibr62-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Woolcock</surname><given-names>M</given-names></name>
</person-group> (<year>2009</year>) <article-title>Toward a plurality of methods in project evaluation: a contextualized approach to understanding impact trajectories and efficacy</article-title>. <source>Journal of Development Effectiveness</source> <volume>1</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>14</lpage>.</citation>
</ref>
<ref id="bibr63-1356389013495210">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Woolcock</surname><given-names>M</given-names></name>
<name><surname>Szreter</surname><given-names>S</given-names></name>
<name><surname>Rao</surname><given-names>V</given-names></name>
</person-group> (<year>2011</year>) <article-title>How and why does history matter for development policy?</article-title> <source>Journal of Development Studies</source> <volume>47</volume>(<issue>1</issue>): <fpage>70</fpage>–<lpage>96</lpage>.</citation>
</ref>
<ref id="bibr64-1356389013495210">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Yin</surname><given-names>RK</given-names></name>
</person-group> (<year>2009</year>) <source>Case Study Research: Design and Methods</source>, <edition>4th edn.</edition> <publisher-loc>Thousand Oaks, CA</publisher-loc>: <publisher-name>SAGE</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>