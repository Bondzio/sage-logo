<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797612458802</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797612458802</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Lip Movements Affect Infants’ Audiovisual Speech Perception</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Yeung</surname><given-names>H. Henny</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Werker</surname><given-names>Janet F.</given-names></name>
</contrib>
<aff id="aff1-0956797612458802">University of British Columbia</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0956797612458802">H. Henny Yeung, Laboratoire Psychologie de la Perception, Université Paris Descartes, 45 rue des Saints-Pères, 75006 Paris, France E-mail: <email>henny.yeung@parisdescartes.fr</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2013</year>
</pub-date>
<volume>24</volume>
<issue>5</issue>
<fpage>603</fpage>
<lpage>612</lpage>
<history>
<date date-type="received">
<day>3</day>
<month>5</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>22</day>
<month>7</month>
<year>2012</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>Speech is robustly audiovisual from early in infancy. Here we show that audiovisual speech perception in 4.5-month-old infants is influenced by sensorimotor information related to the lip movements they make while chewing or sucking. Experiment 1 consisted of a classic audiovisual matching procedure, in which two simultaneously displayed talking faces (visual [i] and [u]) were presented with a synchronous vowel sound (audio /i/ or /u/). Infants’ looking patterns were selectively biased away from the audiovisual matching face when the infants were producing lip movements similar to those needed to produce the heard vowel. Infants’ looking patterns returned to those of a baseline condition (no lip movements, looking longer at the audiovisual matching face) when they were producing lip movements that did not match the heard vowel. Experiment 2 confirmed that these sensorimotor effects interacted with the heard vowel, as looking patterns differed when infants produced these same lip movements while seeing and hearing a talking face producing an unrelated vowel (audio /a/). These findings suggest that the development of speech perception and speech production may be mutually informative.</p>
</abstract>
<kwd-group>
<kwd>infant</kwd>
<kwd>multisensory</kwd>
<kwd>sensorimotor</kwd>
<kwd>audiovisual</kwd>
<kwd>speech perception</kwd>
<kwd>perceptual motor coordination</kwd>
<kwd>language development</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Speech perception is robustly multisensory from infancy, and audio and visual modalities are linked from very early in development. For example, when infants see a side-by-side display of two talking faces and hear a synchronously presented audio track, they spontaneously look more at the face that matches the audio track (<xref ref-type="bibr" rid="bibr15-0956797612458802">Kuhl &amp; Meltzoff, 1982</xref>, <xref ref-type="bibr" rid="bibr16-0956797612458802">1984</xref>; <xref ref-type="bibr" rid="bibr19-0956797612458802">Kuhl, Williams, &amp; Meltzoff, 1991</xref>; <xref ref-type="bibr" rid="bibr23-0956797612458802">MacKain, Studdert-Kennedy, Spieker, &amp; Stern, 1983</xref>; <xref ref-type="bibr" rid="bibr27-0956797612458802">Patterson &amp; Werker, 1999</xref>, <xref ref-type="bibr" rid="bibr28-0956797612458802">2003</xref>). Subsequent work has shown that this audiovisual (AV) matching exhibits unique electrophysiological signatures in the brain (<xref ref-type="bibr" rid="bibr3-0956797612458802">Bristow et al., 2009</xref>) and shapes the learning of phonetic categories (<xref ref-type="bibr" rid="bibr36-0956797612458802">Teinonen, Aslin, Alku, &amp; Csibra, 2008</xref>). As has been demonstrated in adults (<xref ref-type="bibr" rid="bibr26-0956797612458802">McGurk &amp; MacDonald, 1976</xref>), visual speech information can even alter how infants perceive auditory speech (<xref ref-type="bibr" rid="bibr4-0956797612458802">Burnham &amp; Dodd, 2004</xref>; <xref ref-type="bibr" rid="bibr20-0956797612458802">Kushnerenko, Teinonen, Volein, &amp; Csibra, 2008</xref>; <xref ref-type="bibr" rid="bibr31-0956797612458802">Rosenblum, Schmuckler, &amp; Johnson, 1997</xref>).</p>
<p>What mechanisms support this link between audio and visual modalities in infancy? AV speech linkages are unlikely to be learned associations between talking faces and speech sounds, as matching between nonnative speech and faces is also seen (<xref ref-type="bibr" rid="bibr29-0956797612458802">Pons, Lewkowicz, Soto-Faraco, &amp; Sebastián-Gallés, 2009</xref>; <xref ref-type="bibr" rid="bibr38-0956797612458802">Walton &amp; Bower, 1993</xref>). Another possibility is that infants detect amodal properties across modalities, and that this is the basis on which speech AV correspondences are detected from birth (<xref ref-type="bibr" rid="bibr22-0956797612458802">Lewkowicz, 2010</xref>). However, amodal properties like temporal synchrony are experimentally controlled in many studies in which infants nevertheless accomplish AV matching (<xref ref-type="bibr" rid="bibr15-0956797612458802">Kuhl &amp; Meltzoff, 1982</xref>, <xref ref-type="bibr" rid="bibr16-0956797612458802">1984</xref>; <xref ref-type="bibr" rid="bibr23-0956797612458802">MacKain et al., 1983</xref>; <xref ref-type="bibr" rid="bibr27-0956797612458802">Patterson &amp; Werker, 1999</xref>, <xref ref-type="bibr" rid="bibr28-0956797612458802">2003</xref>). An alternative hypothesis is that infants map speech information in auditory and visual modalities onto a common articulatory representation (<xref ref-type="bibr" rid="bibr13-0956797612458802">Kent &amp; Vorperian, 2007</xref>; <xref ref-type="bibr" rid="bibr16-0956797612458802">Kuhl &amp; Meltzoff, 1984</xref>, <xref ref-type="bibr" rid="bibr17-0956797612458802">1988</xref>). AV matching is facilitated for speech relative to nonspeech sounds (<xref ref-type="bibr" rid="bibr16-0956797612458802">Kuhl &amp; Meltzoff, 1984</xref>; <xref ref-type="bibr" rid="bibr19-0956797612458802">Kuhl et al., 1991</xref>), which suggests that the speech signal has domain-specific properties that map onto human faces. Infants also sometimes produce congruent oral gestures in AV speech contexts (<xref ref-type="bibr" rid="bibr15-0956797612458802">Kuhl &amp; Meltzoff, 1982</xref>, <xref ref-type="bibr" rid="bibr16-0956797612458802">1984</xref>, <xref ref-type="bibr" rid="bibr18-0956797612458802">1996</xref>; <xref ref-type="bibr" rid="bibr21-0956797612458802">Legerstee, 1990</xref>; <xref ref-type="bibr" rid="bibr27-0956797612458802">Patterson &amp; Werker, 1999</xref>).</p>
<p>This articulatory hypothesis is bolstered by other research suggesting that AV speech perception is linked with articulatory movements in adults (<xref ref-type="bibr" rid="bibr11-0956797612458802">Hickok &amp; Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bibr30-0956797612458802">Pulvermüller &amp; Fadiga, 2010</xref>). For example, perceiving either auditory or visual speech affects speech production (<xref ref-type="bibr" rid="bibr8-0956797612458802">Galantucci, Fowler, &amp; Goldstein, 2009</xref>; <xref ref-type="bibr" rid="bibr14-0956797612458802">Kerzel &amp; Bekkering, 2000</xref>), and producing articulatory movements has effects on auditory speech perception that are similar to the effects of seeing visual speech (<xref ref-type="bibr" rid="bibr33-0956797612458802">Sams, Möttönen, &amp; Sihvonen, 2005</xref>). Regarding development, however, previous research has shown only that perceiving speech affects early vocalizations: Native-language sound patterns affect newborns’ cries (<xref ref-type="bibr" rid="bibr24-0956797612458802">Mampe, Friederici, Christophe, &amp; Wermke, 2009</xref>), vowel-like utterances (<xref ref-type="bibr" rid="bibr32-0956797612458802">Ruzza, Rocca, Boero, &amp; Lenti, 2006</xref>) and babbling in infants (<xref ref-type="bibr" rid="bibr6-0956797612458802">de Boysson-Bardies, Sagart, &amp; Durand, 1984</xref>; <xref ref-type="bibr" rid="bibr40-0956797612458802">Whalen, Levitt, &amp; Goldstein, 2007</xref>), as well as the earliest word productions by toddlers (<xref ref-type="bibr" rid="bibr25-0956797612458802">McCune &amp; Vihman, 2001</xref>). Conversely, only one study has found correlations between production patterns and infants’ speech perception (<xref ref-type="bibr" rid="bibr7-0956797612458802">DePaolis, Vihman, &amp; Keren-Portnoy, 2011</xref>), and no studies have manipulated infants’ oral gestures while measuring effects on auditory, visual, or AV speech perception.</p>
<p>With few exceptions (<xref ref-type="bibr" rid="bibr2-0956797612458802">Best, 1995</xref>; <xref ref-type="bibr" rid="bibr17-0956797612458802">Kuhl &amp; Meltzoff, 1988</xref>; <xref ref-type="bibr" rid="bibr25-0956797612458802">McCune &amp; Vihman, 2001</xref>; <xref ref-type="bibr" rid="bibr37-0956797612458802">Vihman, 1993</xref>; <xref ref-type="bibr" rid="bibr39-0956797612458802">Werker, 1993</xref>), there has been little theoretical discussion about sensorimotor influences on the development of speech perception. This lack of discussion likely stems from the fact that infants begin perceiving sophisticated phonetic patterns long before their oral gestures can be classified as articulatory (i.e., speech related). For example, auditory language input may affect the production of early oral gestures, like babbling, which implies that such gestures are continuous with later word production and are thus speech related (<xref ref-type="bibr" rid="bibr25-0956797612458802">McCune &amp; Vihman, 2001</xref>). Other researchers argue that many aspects of infants’ babbling reflect universal constraints on the development of the motor system and are not specific to speech (<xref ref-type="bibr" rid="bibr5-0956797612458802">Davis &amp; MacNeilage, 1995</xref>). Moreover, the types of muscle movements made when infants and toddlers babble, suck, or chew do not appear to be continuous with mature speech motor control (<xref ref-type="bibr" rid="bibr35-0956797612458802">Steeve, Moore, Green, Reilly, &amp; McMurtrey, 2008</xref>).</p>
<p>In summary, the basis of infants’ rich AV speech sensitivities remains unclear, although articulatory information may play an important role. It is well established that articulatory information is linked to speech perception in adults, but it is not known whether a similar relation exists in infants, and if such a relation exists, how it could be related to infants’ relatively immature motor development. In two experiments, we tested whether very simple sensorimotor features of nonspeech oral gestures are related to AV speech processing.</p>
<sec id="section1-0956797612458802">
<title>Experiment 1</title>
<p>Experiment 1 relied on the similarity between the lip movements produced when adults articulate /i/ and /u/ and when 4.5-month-old infants engage in chewing and sucking. At 4.5 months, infants are not yet babbling or otherwise producing clear speech, and babies this age are commonly tested in AV speech procedures. A previous matching procedure was used: Two talking faces (either visual [i] or visual [u]) were displayed side by side while a synchronized audio track matching one of the faces (either audio /i/ or audio /u/) was presented for 2 min (<xref ref-type="bibr" rid="bibr1-0956797612458802">Baier, Idsardi, &amp; Lidz, 2007</xref>; <xref ref-type="bibr" rid="bibr15-0956797612458802">Kuhl &amp; Meltzoff, 1982</xref>, <xref ref-type="bibr" rid="bibr16-0956797612458802">1984</xref>; <xref ref-type="bibr" rid="bibr19-0956797612458802">Kuhl et al., 1991</xref>; <xref ref-type="bibr" rid="bibr27-0956797612458802">Patterson &amp; Werker, 1999</xref>, <xref ref-type="bibr" rid="bibr28-0956797612458802">2003</xref>).</p>
<p>The AV matching procedure was shown to some infants who had nothing in their mouths; this was the baseline group. Infants in two other groups produced lip movements that could be described as either /i/-like lip spreading or /u/-like lip rounding (<xref ref-type="fig" rid="fig1-0956797612458802">Fig. 1</xref>). The lip-sound match group produced lip movements that matched the heard vowel (and mismatched the competing vowel), and the lip-sound mismatch group produced lip movements that mismatched the heard vowel (and matched the competing vowel; see <xref ref-type="table" rid="table1-0956797612458802">Table 1</xref>).</p>
<fig id="fig1-0956797612458802" position="float">
<label>Fig. 1.</label>
<caption>
<p>Infants’ lip movements achieved in Experiments 1 and 2. Some of the infants chewed on a toy or finger oriented in such a way that their lips were repetitively spread; similar lip-spreading movements occur when an individual articulates the vowel /i/. Other infants sucked on a pacifier or fingertip, which ensured that their lips were repetitively rounded; similar lip-rounding movements occur when an individual articulates the vowel /u/.</p>
</caption>
<graphic xlink:href="10.1177_0956797612458802-fig1.tif"/>
</fig>
<table-wrap id="table1-0956797612458802" position="float">
<label>Table 1.</label>
<caption>
<p>Mean Proportion of Looking to the Visual [i] Face in Experiments 1 and 2</p>
</caption>
<graphic alternate-form-of="table1-0956797612458802" xlink:href="10.1177_0956797612458802-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Experiment, condition, and heard vowel</th>
<th align="center">Lip movement produced (spreading or rounding)</th>
<th align="center">Proportion of looking at visual [i]</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Experiment 1: baseline</td>
</tr>
<tr>
<td> /i/</td>
<td>Neither</td>
<td>.64 (.23)</td>
</tr>
<tr>
<td> /u/</td>
<td>Neither</td>
<td>.47 (.23)</td>
</tr>
<tr>
<td colspan="3">Experiment 1: lip-sound match</td>
</tr>
<tr>
<td> /i/</td>
<td>Lip spreading</td>
<td>.47 (.26)</td>
</tr>
<tr>
<td> /u/</td>
<td>Lip rounding</td>
<td>.62 (.28)</td>
</tr>
<tr>
<td colspan="3">Experiment 1: lip-sound mismatch</td>
</tr>
<tr>
<td> /i/</td>
<td>Lip rounding</td>
<td>.55 (.30)</td>
</tr>
<tr>
<td> /u/</td>
<td>Lip spreading</td>
<td>.42 (.17)</td>
</tr>
<tr>
<td colspan="3">Experiment 2</td>
</tr>
<tr>
<td> /a/</td>
<td>Lip spreading</td>
<td>.61 (.20)</td>
</tr>
<tr>
<td> /a/</td>
<td>Lip rounding</td>
<td>.52 (.21)</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0956797612458802">
<p>Note: Standard deviations are given in parentheses. <italic>n</italic> = 16 (in each row).</p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>If lip movements were related to AV matching, then results would be expected to differ across groups. One possible pattern is an articulatory assimilation effect, which would echo previous claims regarding adult speech processing (e.g., <xref ref-type="bibr" rid="bibr33-0956797612458802">Sams et al., 2005</xref>). According to this account, the lip-sound match group should activate motor features linked to corresponding audio and visual representations of the heard vowel, perhaps also suppressing competing representations. For example, lip spreading should activate motor features shared with audio /i/ and visual [i], facilitating /i/-[i] matching, whereas lip rounding should lead to the converse, facilitating /u/-[u] matching. The opposite prediction would apply to the lip-sound mismatch group, in which lip spreading should suppress features of audio /u/ and visual [u], impairing /u/-[u] matching, and lip rounding should impair /i/-[i] matching. In summary, evidence for an assimilation effect would be found if the lip-sound match group was similar to the baseline group in showing a bias toward the AV matching face, but the lip-sound mismatch group showed the converse pattern: a bias away from the AV matching face.</p>
<p>A second possibility is an articulatory contrast effect, which would echo theories of action perception from outside the speech domain. It is thought that common representations or processes are shared between perceptual and motor systems, and thus engaging motor processes can withhold related information from perceptual analysis, sometimes even biasing perceptual judgments in the opposite direction (<xref ref-type="bibr" rid="bibr10-0956797612458802">Hamilton, Wolpert, &amp; Frith, 2004</xref>; <xref ref-type="bibr" rid="bibr34-0956797612458802">Schütz-Bosbach &amp; Prinz, 2007</xref>). A contrast effect in the lip-sound match group would result in audiovisual /i/-[i] matching being impaired by lip spreading, and /u/-[u] matching being impaired by lip rounding. The opposite pattern would be predicted in the lip-sound mismatch group: Lip spreading should facilitate /u/-[u] matching, and lip rounding should facilitate /i/-[i] matching. In summary, if a contrast effect were present, the lip-sound match group would be dissimilar to the baseline group in showing a looking bias away from the AV matching face, and the lip-sound mismatch group would show the converse pattern: the same bias as the baseline group toward the AV match.</p>
<sec id="section2-0956797612458802">
<title>Method</title>
<sec id="section3-0956797612458802">
<title>Stimuli<sup><xref ref-type="fn" rid="fn1-0956797612458802">1</xref></sup></title>
<p>Two videos of faces saying [i] and [u] were used; the faces were of the same woman; the side of the screen on which each video appeared varied across infants. Each video was constructed from 10 clips of [i] or [u] articulations, the onsets of which were synchronized and occurred every 2 s. The duration of mouth opening and the onset of blinking were also synchronized, and these 10 clips were looped until each video played continuously for approximately 2 min.</p>
<p>Stimuli videos were presented with an audio track, which was recorded in a separate session by the same woman in the videos. To record the track, she spoke the sounds while watching the videos of herself articulating [i] and [u], to produce vowels timed as closely as possible with the original audio tracks. Ten tokens were used to create new tracks in which the vowel onsets were edited to synchronize with the onsets in the original tracks. The durations of mouth opening in the videos were longer than the durations of the vowel sounds (<italic>M</italic><sub>[i]</sub> = 1.36 s, <italic>M</italic><sub>[u]</sub> = 1.32 s, <italic>M</italic><sub>/i/</sub> = 0.44 s, <italic>M</italic><sub>/u/</sub> = 0.52 s), so as to resemble the temporal dynamics between face and voice in the original recordings.</p>
</sec>
<sec id="section4-0956797612458802">
<title>Procedure</title>
<p>Infants were seated in a caregiver’s lap while their eye gaze was recorded with a Tobii 1750 eye tracker (Tobii, Stockholm, Sweden) positioned 60 cm from the infants at an angle of 30°. Each face display covered a 9.8-cm × 9.8-cm square; the two squares were symmetrically oriented around the center of the screen and were separated horizontally by 2.7 cm. During the test video, sound pressure levels ranged between 60 and 64 dB, and sound emanated from two speakers behind a cardboard barrier surrounding the eye tracker. Each infant saw just one test video. An experimenter monitored the infants through a video feed.</p>
<p>The procedure began with gaze calibration, in which a blue ball appeared sequentially in the center and in the four corners of the screen, accompanied by beeping sounds. Calibration points were marked when infants appeared to fixate on the relevant locations. The experimental procedure began immediately after and closely followed previous paradigms (<xref ref-type="bibr" rid="bibr27-0956797612458802">Patterson &amp; Werker, 1999</xref>, <xref ref-type="bibr" rid="bibr28-0956797612458802">2003</xref>). This procedure began by showing three 9-s videos to inform infants that a different face would be appearing on each side of the screen (<xref ref-type="fig" rid="fig2-0956797612458802">Fig. 2</xref>). For example, an infant might see one face making the /u/ shape in silence for 9 s on the left side of the screen and then the other face making the /i/ shape in silence for 9 s on the right side of the screen. Both faces were then displayed together in silence for another 9 s, and finally the screen went blank for 3 s before the critical 2-min test movie of the two faces (in the same left-right positions) making the different vowel sounds was played, accompanied by one of the audio tracks. The side of the screen on which the first face appeared and whether it was producing an /i/ or /u/ shape were counterbalanced across infants.</p>
<fig id="fig2-0956797612458802" position="float">
<label>Fig. 2.</label>
<caption>
<p>Illustration of the procedure in Experiment 1. An infant was held in a caregiver’s lap facing a monitor. A face making either the /u/ lip shape (shown here) or the /i/ lip shape was silently displayed for 9 s, followed by the same face silently making the other lip shape for 9 s. Next, the two faces were displayed together in silence for another 9 s, and then the screen went blank for 3 s before a 2-min movie was played. The movie consisted of the two talking faces (visual [i] and visual [u]) displayed side by side (shown larger at the lower left, with a dashed gray line around the face making the [u] shape and a solid gray line around the face making the [i] shape), while a synchronized audio track matching one of the faces (either audio /i/ or audio /u/) was heard.</p>
</caption>
<graphic xlink:href="10.1177_0956797612458802-fig2.tif"/>
</fig>
<p>During the test procedure, lip spreaders chewed or mouthed a wide object and spread their lips to accommodate the object’s width. Most lip spreaders (<italic>n</italic> = 22) were given a wooden teething ring (Camden Rose, Ann Arbor, MI) provided by the experimenter (1.2 cm in thickness and 6.8 cm in diameter), but a few infants preferred another commercially available teething toy (<italic>n</italic> = 6), their caregiver’s horizontally oriented finger (<italic>n</italic> = 2), or a combination of any of these objects at different points during the test period (<italic>n</italic> = 2). Lip rounders sucked on a pacifier (<italic>n</italic> = 28), the tip of their caregiver’s finger (<italic>n</italic> = 3), or a combination of the finger and a pacifier (<italic>n</italic> = 1) at different points during the test.</p>
<p>Caregivers were instructed to attend to their baby and to avoid fixating on the visual display. Caregivers in the baseline group were also instructed to prevent their infants from chewing on any hands or clothing. Caregivers in the other groups were instructed to prevent the finger, object, or pacifier from being spit out. When an object was not in their infant’s mouth, caregivers were asked to adjust or replace it immediately. Clean teething rings or pacifiers were available under the caregiver’s chair for this purpose.</p>
</sec>
<sec id="section5-0956797612458802">
<title>Participants</title>
<p>The analysis included 96 infants (48 female, 48 male) with an average age of 4 months 18 days (range = 4 months 0 days to 5 months 3 days). According to parental report, all the infants heard English at least 30% of the time (<italic>M =</italic> 89%, range = 30%–100%). They were randomly assigned to the baseline group (nothing in the mouth), the lip-matching group, or the lip-mismatching group (32 per group). Infants were occasionally reassigned to a different group if they refused to chew or suck, or to allow gender to be balanced across experimental groups. Ten additional infants were tested but were excluded because of experimenter error or equipment failure. Another 34 infants were tested but were excluded on the basis of three a priori criteria derived from preliminary gaze analysis: Infants were excluded if fewer than 4 calibration points could be recorded (<italic>n</italic> = 2); if recorded gaze was less than 40 s during the 2-min test video (i.e., a third of the total), which happened when infants were excessively fussy or disinterested, or if their position shifted so that the eye tracker was unsuccessful at calculating gaze (<italic>n</italic> = 28); or if infants looked for less than 1 s at one of the faces, which demonstrated a bias for one side or the other (<italic>n</italic> = 4). This latter criterion was based on the assumption that these infants had trouble disengaging from one face, as in previous reports (<xref ref-type="bibr" rid="bibr16-0956797612458802">Kuhl &amp; Meltzoff, 1984</xref>; <xref ref-type="bibr" rid="bibr27-0956797612458802">Patterson &amp; Werker, 1999</xref>, <xref ref-type="bibr" rid="bibr28-0956797612458802">2003</xref>).</p>
</sec>
</sec>
<sec id="section6-0956797612458802">
<title>Results</title>
<p>Gaze analysis was conducted in the two regions of interest (i.e., the areas of the two faces, illustrated by the gray boxes in <xref ref-type="fig" rid="fig2-0956797612458802">Fig. 2</xref>) without applying any fixation filters or interpolative calculations. Total gaze was entered into an omnibus analysis of variance (ANOVA) with between-subjects factors of experimental group (baseline, lip-sound match, lip-sound mismatch), gender (male, female), vowel (heard /i/, heard /u/), and side of the AV match (left, right). No significant effects were found (α = .05). Looking at the faces was captured by the eye tracker for an average of 81.03 s (<italic>SD</italic> = 21.94 s) during the 2-min test period. In the remaining time, gaze could not be localized, or infants were looking at other regions of the screen.</p>
<p>The proportion of time infants spent looking at each face was then calculated (see <xref ref-type="table" rid="table1-0956797612458802">Table 1</xref>). The proportion spent looking at the AV matching face was entered as a dependent variable into an omnibus ANOVA with the same between-subjects factors previously described. Results showed only a main effect of experimental group (see <xref ref-type="fig" rid="fig3-0956797612458802">Fig. 3</xref>), <italic>F</italic>(2, 72) = 3.46, <italic>p</italic> = .037, η<sup>2</sup> = .088 (α = .05). Corrected post hoc comparisons (two-tailed, Fisher’s least significant difference) showed that infants in the baseline group looked more at the AV matching face (<italic>M</italic> = .58, <italic>SD</italic> = .23), compared with infants in the lip-sound match group (<italic>M</italic> = .43, <italic>SD</italic> = .27), <italic>t</italic>(72) = 2.38, <italic>p</italic> = .020, 95% confidence interval of the mean difference (CI) = [.03, .28]. Infants in the lip-sound mismatch group also looked more at the AV match (<italic>M</italic> = .57, <italic>SD</italic> = .24), compared with infants in the lip-sound match group, <italic>t</italic>(72) = 2.17, <italic>p</italic> = .033, 95% CI = [.01, .27]. However, infants in the baseline and lip-sound mismatch groups looked at the AV match for the same amount of time, <italic>t</italic>(72) = 0.21, <italic>p</italic> = .84, 95% CI = [−0.12, 0.14].</p>
<fig id="fig3-0956797612458802" position="float">
<label>Fig. 3.</label>
<caption>
<p>Results for Experiment 1: proportion of time spent looking at the face that was an audiovisual (AV) match as a function of experimental group. Error bars indicate standard errors, and asterisks indicate significant differences between groups (<italic>p</italic> &lt; .05). The dashed line indicates the level of chance performance.</p>
</caption>
<graphic xlink:href="10.1177_0956797612458802-fig3.tif"/>
</fig>
</sec>
<sec id="section7-0956797612458802">
<title>Discussion</title>
<p>Our results revealed two patterns. First, looking differed from baseline when lip movements matched the heard vowel, and second, looking remained unchanged from baseline when lip movements mismatched the heard vowel. This first pattern suggests an articulatory contrast effect, as infants <italic>suppressed</italic> critical speech representations when producing matching lip movements. The second pattern suggests no facilitation effect, as suppressing the competing vowel did not affect AV matching. This lack of facilitation could suggest that vowel representations were not in competition in this task, or simply that the proportions of matching were at ceiling (cf. <xref ref-type="bibr" rid="bibr1-0956797612458802">Baier et al., 2007</xref>).</p>
<p>What specific processes explain the contrast effect? One possibility is that sensorimotor input biases visual preferences away from faces making lip movements similar to produced lip shapes. In other words, infants in both the lip-sound match and the lip-sound mismatch groups may have preferred the dissimilar facial expression irrespective of what vowel was presented. Although facial-expression matching might be powerful enough to override AV matching, this account does not necessarily suggest that sensorimotor and AV information interact.</p>
<p>A second possibility is that the observed effect reflects an AV interaction. In this account, motor information suppressed the ability of infants in the lip-sound match group to match audio and visual representations of the heard vowel, perhaps increasing activation of the competing representation. This resulted in a bias away from the AV matching face in this group, compared with the baseline group. For the lip-sound mismatch group, motor information selectively suppressed information about the competing vowel, which resulted in performance similar to that of the baseline group. Thus, motor information may selectively interact with AV speech processing only when it is aligned with both auditory and visual modalities.</p>
</sec>
</sec>
<sec id="section8-0956797612458802">
<title>Experiment 2</title>
<p>Experiment 2 was conducted to distinguish between the two possible interpretations of the effect seen in Experiment 1. In the same test procedure, another group of infants was prompted to make lip movements, except that the presented vowel (audio /a/) was neutral with respect to both the infants’ lip movements and the talking faces’ lip movements (see <xref ref-type="table" rid="table1-0956797612458802">Table 1</xref>). According to the first account (i.e., facial-expression matching), infants in this condition should continue to avoid the face matching the lip shape they themselves produce (i.e., the <italic>lip-matching face</italic>). According to the second account (i.e., AV-motor interactions), infants in this condition should show a different pattern from the infants in Experiment 1, as the auditory information from the vowel presented in this case (audio /a/) is unrelated to either infants’ lip movements or the faces’ lip movements.</p>
<sec id="section9-0956797612458802">
<title>Method</title>
<sec id="section10-0956797612458802">
<title>Stimuli</title>
<p>Stimuli from Experiment 1 were used, except the audio track contained 10 tokens of the vowel /a/. Tokens were recorded by the same speaker in the same manner as the original /i/ and /u/ vowels (see <xref ref-type="bibr" rid="bibr1-0956797612458802">Baier et al., 2007</xref>) and had correspondingly similar durations (<italic>M</italic><sub>/a/</sub> = 0.44 s). Tokens were again placed at the onsets of the original audio track to create the videos.</p>
</sec>
<sec id="section11-0956797612458802">
<title>Procedure</title>
<p>The procedure from Experiment 1 was used. Lip spreaders were given the same teething ring as before (<italic>n</italic> = 14), except that 1 preferred another commercially available teething toy, and 1 preferred a combination of the ring and a horizontally oriented finger. Lip rounders sucked on a pacifier (<italic>n</italic> = 14), except for 2 infants who preferred the tip of their caregiver’s finger.</p>
</sec>
<sec id="section12-0956797612458802">
<title>Participants</title>
<p>The analysis included 32 infants (16 female, 16 male) with an average age of 4 months 20 days (range = 4 months 4 days to 5 months 12 days). According to parental report, all the infants heard English at least 30% of the time (<italic>M</italic> = 87%, range = 50%–100%). Five additional infants were tested but excluded because of experimenter error (<italic>n</italic> = 2) or because they did not meet the criterion of hearing English at least 30% of the time (<italic>n</italic> = 3). Twenty-six<sup><xref ref-type="fn" rid="fn2-0956797612458802">2</xref></sup> others were excluded on the basis of the a priori criteria from Experiment 1: Their recorded gaze was less than 40 s (<italic>n</italic> = 25), or a bias for a side was observed (<italic>n</italic> = 1). (No infants had to be excluded because fewer than four calibration points were recorded.)</p>
</sec>
</sec>
<sec id="section13-0956797612458802">
<title>Results</title>
<p>Data from the infants in Experiment 2 (see <xref ref-type="table" rid="table1-0956797612458802">Table 1</xref>) were analyzed along with data from the infants in Experiment 1 who produced lip movements (lip spreading or lip rounding). Total gaze time was entered into an omnibus ANOVA with between-subjects factors of experiment (Experiment 1, Experiment 2), gender (male, female), lip shape (lip spreading, lip rounding), and side of presentation of the lip-matching face (left, right). No significant effects were found (α = .05), and overall looking at the faces averaged 82.34 s (<italic>SD</italic> = 20.30 s) during the 2-min test period.</p>
<p>To explicitly test the facial-expression-matching hypothesis, we conducted an omnibus ANOVA on the proportion of time looking at the lip-matching face (the face that matched the lip shapes produced by infants themselves); this analysis included the same between-subjects factors described in the previous paragraph. Results (see <xref ref-type="fig" rid="fig4-0956797612458802">Fig. 4</xref>) showed only a main effect of experiment, <italic>F</italic>(1, 80) = 4.77, <italic>p</italic> = .032, η<sup>2</sup> = .056 (α = .05): Infants in Experiment 1 looked significantly less at the lip-matching face (<italic>M</italic> = .43, <italic>SD</italic> = .25), compared with infants in Experiment 2 (<italic>M</italic> = .54, <italic>SD</italic> = .21). This finding provides evidence against the facial-expression matching hypothesis, as there was not a global tendency to look at the facial expression matching the lip shapes produced by the infants. Instead, the results suggest that infants’ looking patterns are influenced by interactions between both their lip movements and the heard vowel. Results from Experiment 1 were thus due to an interaction between the motor and AV speech processes.</p>
<fig id="fig4-0956797612458802" position="float">
<label>Fig. 4.</label>
<caption>
<p>Proportion of time infants in Experiments 1 and 2 spent looking at the lip-matching face (the face that matched the lip shapes produced by infants themselves). For Experiment 1, results are shown only for infants who produced lip movements. Infants in Experiment 2 produced the same lip shapes but heard the neutral sound /a/. Error bars indicate standard errors, and the asterisk indicates a significant difference between groups (<italic>p</italic> &lt; .05). The dashed line indicates the level of chance performance.</p>
</caption>
<graphic xlink:href="10.1177_0956797612458802-fig4.tif"/>
</fig>
</sec>
<sec id="section14-0956797612458802">
<title>Discussion</title>
<p>Infants in Experiment 2 achieved lip shapes identical to those in Experiment 1 (lip spreading and lip rounding), but a neutral vowel (audio /a/) rather than a vowel matching one of the displayed faces (audio /i/ or audio /u/) was played. Unlike the infants in Experiment 1, infants in Experiment 2 did not show a bias toward the visual face mismatching the infants’ achieved lip shape. This indicates that the results from Experiment 1 cannot be explained by facial-expression matching without any interaction with the heard vowel.</p>
</sec>
</sec>
<sec id="section15-0956797612458802" sec-type="discussion">
<title>General Discussion</title>
<p>Our findings reveal that sensorimotor information is directly implicated in AV speech processing from early in infancy. Looking patterns observed in Experiment 1 reflect a selective bias away from the AV matching face in the lip-sound match group, and a return to baseline looking (back toward the AV matching face) in the lip-sound mismatch group. Experiment 2 confirms that these results reflect an AV-motor interaction and not simple facial-expression matching.</p>
<p>The observed contrast effect differs from what is typically obtained in adult speech research, which often shows that speech information is biased toward adults’ articulations (i.e., an assimilation effect). However, our results are compatible with the literature showing both assimilation and contrast effects in visual perception and bodily action (e.g., <xref ref-type="bibr" rid="bibr10-0956797612458802">Hamilton et al., 2004</xref>; <xref ref-type="bibr" rid="bibr34-0956797612458802">Schütz-Bosbach &amp; Prinz, 2007</xref>). For example, executing certain arm movements (i.e., drawing rising arcs) while viewing related visual displays (i.e., dots moving in arc-shaped trajectories) biases perceptual identification away from visual features shared with the performed actions (i.e., dots appear to move in flatter arcs; <xref ref-type="bibr" rid="bibr9-0956797612458802">Grosjean, Zwickel, &amp; Prinz, 2009</xref>). When such effects are observed in adults, it is hypothesized that shared information between perception and action is withheld (or inhibited) in perceptual analysis (<xref ref-type="bibr" rid="bibr9-0956797612458802">Grosjean et al., 2009</xref>), as judgments are biased toward perceptual hypotheses that do not recruit the same features as the performed action (<xref ref-type="bibr" rid="bibr10-0956797612458802">Hamilton et al., 2004</xref>). Lip spreading or lip rounding could have a similar effect on AV speech, biasing perceptual preferences toward the contrasting vowel.</p>
<p>Varying task demands can alternately elicit assimilation and contrast effects (<xref ref-type="bibr" rid="bibr9-0956797612458802">Grosjean et al., 2009</xref>), which suggests that the pattern reported here might similarly vary as a function of task. Results may also vary as a function of developmental level: As infants develop mastery over more-complex articulatory schemas and achieve closer approximations to what adults are doing, contrast effects may prove to be unstable, perhaps disappearing at one age and later reappearing as assimilation effects at another.</p>
<p>Our results indicate that coarse-grained sensorimotor information about the articulators (e.g., lip rounding, lip spreading, jaw opening) is available to perceptual systems processing AV speech. Further work is needed to determine the precise format of this information: Is it amodal, based on gestural events (<xref ref-type="bibr" rid="bibr2-0956797612458802">Best, 1995</xref>), or is it somatosensory, based on feedback from skin receptors about oral gestures (<xref ref-type="bibr" rid="bibr12-0956797612458802">Ito, Tiede, &amp; Ostry, 2009</xref>)? Moreover, would such effects generalize to speech events on smaller time scales (i.e., consonants instead of vowels)? Our intuition is that as sensorimotor features become embedded in richer and more coordinated gestural movements, their effects on behavior likely become increasingly restricted to perceptual events that more closely resemble speechlike gestures (see also <xref ref-type="bibr" rid="bibr2-0956797612458802">Best, 1995</xref>). We further hypothesize that this development may similarly track the development of speech motor control, in which patterns of muscle activity begin to distinguish babbling from chewing and sucking from at least 9 months of age, and then speech from other oral gestures from around 15 months of age (<xref ref-type="bibr" rid="bibr35-0956797612458802">Steeve et al., 2008</xref>).</p>
<p>The established view in developmental research is that sensorimotor information has little influence on the perceptual development of speech in the 1st year. Several reports have suggested that speech perception is correlated with and guided by indirect feedback from an infant’s babbles or vocal motor schemes (<xref ref-type="bibr" rid="bibr7-0956797612458802">DePaolis et al., 2011</xref>; <xref ref-type="bibr" rid="bibr25-0956797612458802">McCune &amp; Vihman, 2001</xref>; <xref ref-type="bibr" rid="bibr37-0956797612458802">Vihman, 1993</xref>), but no work had directly manipulated sensorimotor information in infants to see whether this can influence on-line behavior in a speech-perception task. We report such a manipulation, and our results show that even nonspeech oral gestures, like chewing and sucking, can be linked to AV speech perception. These results provide striking evidence of how linkages between speech perception and production may be established earlier in development than previously hypothesized, even before speech gestures are clearly produced.</p>
<p>Further work must be done to determine whether this sensorimotor information is specifically implicated in the mapping between auditory and visual modalities, or whether these effects act within either modality (or both modalities). Nevertheless, our results suggest that the development of sensorimotor systems could be pivotal in explaining why developmental changes happen when they do in AV speech perception (<xref ref-type="bibr" rid="bibr29-0956797612458802">Pons et al., 2009</xref>), and perhaps even in auditory speech perception in general.</p>
</sec>
</body>
<back>
<ack>
<p>We thank Neda Rezaz-Rahmati and Priya Kandahadai for helping collect data, as well as Eric Bateson, Jim Enns, Bryan Gick, Ali Greuel, David Lewkowicz, and Athena Vouloumanos for commenting on the manuscript.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This work was supported by grants to J. F. W. from the Natural Sciences and Engineering Research Council of Canada and the James S. McDonnell Foundation and by a fellowship to H. H. Y. from the Fondation Fyssen.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-0956797612458802">
<label>1.</label>
<p>Rebecca Baier, Bill Idsardi, and Jeff Lidz generously offered these stimuli.</p>
</fn>
<fn fn-type="other" id="fn2-0956797612458802">
<label>2.</label>
<p>Attrition was higher in Experiment 2 than in Experiment 1 because all infants were obliged to either suck or chew in Experiment 2, but not in Experiment 1 (i.e., the baseline condition).</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797612458802">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Baier</surname><given-names>R.</given-names></name>
<name><surname>Idsardi</surname><given-names>W. J.</given-names></name>
<name><surname>Lidz</surname><given-names>J.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Two-month-olds are sensitive to lip rounding in dynamic and static speech events</article-title>. In <person-group person-group-type="editor">
<name><surname>Vroomen</surname><given-names>J.</given-names></name>
<name><surname>Swerts</surname><given-names>M.</given-names></name>
<name><surname>Krahmer</surname><given-names>E.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the International Conference on Auditory-Visual Speech Processing</source>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.isca-speech.org/archive_open/archive_papers/avsp07/av07_L6-2.pdf">http://www.isca-speech.org/archive_open/archive_papers/avsp07/av07_L6-2.pdf</ext-link></citation>
</ref>
<ref id="bibr2-0956797612458802">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Best</surname><given-names>C. T.</given-names></name>
</person-group> (<year>1995</year>). <article-title>A direct realist view of cross-language speech perception</article-title>. In <person-group person-group-type="editor">
<name><surname>Strange</surname><given-names>W.</given-names></name>
</person-group> (Ed.), <source>Speech perception and linguistic experience: Issues in cross-language research</source> (pp. <fpage>171</fpage>–<lpage>204</lpage>). <publisher-loc>Timonium, MD</publisher-loc>: <publisher-name>York Press</publisher-name>.</citation>
</ref>
<ref id="bibr3-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bristow</surname><given-names>D.</given-names></name>
<name><surname>Dehaene-Lambertz</surname><given-names>G.</given-names></name>
<name><surname>Mattout</surname><given-names>J.</given-names></name>
<name><surname>Soares</surname><given-names>C.</given-names></name>
<name><surname>Gliga</surname><given-names>T.</given-names></name>
<name><surname>Baillet</surname><given-names>S.</given-names></name>
<name><surname>Mangin</surname><given-names>J.-F.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Hearing faces: How the infant brain matches the face it sees with the speech it hears</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>21</volume>, <fpage>905</fpage>–<lpage>921</lpage>.</citation>
</ref>
<ref id="bibr4-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Burnham</surname><given-names>D.</given-names></name>
<name><surname>Dodd</surname><given-names>B.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Auditory-visual speech integration by prelinguistic infants: Perception of an emergent consonant in the McGurk effect</article-title>. <source>Developmental Psychobiology</source>, <volume>45</volume>, <fpage>204</fpage>–<lpage>220</lpage>.</citation>
</ref>
<ref id="bibr5-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Davis</surname><given-names>B. L.</given-names></name>
<name><surname>MacNeilage</surname><given-names>P. F.</given-names></name>
</person-group> (<year>1995</year>). <article-title>The articulatory basis of babbling</article-title>. <source>Journal of Speech and Hearing Research</source>, <volume>38</volume>, <fpage>1199</fpage>–<lpage>1211</lpage>.</citation>
</ref>
<ref id="bibr6-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>de Boysson-Bardies</surname><given-names>B.</given-names></name>
<name><surname>Sagart</surname><given-names>L.</given-names></name>
<name><surname>Durand</surname><given-names>C.</given-names></name>
</person-group> (<year>1984</year>). <article-title>Discernible differences in the babbling of infants according to target language</article-title>. <source>Journal of Child Language</source>, <volume>11</volume>, <fpage>1</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr7-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>DePaolis</surname><given-names>R. A.</given-names></name>
<name><surname>Vihman</surname><given-names>M. M.</given-names></name>
<name><surname>Keren-Portnoy</surname><given-names>T.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Do production patterns influence the processing of speech in prelinguistic infants?</article-title> <source>Infant Behavior &amp; Development</source>, <volume>34</volume>, <fpage>590</fpage>–<lpage>601</lpage>.</citation>
</ref>
<ref id="bibr8-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Galantucci</surname><given-names>B.</given-names></name>
<name><surname>Fowler</surname><given-names>C. A.</given-names></name>
<name><surname>Goldstein</surname><given-names>L. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Perceptuomotor compatibility effects in speech</article-title>. <source>Attention, Perception, &amp; Psychophysics</source>, <volume>71</volume>, <fpage>1138</fpage>–<lpage>1149</lpage>.</citation>
</ref>
<ref id="bibr9-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Grosjean</surname><given-names>M.</given-names></name>
<name><surname>Zwickel</surname><given-names>J.</given-names></name>
<name><surname>Prinz</surname><given-names>W.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Acting while perceiving: Assimilation precedes contrast</article-title>. <source>Psychological Research</source>, <volume>73</volume>, <fpage>3</fpage>–<lpage>13</lpage>.</citation>
</ref>
<ref id="bibr10-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hamilton</surname><given-names>A.</given-names></name>
<name><surname>Wolpert</surname><given-names>D.</given-names></name>
<name><surname>Frith</surname><given-names>U.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Your own action influences how you perceive another person’s action</article-title>. <source>Current Biology</source>, <volume>14</volume>, <fpage>493</fpage>–<lpage>498</lpage>.</citation>
</ref>
<ref id="bibr11-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hickok</surname><given-names>G.</given-names></name>
<name><surname>Poeppel</surname><given-names>D.</given-names></name>
</person-group> (<year>2007</year>). <article-title>The cortical organization of speech processing</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>8</volume>, <fpage>393</fpage>–<lpage>402</lpage>.</citation>
</ref>
<ref id="bibr12-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ito</surname><given-names>T.</given-names></name>
<name><surname>Tiede</surname><given-names>M.</given-names></name>
<name><surname>Ostry</surname><given-names>D. J.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Somatosensory function in speech perception</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>106</volume>, <fpage>1245</fpage>–<lpage>1248</lpage>.</citation>
</ref>
<ref id="bibr13-0956797612458802">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kent</surname><given-names>R. D.</given-names></name>
<name><surname>Vorperian</surname><given-names>H. K.</given-names></name>
</person-group> (<year>2007</year>). <article-title>In the mouths of babes: Anatomic, motor, and sensory foundations of speech development in children</article-title>. In <person-group person-group-type="editor">
<name><surname>Paul</surname><given-names>R.</given-names></name>
</person-group> (Ed.), <source>Language disorders from a developmental perspective: Essays in honor of Robin S. Chapman</source> (pp. <fpage>55</fpage>–<lpage>81</lpage>). <publisher-loc>Mahwah, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr14-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kerzel</surname><given-names>D.</given-names></name>
<name><surname>Bekkering</surname><given-names>H.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Motor activation from visible speech: Evidence from stimulus response compatibility</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>26</volume>, <fpage>634</fpage>–<lpage>647</lpage>.</citation>
</ref>
<ref id="bibr15-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhl</surname><given-names>P. K.</given-names></name>
<name><surname>Meltzoff</surname><given-names>A. N.</given-names></name>
</person-group> (<year>1982</year>). <article-title>The bimodal perception of speech in infancy</article-title>. <source>Science</source>, <volume>218</volume>, <fpage>1138</fpage>–<lpage>1141</lpage>.</citation>
</ref>
<ref id="bibr16-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhl</surname><given-names>P. K.</given-names></name>
<name><surname>Meltzoff</surname><given-names>A. N.</given-names></name>
</person-group> (<year>1984</year>). <article-title>The intermodal representation of speech in infants</article-title>. <source>Infant Behavior &amp; Development</source>, <volume>7</volume>, <fpage>361</fpage>–<lpage>381</lpage>.</citation>
</ref>
<ref id="bibr17-0956797612458802">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kuhl</surname><given-names>P. K.</given-names></name>
<name><surname>Meltzoff</surname><given-names>A. N.</given-names></name>
</person-group> (<year>1988</year>). <article-title>Speech as an intermodal object of perception</article-title>. In <person-group person-group-type="editor">
<name><surname>Yonas</surname><given-names>A.</given-names></name>
</person-group> (Ed.), <source>The Minnesota Symposium on Child Development: Vol. 20. Perceptual development in infancy</source> (pp. <fpage>235</fpage>–<lpage>266</lpage>). <publisher-loc>Hillsdale, NJ</publisher-loc>: <publisher-name>Erlbaum</publisher-name>.</citation>
</ref>
<ref id="bibr18-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhl</surname><given-names>P. K.</given-names></name>
<name><surname>Meltzoff</surname><given-names>A. N.</given-names></name>
</person-group> (<year>1996</year>). <article-title>Infant vocalizations in response to speech: Vocal imitation and developmental change</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>100</volume>, <fpage>2425</fpage>–<lpage>2438</lpage>.</citation>
</ref>
<ref id="bibr19-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kuhl</surname><given-names>P. K.</given-names></name>
<name><surname>Williams</surname><given-names>K. A.</given-names></name>
<name><surname>Meltzoff</surname><given-names>A. N.</given-names></name>
</person-group> (<year>1991</year>). <article-title>Cross-modal speech perception in adults and infants using nonspeech auditory stimuli</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>17</volume>, <fpage>829</fpage>–<lpage>840</lpage>.</citation>
</ref>
<ref id="bibr20-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kushnerenko</surname><given-names>E.</given-names></name>
<name><surname>Teinonen</surname><given-names>T.</given-names></name>
<name><surname>Volein</surname><given-names>A.</given-names></name>
<name><surname>Csibra</surname><given-names>G.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Electrophysiological evidence of illusory audiovisual speech percept in human infants</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>105</volume>, <fpage>11442</fpage>–<lpage>11445</lpage>.</citation>
</ref>
<ref id="bibr21-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Legerstee</surname><given-names>M.</given-names></name>
</person-group> (<year>1990</year>). <article-title>Infant use of multimodal information to imitate speech sounds</article-title>. <source>Infant Behavior &amp; Development</source>, <volume>13</volume>, <fpage>343</fpage>–<lpage>354</lpage>.</citation>
</ref>
<ref id="bibr22-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lewkowicz</surname><given-names>D. J.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Infant perception of audio-visual speech synchrony</article-title>. <source>Developmental Psychology</source>, <volume>46</volume>, <fpage>66</fpage>–<lpage>77</lpage>.</citation>
</ref>
<ref id="bibr23-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>MacKain</surname><given-names>K.</given-names></name>
<name><surname>Studdert-Kennedy</surname><given-names>M.</given-names></name>
<name><surname>Spieker</surname><given-names>S.</given-names></name>
<name><surname>Stern</surname><given-names>D.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Infant intermodal speech perception is a left- hemisphere function</article-title>. <source>Science</source>, <volume>219</volume>, <fpage>1347</fpage>–<lpage>1349</lpage>.</citation>
</ref>
<ref id="bibr24-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mampe</surname><given-names>B.</given-names></name>
<name><surname>Friederici</surname><given-names>A. D.</given-names></name>
<name><surname>Christophe</surname><given-names>A.</given-names></name>
<name><surname>Wermke</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Newborns’ cry melody is shaped by their native language</article-title>. <source>Current Biology</source>, <volume>19</volume>, <fpage>1994</fpage>–<lpage>1997</lpage>.</citation>
</ref>
<ref id="bibr25-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McCune</surname><given-names>L.</given-names></name>
<name><surname>Vihman</surname><given-names>M. M.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Early phonetic and lexical development: A productivity approach</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>, <volume>44</volume>, <fpage>670</fpage>–<lpage>684</lpage>.</citation>
</ref>
<ref id="bibr26-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McGurk</surname><given-names>H.</given-names></name>
<name><surname>MacDonald</surname><given-names>J.</given-names></name>
</person-group> (<year>1976</year>). <article-title>Hearing lips and seeing voices</article-title>. <source>Nature</source>, <volume>264</volume>, <fpage>746</fpage>–<lpage>748</lpage>.</citation>
</ref>
<ref id="bibr27-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Patterson</surname><given-names>M. L.</given-names></name>
<name><surname>Werker</surname><given-names>J. F.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Matching phonetic information in lips and voice is robust in 4.5-month- old infants</article-title>. <source>Infant Behavior &amp; Development</source>, <volume>22</volume>, <fpage>237</fpage>– <lpage>247</lpage>.</citation>
</ref>
<ref id="bibr28-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Patterson</surname><given-names>M. L.</given-names></name>
<name><surname>Werker</surname><given-names>J. F.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Two-month-old infants match phonetic information in lips and voice</article-title>. <source>Developmental Science</source>, <volume>6</volume>, <fpage>191</fpage>–<lpage>196</lpage>.</citation>
</ref>
<ref id="bibr29-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pons</surname><given-names>F.</given-names></name>
<name><surname>Lewkowicz</surname><given-names>D. J.</given-names></name>
<name><surname>Soto-Faraco</surname><given-names>S.</given-names></name>
<name><surname>Sebastián-Gallés</surname><given-names>N.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Narrowing of intersensory speech perception in infancy</article-title>. <source>Proceedings of the National Academy of Sciences, USA</source>, <volume>106</volume>, <fpage>10598</fpage>–<lpage>10602</lpage>.</citation>
</ref>
<ref id="bibr30-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pulvermüller</surname><given-names>F.</given-names></name>
<name><surname>Fadiga</surname><given-names>L.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Active perception: Sensorimotor circuits as a cortical basis for language</article-title>. <source>Nature Reviews Neuroscience</source>, <volume>11</volume>, <fpage>351</fpage>–<lpage>360</lpage>.</citation>
</ref>
<ref id="bibr31-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rosenblum</surname><given-names>L. D.</given-names></name>
<name><surname>Schmuckler</surname><given-names>M. A.</given-names></name>
<name><surname>Johnson</surname><given-names>J. A.</given-names></name>
</person-group> (<year>1997</year>). <article-title>The McGurk effect in infants</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>59</volume>, <fpage>347</fpage>–<lpage>357</lpage>.</citation>
</ref>
<ref id="bibr32-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ruzza</surname><given-names>B.</given-names></name>
<name><surname>Rocca</surname><given-names>F.</given-names></name>
<name><surname>Boero</surname><given-names>D. L.</given-names></name>
<name><surname>Lenti</surname><given-names>C.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Investigating the musical qualities of early infant sounds</article-title>. <source>Annals of the New York Academy of Sciences</source>, <volume>999</volume>, <fpage>527</fpage>–<lpage>529</lpage>.</citation>
</ref>
<ref id="bibr33-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sams</surname><given-names>M.</given-names></name>
<name><surname>Möttönen</surname><given-names>R.</given-names></name>
<name><surname>Sihvonen</surname><given-names>T.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Seeing and hearing others and oneself talk</article-title>. <source>Cognitive Brain Research</source>, <volume>23</volume>, <fpage>429</fpage>–<lpage>435</lpage>.</citation>
</ref>
<ref id="bibr34-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schütz-Bosbach</surname><given-names>S.</given-names></name>
<name><surname>Prinz</surname><given-names>W.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Perceptual resonance: Action-induced modulation of perception</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>11</volume>, <fpage>349</fpage>–<lpage>355</lpage>.</citation>
</ref>
<ref id="bibr35-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Steeve</surname><given-names>R. W.</given-names></name>
<name><surname>Moore</surname><given-names>C. A.</given-names></name>
<name><surname>Green</surname><given-names>J. R.</given-names></name>
<name><surname>Reilly</surname><given-names>K. I.</given-names></name>
<name><surname>McMurtrey</surname><given-names>J. R.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Babbling, chewing, and sucking: Oromandibular coordination at 9 months</article-title>. <source>Journal of Speech, Language, and Hearing Research</source>, <volume>51</volume>, <fpage>1390</fpage>–<lpage>1404</lpage>.</citation>
</ref>
<ref id="bibr36-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Teinonen</surname><given-names>T.</given-names></name>
<name><surname>Aslin</surname><given-names>R. N.</given-names></name>
<name><surname>Alku</surname><given-names>P.</given-names></name>
<name><surname>Csibra</surname><given-names>G.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Visual speech contributes to phonetic learning in 6-month-old infants</article-title>. <source>Cognition</source>, <volume>108</volume>, <fpage>850</fpage>–<lpage>855</lpage>.</citation>
</ref>
<ref id="bibr37-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vihman</surname><given-names>M. M.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Vocal motor schemes, variation and the production-perception link</article-title>. <source>Journal of Phonetics</source>, <volume>21</volume>, <fpage>163</fpage>–<lpage>169</lpage>.</citation>
</ref>
<ref id="bibr38-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Walton</surname><given-names>G. E.</given-names></name>
<name><surname>Bower</surname><given-names>T. G. R.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Amodal representation of speech in infants</article-title>. <source>Infant Behavior &amp; Development</source>, <volume>16</volume>, <fpage>233</fpage>–<lpage>243</lpage>.</citation>
</ref>
<ref id="bibr39-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Werker</surname><given-names>J. F.</given-names></name>
</person-group> (<year>1993</year>). <article-title>The contribution of the relation between vocal production and perception to a developing phonological system</article-title>. <source>Journal of Phonetics</source>, <volume>21</volume>, <fpage>177</fpage>–<lpage>180</lpage>.</citation>
</ref>
<ref id="bibr40-0956797612458802">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Whalen</surname><given-names>D. H.</given-names></name>
<name><surname>Levitt</surname><given-names>A. G.</given-names></name>
<name><surname>Goldstein</surname><given-names>L. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>VOT in the babbling of French- and English-learning infants</article-title>. <source>Journal of Phonetics</source>, <volume>35</volume>, <fpage>341</fpage>–<lpage>352</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>