<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="editorial">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">ALH</journal-id>
<journal-id journal-id-type="hwp">spalh</journal-id>
<journal-title>Active Learning in Higher Education</journal-title>
<issn pub-type="ppub">1469-7874</issn>
<issn pub-type="epub">1741-2625</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1469787412441878</article-id>
<article-id pub-id-type="publisher-id">10.1177_1469787412441878</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Editorial</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Editorial</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Baldwin</surname><given-names>Lynne P.</given-names></name>
</contrib>
<aff id="aff1-1469787412441878">Brunel University, UK</aff>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>7</month>
<year>2012</year>
</pub-date>
<volume>13</volume>
<issue>2</issue>
<fpage>93</fpage>
<lpage>100</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
</article-meta>
</front>
<body>
<p>With the ubiquity of the internet and the use of Web 2.0 and other ICT (information and communication technologies) that we educators use to support our learners in their learning, it is no surprise that, these days, the literature in the field of learning and teaching in higher education is dominated by studies which look at particular technologies and how, where and when we can or should use them in our classrooms. Or, more accurately, how we can support our learners <italic>outside</italic> of the classroom, which is where our learners spend far more time and where the majority of the learning actually takes place. Whether we are involved in e-learning, blended learning, distance learning or whatever, it is clear that there is much more that we need to understand about these technologies, not only in terms of the capabilities of the technologies themselves (a must for those of us who are ‘technologically challenged’) but also in terms of how we might best exploit such capabilities in supporting our learners in their learning.</p>
<p>Before the advent of these technologies, or indeed any technology at all, there was always the issue of there being a generation gap, that is, the learners in the classroom were, as a general rule, the younger generation and their teachers were of another, older, generation. This, in itself, raises its own issues, but today there is also a generational divide in terms of the exposure to, and use of, information and communication technologies, including those of social networking sites (SNS). Generations, and the issue of digital divides, are the focus of the first article in this issue. Entitled ‘Towards an international culture: Gen Y students and SNS?’, its author, Jessica Lichy, at the Idrac Centre for Applied Research in Lyon, France, describes the various current generations such as the ‘<italic>silent generation</italic>’, the ‘<italic>baby boomers</italic>’ and, pertinent to the use of technologies, the generation known as ‘<italic>generation X</italic>’, that is, those born between 1965 and 1975, and ‘<italic>generation Y</italic>’, that is, those born between 1976 and 1994. Those who comprise the <italic>Y Generation</italic> are also known as the <italic>Facebook Generation</italic> or <italic>Y-Geners</italic>, says the author. Unsurprisingly, having had the X and Y Generations, what follows is the <italic>Z Generation</italic>, that is, those born since 1995, those who, says the author, have integrated the internet into every single aspect of their lives. Quite what those who are born after the Z Generation will be called (the alphabet theme now being exhausted) remains to be seen!</p>
<p>As we, the educators, are likely to be either someone from the X Generation (at the time of writing, someone between the ages of 36 and 46) or perhaps a <italic>‘</italic>baby boomer’ (at the time of writing, someone between the ages of 47 and 65), we are not the generation which comprises the majority of our undergraduate students, that is, the Y Generation (at the time of writing, someone between the ages of 18 and 35). The Y Generation, says the author, are described as being those who multi-task, are confident and are team players. Those of us who are either from the X Generation or a ‘baby boomer’ are described as being ‘recognized for being techno-literate and informal’ or ‘distinguished by their optimism and personal growth’, respectively. Whether or not you or I recognize ourselves by these descriptions is another matter! And, indeed, the author makes this very point, namely, that the literature on the generations has overly focused not only on age per se but also on the differences <italic>between</italic> generations rather than <italic>within</italic> generations. Importantly, and pertinent to the understanding of how our students, many of them between the ages of 18 and 35 and thus of the Y Generation, actually <italic>use</italic> the internet, the author posits that we assume that students the world over exhibit similar internet use behaviour yet that this may not be the case. There is, says the author, a new type of ‘generation gap’, that is, one that stems from the different perceptions, and different uses of, internet-based and other technologies. The Y Generation often use technologies that we educators not only probably do not use much if at all but, worse, may not even have heard of. It is, then, vital to better understand how our learners, mainly between the ages of 18 and 35 and thus of the Y Generation, actually use such technology, given that we may well be using the same technologies to support them in their learning in one way or another. The author cites literature stating that if our Y Generation (and later the Z Generation) learners approach everything in their lives, including learning, in ‘fundamentally different ways’ then it follows from this that we, as educators, may have to support them in their learning in ‘fundamentally different ways’; in short, we need to re-visit our perceptions of what we mean by ‘teaching’ and ‘learning’. How we might need to adapt or fundamentally change the way we support our learners is described in the comprehensive review of the literature of this first article: a ‘must read’ (and fortunately an easy read), for those of us who need to better understand the technology that the Y Generation have grown up using, and which they expect to use if not in our classrooms certainly when they are at home, both in their social lives and in support of their academic learning experiences.</p>
<p>One way or another, using technology usually involves typing. This might be tapping keys on a mobile phone or typing on a computer. Some of us might use a couple of fingers for this or, in the case of using a computer, be ‘touch typists’, that is, using all fingers and not looking at the keyboard whilst typing, as typists used to do on typewriters. In terms of coursework, many if not most of us insist that our students submit coursework that is word processed. Some go further and do not accept handwritten coursework at all. Many schools or institutions require students to submit their word-processed coursework in electronic form, that is, students do not submit a printed hard copy but instead submit an electronic copy to the university’s computer system, for example, WebCT. Submitting coursework online has many advantages of course, the most obvious of which is that students can submit their coursework from anywhere at all; they do not have to be on campus to do so. However, whilst coursework is usually both written and submitted electronically, the same is not the case for examinations. We normally ask students to write their examinations by hand, without the use of a computer. The only exception to this is for those students with special needs of some sort, who are, for whatever reason, unable to write by hand, or unable to write quickly enough or without a spellchecker or some other support. Yet, it might be argued, for those without special needs, the vast majority, asking them to write by hand is anachronistic and, given that our students rarely if ever write anything by hand either in university or elsewhere, asking them to do so for one particular assessment only might disadvantage them in some way. However, the issue of ‘fairness’ is a complex one, as the authors of the second article in this issue readily admit, listing the factors affecting that complexity. In their article entitled ‘Students’ choices between typing and handwriting in examinations’, Nora Mogey, John Cowan, Jessie Paterson and Mike Purcell rightly say that the environment in which the examination takes place is a factor which may influence the candidates’ performance and, as assessment is so vital an aspect of higher education, it merits closer analysis.</p>
<p>The authors argue that we have perhaps given too little thought to the time-limited aspect of examinations, very different from that of coursework, where the time spent on the task is determined by the student and usually spans hours, weeks or even months and where they can dip in and out at a time of their choosing. In the examination room students produce what the authors say is basically a ‘first draft’ of their thoughts. And, as we all know, whether in the university context or elsewhere, our first draft is rarely our best work. When we write we naturally have to know the ‘content’ but we also need to be able to write quickly and easily in terms of physically being able to do so. We, and our students, may be slowed down in both our thinking and our writing by having to handwrite rather than type what we want to say. As an academic, I myself could not possibly imagine having to produce text in handwritten form; other than the odd personal note for my own consumption (I am not sure that anyone else could read my handwriting!) or some jottings, 99% of what I need to produce is produced whilst sitting and typing text on a computer. Why, then, do we ask students to write by hand when it comes to examinations? The authors also point out the problems associated with the marking of text that has been handwritten; even if we claim otherwise, we may be biased. That is, if we see beautiful handwriting and a text that is free of crossings out, we may link this to the content in some way and assume a correlation between good handwriting and ‘good thinking’. And, of course, the opposite: that is, if the handwriting is poor, we may assume the thinking is too. Even if we do not think this, there is also the issue of legibility: we cannot award marks if we cannot read what is in front of us. Whilst legibility is an issue for a native speaker it is perhaps even more of a problem for the non-native speaker whose normal form of handwriting is perhaps very different indeed from the one that they have to produce in the examination room. For example, those whose first written language is Arabic but who have to write their examinations in English; the scripts are fundamentally different on many levels. Many if not most of our postgraduate students, in the UK at least, are non-native speakers of English and many grew up writing, by hand, using a script that is very different from English.</p>
<p>Whatever the first language of the writer or the language that they use to write their text, being able to write quickly, whether by hand or using a computer, is clearly an advantage. The vast majority of our students, whether ‘Y Generation’ or any other, type faster than they can write by hand. As an examination has to be done under a time constraint set by us, the assessors, and given that typing is faster than writing by hand, the authors of the second article posit whether we should allow <italic>all</italic> students to type their essay-type examinations. Offering students the choice to either type using a computer or write their essay-type examinations by hand (or requiring them to type rather than offering them the choice) is, to my knowledge, not a practice undertaken at any university in the UK. Yet, say the authors of the second article, there are schools and universities in the US which do so, so it is not new. The motivation underpinning the study described in the article is that there is more that we need to understand about the issues involved in offering choice of either writing essay-type examination answers by hand or using a computer (requiring one or the other as choice, in itself, sets up its own problems when it comes to assessment). As traditional essay-type examinations remain the norm in higher education, say the authors, we are obliged to reflect on what we do and why, not least because ‘traditional essay-type examinations are . . . known to disadvantage students in a multiplicity of ways’, and we assessors play a part in this, given that we often fail to make sufficiently transparent the criteria used in marking and to prepare students adequately for what is a very different experience. The results from the study describe the many, and varied, reasons that students give for choosing to write by hand rather than write using a computer. Most interestingly, perhaps, the study reveals that we do not need to be concerned about typing speed, a factor that we assessors might put forward for not allowing (or requiring) our students to use a computer to type their essay-type examination answers. The discussion and conclusions of this important and interesting study reveal some useful insights for us, as assessors.</p>
<p>And the authors pose a key question for us, namely, is it really necessary to have a rigid time limit for an examination? Why does everyone have to put their pens down or stop typing at or before the precise moment allotted? These days, with the ‘employability agenda’ at the forefront of what we, in higher education, are doing in terms of preparing our students for the workplace in which they will find themselves after graduation, the authors remind us that, as deadlines set in the business/work environment are rarely so rigid, we need to reflect on the reason(s) why we choose (it is a choice) to impose such a time limit in examinations. After all, say the authors, ‘if the point of an examination is to demonstrate understanding of content and the ability to create a high-quality argument without collaboration, and especially if we wish to allow our students to produce and show their best work, then why impose a [rigid] time limit?’ Indeed.</p>
<p>If assessing our students via an examination is problematic, so too is the assessment of another skill, namely, making an oral presentation, the subject of the third article in this issue. Authors Luc De Grez, Martin Valcke and Irene Roozen at, respectively, the University College Brussels, the University of Ghent and University College Brussels, Belgium, look at the thorny issue of assessment in terms of how we, the assessors, assess and how our students assess the same performance, whether their own or that of their peers. Their article is entitled ‘How effective are self- and peer assessment of oral presentation skills compared with teachers’ assessments?’ and in their review of the literature the authors discuss the formative aspect of assessment, what is called ‘consequential validity’ of assessment, and the two other qualities of assessment, namely reliability and validity. As self-assessment and peer assessment are on the increase in higher education, say the authors, and the ability to reflect on and judge our own performance and the performance of others is a key skill in the workplace and elsewhere, and also in terms of personal and/or intellectual development per se, we need to do all that we can to help our learners to develop their self-regulating skills.</p>
<p>Self- and peer assessment are, therefore, one way of helping our learners to develop their self-regulating skills, although this is not without significant challenges, as the article demonstrates. In their discussion of inter-rater reliability of self- and peer assessments, the authors say that there is considerable debate; in other words, it is not without controversy. Fuelling this debate is the issue of whether or not marks given by students on their own performance or on that of their peers are in line with the marks awarded by us, their teachers. Or, indeed, whether we can or should rely solely on marks awarded by students for their own performance or that of their peers. Reading the authors’ review of the literature, it seems that the debate still rages (assessment is an emotive subject). The authors argue that, whilst there are studies which have looked at self- and peer assessment of written work, there are very few that analyse the variables affecting the quality of self- and peer assessment of oral presentations. There is as much debate about improving the quality of self- and peer assessment as there is about whether or not we should use it, it seems from this article. The authors report on a study which looks at how closely, or whether, and in what ways, there is agreement between students’ assessment of the performance of their peers in oral presentations and the assessment made by their teachers, and also between students’ assessment of their own performance and the assessment made by their teachers. The discussion and conclusions support the notion that self- and peer assessment of oral presentation skills is valuable but, as the review of the literature shows, the debate continues, not least when it comes to the matter of whether the marks awarded are summative rather than formative in nature and thus make their contribution to the mark for the module/course and ultimately to the award itself. Our students may well learn a lot from assessing their own performance or those of their peers but those of us working in universities have to make awards based on the performance as demonstrated and assessed. If we are to use self- and peer assessment in assessments that contribute to the award, we need to be assured that we can have as much confidence in the marks awarded by students as the marks awarded by those of us employed in the higher education sector, who have been tasked to assess. As yet, and as the study demonstrates, the debate continues.</p>
<p>The issue of formative assessment and how and in what way this makes a contribution to the award itself is the subject of the fourth article comprising this issue In their article entitled ‘Mind the gap: An analysis of how quality assurance procedures influence programme assessment patterns’, authors Tansy Jessop, Nicole McNab and Laura Gubby make the argument that, whilst we provide feedback to students on formative assessment, there is more that we need to understand about the relationship between formative assessment and what we write/specify in our degree specifications. Our degree specifications should set out the learning outcomes for the degree programmes and specify the assessment mechanisms by which we expect our learners to meet these outcomes. The authors cite literature supporting the view that none can disagree with, namely that formative assessment is of vital importance when it comes to student learning, regardless of institution or geographical context. However, and for the reasons explained in the article, one of the factors driving an assessment regime that is, it is argued, overly focused on summative rather than formative assessment is the modular degree structure. A modular degree structure is one where learning is ‘chunked’ into modules/units of normally one semester in length and where, for most of us, we stick to the tried-and-tested method of ensuring that we test students, and award marks to their achievement, at/by the end of the module/unit, that is, to ‘tick off’ the number of credits gained at or very near the end of the module/unit in question. Whilst there are perhaps reasons for this, transferability being the main reason given, this ‘chunking’ and the assessment practices associated with it make for a message to our learners that says ‘do a bit of learning, do a bit of assessment, collect a mark for it, collect enough marks for each bit and, at the end, if you have collected enough credits, we will give you an award’ rather than one which says ‘we focus our attention on your learning as a whole, at degree/course level’. And this is the point that the authors make: if we are not careful, it is all too easy to focus our attention, and that of our students, on the module/unit level rather than the degree/programme level. This focus of attention on the module/unit level rather than the degree/programme level is reflected in the documentation we produce, say the authors. In the module/unit documentation we specify in great detail the summative assessment that the students must satisfactorily pass for the award of a module/unit, but at this level, and as a consequence at degree/programme level, ‘formative assessment is virtually invisible’, say the authors. The article looks at what we write in our degree/programme-level documentation (which sits within a national and perhaps international system of certification and accountability) and how this is interpreted and enacted when it comes to actual assessment practice.</p>
<p>The results from this study identify why, like many of us no doubt, module/unit leaders focus much if not all of their attention on devising (and marking and all that goes with it, that is, second or double marking, moderation, external examiners and the rest) summative rather than formative assessments. It is important that we provide our learners with more formative assessment (and feedback on it). It seems from this study that there is much that we need to do to embed more formative assessment in our programmes than is currently the case, and to ensure that its value is recognized, formally, by way of the documentation that we produce.</p>
<p>Skills and abilities that are relevant and necessary across the degree/programme more generally, not just for a single discrete module/unit, are the subject of the fifth article in this issue, which discusses the writing or other skills that our students need, and need to demonstrate, in higher education. There are, say the authors, Steven Pryjmachuk, Anita Gill, Patricia Wood, Nicola Olleveant and Philip Keeley, several ‘skills clusters’, namely self-management skills, data and knowledge acquisition skills, interpretative, critical and analytical skills and oral and written communication skills. In their article entitled ‘Evaluation of an online study skills course’, the authors review literature stating that providing study skills support has a number of purported benefits. Whilst such skills support is sometimes provided at postgraduate level, it is probably fair to say that much of this support is provided to new, incoming undergraduates in their first term or year. These skills and abilities are needed for study but also fit the category of ‘transferable skills’, that is, skills that have benefits beyond higher education. These skills ‘foster student independence, enhance intellectual development, promote a deeper approach to learning, increase self-confidence and self-efficacy in students, assist with the process of academic and professional socialization, and decrease attrition arising from academic failure’. However, despite the fact that there is no institution which does not provide such skills development in one form or another, the authors say that we have little evidence from empirical research that these skills do what we claim, or believe, that they do, hence the motivation for the study described. Whilst the development of these skills has benefits for students themselves, the authors rightly say that providing such skills support helps us to address some of the challenges that we currently face in higher education the world over, namely, far higher numbers of students coming into higher education, technological advances, complaints that secondary education has not sufficiently prepared our undergraduates for study at university, a demand from students (and from some staff) for more flexible ways of learning, and the notion that, today, we need to foster greater independence in terms of learning, amongst others.</p>
<p>Those ‘technological advances’ mean that, today, there is hardly any lecturer/teacher who does <italic>not</italic> use technology, whether in an e-learning, a blended learning or a distance learning environment. It is therefore not surprising that assisting our learners in developing their study skills has also ‘gone online’. The article provides a comprehensive overview of online learning in its many forms, highlighting the need, regardless of learning environment, for opportunities for formative work and feedback. Also crucial, regardless of learning environment, is a well-designed module/course, naturally enough. In terms of successful module/course design in online learning, the authors cite literature stating that the key elements are interactivity, a team approach, enjoyment and a sense of community. The authors conclude that ‘coupling elements of successful course design with a blended learning approach and guided discovery principles should elicit a high-quality online learning experience for students’ and, given that there have been few empirical studies to support such a claim, they carried out a study to explore whether or not a well-designed online study skills module/course delivered what it was designed to deliver and how effective or otherwise it was to provide this online (rather than not online, which is the norm in most institutions). Whilst the authors are only too ready to admit that their own online study skills module/course is not perfect (nothing in life, including higher education, is perfect), it is nonetheless as good an online study skills module/course as can be found, and the results from their study show a large and statistically significant change between entry and exit measures of student knowledge and confidence in a variety of study-related skills. However, the authors question whether this might be seen as of ‘educational significance’ given that, although students reported increased confidence in their abilities in terms of study skills, there is the question of how much or how little confidence is related to actual performance, and that, whilst the students reported feeling more confident, they also believed that they needed more practice in applying those skills. Many of us will recognize the difficulties faced by the authors in this study in providing feedback, and immediate feedback at that, by way of a mark and/or model answer in response to tests. To us, as educators, this certainly fits what we understand by the term ‘feedback’; yet, when the students in the study were asked about feedback, the students most certainly did not see this as ‘feedback’. It seems that we still have much to do when it comes to clarifying what ‘feedback’ is and is not in higher education.</p>
<p>Of some considerable surprise to many of us reading this study, is that, as well as online tasks, there were follow-up tutorials, that is, students and their teachers/lecturers met in a ‘real’ classroom, face to face. The tutorials were designed, say the authors, to provide ‘scaffolding and community support’, something that many of us provide when we use blended learning (most if not all of us offer blended learning in one form or another/to a greater or lesser extent across the curriculum). Yet, say the authors, there was a great deal of antagonism towards these tutorials, all the more surprising given that ‘none of the students interviewed felt that they belonged to an online learning community’. As the feeling of belonging to a community is said to be important in a learning environment, online or any other, the results from this study pose questions as to what ‘community’ means and how it is fostered. Or, indeed, if it is necessary at all (if the students felt no sense of community online and did not want one in a ‘real’ classroom, it raises the issue of whether ‘community’ is as important as some would claim it to be). However, the authors say that, as with all of us, the curriculum across the whole year is predominantly face-to-face, so perhaps ‘community’ is developed curriculum-wide and also in ways that are not necessarily restricted to the classroom. For those of us who want to know whether such study skills support can be provided successfully online, the results of this study (and from experiences of others elsewhere) suggest a clear ‘yes’, as long as we ensure that we design our modules/courses well (which is easy enough to do, if we follow the published and tried-and-tested principles).</p>
</body>
</article>