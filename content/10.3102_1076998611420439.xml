<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">JEB</journal-id>
<journal-id journal-id-type="hwp">spjeb</journal-id>
<journal-title>Journal of Educational and Behavioral Statistics</journal-title>
<issn pub-type="ppub">1076-9986</issn>
<issn pub-type="epub">1935-1054</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.3102/1076998611420439</article-id>
<article-id pub-id-type="publisher-id">10.3102_1076998611420439</article-id>
<title-group>
<article-title>Estimating a Meaningful Point of Change</article-title>
<subtitle>A Comparison of Exploratory Techniques Based on Nonparametric Regression</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name>
<surname>Klotsche</surname>
<given-names>Jens</given-names>
</name>
<xref ref-type="aff" rid="aff1-1076998611420439"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Gloster</surname>
<given-names>Andrew T.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1076998611420439"/>
</contrib>
<bio>
<title>Authors</title>
<p>JENS KLOTSCHE's research is focused on nonparametric regression models where the regression function can show discontinuities. He currently works at the Technische Universitaet Dresden, Institute of Clinical Psychology and Psychotherapy, Chemnitzer Str. 46, 01187 Dresden, Germany; <email>klotsche.jens@googlemail.com</email>.</p>
<p>ANDREW T. GLOSTER's research concentrates on the nature and treatment of pschiatric disorders, with a concentration on anxiety disorders. He currently works at the Technische Universitaet Dresden, Institute of Clinical Psychology and Psychotherapy, Chemnitzer Str. 46, 01187 Dresden, Germany; <email>gloster@psychologie.tu-dresden.de</email>.</p>
</bio>
</contrib-group>
<aff id="aff1-1076998611420439">Technische Universitaet Dresden</aff>
<pub-date pub-type="epub-ppub">
<month>10</month>
<year>2012</year>
</pub-date>
<volume>37</volume>
<issue>5</issue>
<fpage>579</fpage>
<lpage>600</lpage>
<history>
<date date-type="received">
<day>10</day>
<month>9</month>
<year>2010</year>
</date>
<date date-type="rev-recd">
<day>31</day>
<month>1</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>20</day>
<month>5</month>
<year>2011</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012 AERA</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">American Educational Research Association</copyright-holder>
</permissions>
<abstract>
<p>Longitudinal studies are increasingly common in psychological research. Characterized by repeated measurements, longitudinal designs aim to observe phenomena that change over time. One important question involves identification of the exact point in time when the observed phenomena begin to meaningfully change above and beyond baseline fluctuations. The authors introduce a nonparametric modeling framework to estimate the change-point of interest using derivatives of the underlying regression function for an outcome variable across time. The estimator of Huh and Carriere (HC) consistently performed acceptably when using a plug-in bandwidth in a Monte Carlo simulation study. It was shown that the estimator performed well with a minimum number of 15 design points. This procedure was applied to longitudinal data collected on performance anxiety. Results suggest that the HC estimator combined with plug-in bandwidth selection provides an efficient strategy for investigating a possible change-point at which an observed outcome variable starts changing across time.</p>
</abstract>
<kwd-group>
<kwd>change-point estimation</kwd>
<kwd>bandwidth selection</kwd>
<kwd>nonparametric regression</kwd>
<kwd>long-range dependent errors</kwd>
<kwd>short-range dependent errors</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Longitudinal studies are increasingly common in psychological research. Characterized by repeated measurements, longitudinal designs aim to observe outcome variables that change over time, regardless of whether the obtained data are molar or molecular in nature. Within the dynamic nature of these data, one important question involves identification of the time point when the observed outcome variable begins to meaningfully change above and beyond baseline fluctuations.</p>
<p>Consider the two following hypothetical situations, each of which explicitly or implicitly seeks to identify the point of meaningful change. (a) In a typical ecological momentary assessment (EMA) design (<xref ref-type="bibr" rid="bibr12-1076998611420439">Gloster et al., 2008</xref>; <xref ref-type="bibr" rid="bibr20-1076998611420439">Kashdan &amp; Steger, 2006</xref>; <xref ref-type="bibr" rid="bibr34-1076998611420439">Shiffman, Stone, &amp; Hufford, 2008</xref>; <xref ref-type="bibr" rid="bibr35-1076998611420439">Stone, Schwartz, Broderick, &amp; Shiffman, 2005</xref>; <xref ref-type="bibr" rid="bibr36-1076998611420439">Stone &amp; Shiffman, 2002</xref>), multiple variables are assessed multiple times per day in one’s natural environment. Analyses of EMA data are commonly focused on detecting predictor variables for a meaningful fluctuation in the outcome variable across time. In this constellation, the time point at which fluctuations in the outcome variable rises above baseline variation may provide important information regarding the focal phenomena. (b) A typical psychophysiological experiment may examine the interaction between physiological variables and standardized events across a battery of paradigms. For example, heart rate, blood pressure, and electrodermal activation (EEG) are collected during a standardized experiment in patients with an anxiety disorder. The aim of the experiment is to investigate the course of physiological characteristics of patients in a standardized setting. If a meaningful change in any variable appears, it may be of crucial importance to identify the time point when the change in a parameter started in order to find out what precipitated the observed changes in the outcome variable.</p>
<p>The particular time point at which an observed outcome variable begins to change is denoted by θ. The slope of the hypothesized regression curve starts changing at the change-point θ. There are several reasons for the importance of estimating a particular change-point. In addition to proving important information about the research question, identification of the estimated change-point provides valued statistical information that can be used in the model-building process of data analyses. Information about θ influences subsequent analysis and generates new hypotheses. Predictor variables for the change itself and the magnitude of change can be identified in relation to the point of initial change. Furthermore, the change-point can also have causal implications as displayed by the two introductory examples.</p>
<p>The modeling framework of interrupted time-series analyses has been developed for modeling a time series with a change-point. But the change-point is assumed to be known before analyses in interrupted time-series analysis and therefore it is not a parameter to be estimated (<xref ref-type="bibr" rid="bibr16-1076998611420439">Hartmann et al., 1980</xref>; <xref ref-type="bibr" rid="bibr22-1076998611420439">Morgan, Griffiths, &amp; Majeed, 2007</xref>; <xref ref-type="bibr" rid="bibr38-1076998611420439">Wagner, Soumerai, Zhang, &amp; Ross-Degnan, 2002</xref>).</p>
<p>An option to identify the change-point θ is the parametric segmented regression (SR) analysis (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>). In this approach, the regression line is piecewise linear with partitions defined by the assumed change-points. This approach is flexible. It can be generalized to the generalized linear model framework with linear predictors and a finite number of change-points (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>). The unknown change-point θ is also a parameter to be estimated in the SR analyses besides the simultaneously fitted regression lines. The performance of this approach depends on several assumptions that are not always met with real data. For example, a functional association between the response variable and design variable (e.g., linear) must be assumed without a priori knowledge of whether or not the stipulated relation can be met. The estimation procedure of the SR model strongly depends on the existence of the change-point θ (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>). The nonconvergence of the estimation algorithm is not equivalent to the nonexistence of a possible change-point (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>). A false configuration of the SR model to the data could also result in a failing of the algorithm (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>). Another reason for a possible failure of the algorithm could be based on the inappropriate choice of starting values for the parameter, what is required for a nonlinear optimization algorithm (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>). Therefore, different starting values for the parameter estimates could yield different estimates.</p>
<p>Accurate and unbiased estimation of the change-point θ is indeed a challenge and problems often arise when applying extant methods to real data. Procedures such as those described above make assumptions that often cannot be met in real data or overly rely on a trial and error strategy.</p>
<p>To help overcome these problems, we consider a nonparametric modeling framework for estimating the change-point θ of interest. The basic principle of estimating θ is based on nonparametric regression analysis as described in the first part of the article. Three exploratory estimation strategies are compared in a Monte Carlo simulation study. The consistently best performing estimator was then applied to longitudinal data about anticipatory performance anxiety of five musicians in the days leading up to a performance. The parametric SR analyses failed in the data about performance anxiety. It suggests that when the particular change-point θ at which an observed outcome variable starts changing across time is of interest, the nonparametric estimation procedure can be the method of choice.</p>
<sec id="section1-1076998611420439">
<title>Statistical Model for Estimating the Change-Point θ</title>
<p>The association of an outcome variable <italic>Y</italic> with a covariate <italic>T</italic> can be described by the nonparametric regression model <inline-formula id="inline-formula1-1076998611420439">
<mml:math id="mml-inline1-1076998611420439">
<mml:mi>Y</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>T</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:math>
</inline-formula> with <inline-formula id="inline-formula2-1076998611420439">
<mml:math id="mml-inline2-1076998611420439">
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">[</mml:mo>
<mml:mi>Y</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>T</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula> for <inline-formula id="inline-formula3-1076998611420439">
<mml:math id="mml-inline3-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow/>
</mml:mrow>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mo fence="false" stretchy="false">{</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal" stretchy="false">,</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">…</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal" stretchy="false">,</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula>, where <inline-formula id="inline-formula4-1076998611420439">
<mml:math id="mml-inline4-1076998611420439">
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">⋅</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> denotes the unknown regression function and ϵ a random variable representing the error in the equation with zero mean (<xref ref-type="bibr" rid="bibr14-1076998611420439">Haerdle, 1990</xref>, p. 3). We assume that the first derivative of <italic>m</italic> has a jump discontinuity and is otherwise a smooth curve. <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1A</xref> shows a hypothetical example with noisy data <inline-formula id="inline-formula5-1076998611420439">
<mml:math id="mml-inline5-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and its model generating regression curve <italic>m</italic> (light gray line). Typically, the covariate <italic>T</italic> represents time in longitudinal analyses as in our considerations, but other covariates are applicable. The nonparametric regression model <inline-formula id="inline-formula6-1076998611420439">
<mml:math id="mml-inline6-1076998611420439">
<mml:mi>Y</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>T</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi mathvariant="italic">ϵ</mml:mi>
</mml:math>
</inline-formula> is considered asymptotically equivalent to a white noise time-series model (<xref ref-type="bibr" rid="bibr2-1076998611420439">Brown &amp; Low, 1996</xref>). It implies that asymptotical results from nonparametric regression analyses yield a corresponding solution in a white noise time-series model (<xref ref-type="bibr" rid="bibr2-1076998611420439">Brown &amp; Low, 1996</xref>).</p>
<fig id="fig1-1076998611420439" position="float">
<label>Figure 1.</label>
<caption>
<p>Trajectory for a hypothetical example (A: solid light gray line: Model generating regression curve <italic>m</italic>, solid black line: first derivative of <italic>m;</italic> B: solid light gray line: Model generating regression curve <italic>m</italic>, solid black line: second derivative of <italic>m;</italic> C: solid light gray line: Model generating regression curve <italic>m</italic>, solid black line: continuous part of <italic>m</italic>, dashed black line: Discontinuous part of <italic>m</italic> described by a step function).</p>
</caption>
<graphic xlink:href="10.3102_1076998611420439-fig1.tif"/>
</fig>
<p>The data are collected from the same subject over time, where the assumption of independent and identical distributed errors does not hold. The response at time <italic>t<sub>j</sub>
</italic> depends on the responses at times <inline-formula id="inline-formula7-1076998611420439">
<mml:math id="mml-inline7-1076998611420439">
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>2</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
</mml:math>
</inline-formula>. We consider long-range and short-range dependent errors in our statistical model. Short-range dependent errors are characterized by the dependence of <inline-formula id="inline-formula8-1076998611420439">
<mml:math id="mml-inline8-1076998611420439">
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>l</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> on <inline-formula id="inline-formula9-1076998611420439">
<mml:math id="mml-inline9-1076998611420439">
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>k</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> for the time point <italic>k</italic> in the recent past of time point <italic>1</italic>, in contrast to long-range dependent errors.</p>




<sec id="section2-1076998611420439">
<title>Nonparametric Regression Analysis</title>
<p>In this article, the estimator for the meaningful change-point θ is based on nonparametric regression. Nonparametric regression analysis attempts to identify a functional relationship between an outcome variable <italic>Y</italic> and a predictor variable <italic>T</italic> without needing to assume a particular functional relationship.</p>
<p>The local polynomial regression approach is a widely used smoothing technique besides other smoothing procedures. The regression function <italic>m</italic> is locally approximated for all <italic>t</italic> by a polynomial in the neighborhood <inline-formula id="inline-formula10-1076998611420439">
<mml:math id="mml-inline10-1076998611420439">
<mml:mo stretchy="false">[</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula> of <italic>t</italic>, <inline-formula id="inline-formula11-1076998611420439">
<mml:math id="mml-inline11-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow/>
</mml:mrow>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mrow>
<mml:mrow/>
</mml:mrow>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> and <italic>h</italic> &gt; 0 (<xref ref-type="bibr" rid="bibr14-1076998611420439">Haerdle, 1990</xref>, p. 39). The local polynomial approximation <inline-formula id="inline-formula12-1076998611420439">
<mml:math id="mml-inline12-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi mathvariant="italic">υ</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula13-1076998611420439">
<mml:math id="mml-inline13-1076998611420439">
<mml:mi mathvariant="italic">υ</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>q</mml:mi>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> of <italic>m</italic> for a date <italic>t</italic>, <inline-formula id="inline-formula14-1076998611420439">
<mml:math id="mml-inline14-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow/>
</mml:mrow>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mrow>
<mml:mrow>
<mml:mo fence="false" stretchy="false">{</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mo mathvariant="normal" stretchy="false">,</mml:mo>
</mml:mrow>
</mml:mrow>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:mrow>
</mml:mrow>
</mml:math>
</inline-formula> is estimated by minimizing the least square equation<disp-formula id="disp-formula1-1076998611420439">
<mml:math id="mml-disp1-1076998611420439">
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
<mml:mi>T</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:mi>K</mml:mi>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:mrow>
<mml:mfrac>
<mml:mrow>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>t</mml:mi>
</mml:mrow>
<mml:mi>h</mml:mi>
</mml:mfrac>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
<mml:msup>
<mml:mfenced close=")" open="(">
<mml:mrow>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:munderover>
<mml:mo movablelimits="false" stretchy="false">∑</mml:mo>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>0</mml:mn>
</mml:mrow>
<mml:mi>q</mml:mi>
</mml:munderover>
</mml:mrow>
<mml:mrow>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>l</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:msup>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi>j</mml:mi>
</mml:msup>
</mml:mrow>
</mml:mrow>
</mml:mrow>
</mml:mfenced>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">,</mml:mo>
</mml:math>
<graphic alternate-form-of="disp-formula1-1076998611420439" xlink:href="10.3102_1076998611420439-eq1.tif"/>
</disp-formula>where <italic>q</italic> denotes the degree of the polynomial, <italic>h</italic> the smoothing parameter, and <italic>K</italic> a symmetric kernel function with support on <inline-formula id="inline-formula15-1076998611420439">
<mml:math id="mml-inline15-1076998611420439">
<mml:mo stretchy="false">[</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>z</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mi mathvariant="normal">ℜ</mml:mi>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula16-1076998611420439">
<mml:math id="mml-inline16-1076998611420439">
<mml:mrow>
<mml:mstyle displaystyle="true">
<mml:mrow>
<mml:munderover>
<mml:mo>∫</mml:mo>
<mml:mrow>
<mml:mo>−</mml:mo>
<mml:mi>z</mml:mi>
</mml:mrow>
<mml:mi>z</mml:mi>
</mml:munderover>
<mml:mrow>
<mml:mi>K</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mi>d</mml:mi>
<mml:mi>u</mml:mi>
<mml:mo>=</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:mrow>
</mml:mstyle>
</mml:mrow>
</mml:math>
</inline-formula>. The crucial choice of the smoothing parameter <italic>h</italic> and the kernel function <italic>K</italic> will be discussed separately because of their importance in the smoothing process. The local estimate of the regression function <italic>m</italic> in date <italic>t</italic> is given by <inline-formula id="inline-formula17-1076998611420439">
<mml:math id="mml-inline17-1076998611420439">
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and the estimate of the <italic>ν</italic>-the derivative is defined by <inline-formula id="inline-formula18-1076998611420439">
<mml:math id="mml-inline18-1076998611420439">
<mml:msup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mfenced close=")" open="(">
<mml:mi mathvariant="italic">ν</mml:mi>
</mml:mfenced>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mi mathvariant="italic">ν</mml:mi>
<mml:mo stretchy="false">!</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi mathvariant="italic">ν</mml:mi>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula19-1076998611420439">
<mml:math id="mml-inline19-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> (<xref ref-type="bibr" rid="bibr14-1076998611420439">Haerdle, 1990</xref>, p. 39).</p>
<p>
<xref ref-type="bibr" rid="bibr40-1076998611420439">Yang (2001)</xref> and <xref ref-type="bibr" rid="bibr18-1076998611420439">Hidalgo (1997</xref>) showed that the regression function <italic>m</italic> can be estimated as in the case of independent and identically distributed errors for assumed correlated errors. It justifies the application of the local polynomial regression estimator for <italic>m</italic> in our introduced model.</p>
</sec>
<sec id="section3-1076998611420439">
<title>Estimation of the Meaningful Change-Point θ</title>
<p>A more detailed study of the curve in <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1A</xref> shows another remarkable characteristic of <italic>m</italic> at the hypothesized change-point θ. The first derivative (solid black line) of the regression function <italic>m</italic> has a jump discontinuity at θ. This fact forms the base of the approach to estimate the change-point in longitudinal data. There are three possible strategies for estimating the location of the jump discontinuity in the first derivative of <italic>m</italic> based on properties of the curve: (a) The left-hand (lhl) and right-hand limits (rhl) of the first derivative are unequal at the point of discontinuity as highlighted by the solid black line in <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1A</xref>. (b) The first derivative of the regression function does not exist at θ, and its value of the second derivative is defined to be infinity as exemplified by the solid black line in <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1B</xref>. (c) Another option provides the separation of the “continuous part” from the “jump part” in the first derivative of the regression function <italic>m</italic> and considering the part containing the information about the jump. The idea of this approach is based on the fact that the discontinuous first derivative can be described as the sum of a continuous function (solid black line in <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1C</xref>) and a step function with a jump in θ (dashed black line in <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1C</xref>).</p>



<sec id="section4-1076998611420439">
<title>Estimation strategy (1)</title>
<p>The first derivative <inline-formula id="inline-formula20-1076998611420439">
<mml:math id="mml-inline20-1076998611420439">
<mml:msup>
<mml:mi>m</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula> has a discontinuity at the change-point θ in the hypothesized model. The respective rhl <inline-formula id="inline-formula21-1076998611420439">
<mml:math id="mml-inline21-1076998611420439">
<mml:msubsup>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
</mml:math>
</inline-formula> and lhl <inline-formula id="inline-formula22-1076998611420439">
<mml:math id="mml-inline22-1076998611420439">
<mml:msubsup>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
</mml:math>
</inline-formula> are unequal at θ by definition of continuity of a function. The estimates for <inline-formula id="inline-formula23-1076998611420439">
<mml:math id="mml-inline23-1076998611420439">
<mml:msubsup>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula24-1076998611420439">
<mml:math id="mml-inline24-1076998611420439">
<mml:msubsup>
<mml:mi>m</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
</mml:math>
</inline-formula> are defined as <inline-formula id="inline-formula25-1076998611420439">
<mml:math id="mml-inline25-1076998611420439">
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula26-1076998611420439">
<mml:math id="mml-inline26-1076998611420439">
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> for <inline-formula id="inline-formula27-1076998611420439">
<mml:math id="mml-inline27-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈&lt;</mml:mo>
</mml:math>
</inline-formula>. The coefficient vectors <inline-formula id="inline-formula28-1076998611420439">
<mml:math id="mml-inline28-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula29-1076998611420439">
<mml:math id="mml-inline29-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> are determined by minimizing the least square function (1) for <inline-formula id="inline-formula30-1076998611420439">
<mml:math id="mml-inline30-1076998611420439">
<mml:mover>
<mml:mi>t</mml:mi>
<mml:mo accent="false" stretchy="true">-</mml:mo>
</mml:mover>
<mml:mo stretchy="false">:=</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>t</mml:mi>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula31-1076998611420439">
<mml:math id="mml-inline31-1076998611420439">
<mml:mover>
<mml:mi>t</mml:mi>
<mml:mo accent="false" stretchy="true">-</mml:mo>
</mml:mover>
<mml:mo stretchy="false">:=</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula32-1076998611420439">
<mml:math id="mml-inline32-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mrow>
<mml:mrow>
<mml:mi mathvariant="normal">t</mml:mi>
</mml:mrow>
</mml:mrow>
<mml:mrow>
<mml:mrow>
<mml:mn mathvariant="normal">1</mml:mn>
</mml:mrow>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>, applying a one-sided kernel function <italic>K</italic> with support on <inline-formula id="inline-formula33-1076998611420439">
<mml:math id="mml-inline33-1076998611420439">
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula> satisfying <italic>K</italic>(0) &gt; 0 and <italic>K</italic>(<italic>u</italic>) ≥ 0 for <inline-formula id="inline-formula34-1076998611420439">
<mml:math id="mml-inline34-1076998611420439">
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula>. The change-point θ is estimated by choosing the date with maximum difference between the rhl and lhl, <inline-formula id="inline-formula35-1076998611420439">
<mml:math id="mml-inline35-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo form="prefix" movablelimits="false">arg</mml:mo>
<mml:mo form="prefix" movablelimits="true">max</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">:</mml:mo>
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>. If the assumption of a jump discontinuity in the first derivative in the considered regression model is not valid, the function <inline-formula id="inline-formula36-1076998611420439">
<mml:math id="mml-inline36-1076998611420439">
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is zero for all <inline-formula id="inline-formula37-1076998611420439">
<mml:math id="mml-inline37-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> and the respective maxima of <inline-formula id="inline-formula38-1076998611420439">
<mml:math id="mml-inline38-1076998611420439">
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">+</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">−</mml:mo>
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">−</mml:mo>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
</mml:math>
</inline-formula> is constant zero.</p>
<p>The principle of comparing the rhl and lhl of the first derivative of <italic>m</italic> was introduced by <xref ref-type="bibr" rid="bibr19-1076998611420439">Huh and Carriere (2002)</xref>, <xref ref-type="bibr" rid="bibr25-1076998611420439">Mueller (1992)</xref>, and <xref ref-type="bibr" rid="bibr26-1076998611420439">Mueller and Song (1997)</xref>. The estimator <inline-formula id="inline-formula39-1076998611420439">
<mml:math id="mml-inline39-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> is based on the presentation of <xref ref-type="bibr" rid="bibr19-1076998611420439">Huh and Carriere (2002)</xref>. Huh and Carriere observed better performance of <inline-formula id="inline-formula40-1076998611420439">
<mml:math id="mml-inline40-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> in a Monte Carlo simulation study compared to the estimator of <xref ref-type="bibr" rid="bibr25-1076998611420439">Mueller and Song (1997) and Mueller (1992)</xref>, who applied a kernel estimator instead of local polynomial approximation.</p>
</sec>
<sec id="section6-1076998611420439">
<title>Estimation strategy (2)</title>
<p>The second derivative of <italic>m</italic> does not exist and takes the value of infinity in case of a discontinuity of the first derivative in date θ. The second strategy of detecting a meaningful change is looking for dates with large values in the second derivative of <italic>m</italic>. An estimate of the second derivative is given by <inline-formula id="inline-formula41-1076998611420439">
<mml:math id="mml-inline41-1076998611420439">
<mml:msup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>2</mml:mn>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> for all <inline-formula id="inline-formula42-1076998611420439">
<mml:math id="mml-inline42-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> with <inline-formula id="inline-formula43-1076998611420439">
<mml:math id="mml-inline43-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">α</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> minimizing the least square function (1). This strategy results in the estimator <inline-formula id="inline-formula44-1076998611420439">
<mml:math id="mml-inline44-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo form="prefix" movablelimits="false">arg</mml:mo>
<mml:mo form="prefix" movablelimits="true">max</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">:</mml:mo>
<mml:msup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> for θ. When considering a regression model without a discontinuity of the first derivative <inline-formula id="inline-formula45-1076998611420439">
<mml:math id="mml-inline45-1076998611420439">
<mml:msup>
<mml:mi>m</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>, the second derivative does not take extreme values for any <inline-formula id="inline-formula46-1076998611420439">
<mml:math id="mml-inline46-1076998611420439">
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>.</p>
<p>
<xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels and Goderniaux (2004b)</xref> discussed a two-step procedure for the refinement of <inline-formula id="inline-formula47-1076998611420439">
<mml:math id="mml-inline47-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>. Briefly, a least square estimation procedure is implemented in a small neighborhood of <inline-formula id="inline-formula48-1076998611420439">
<mml:math id="mml-inline48-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> resulting in an improved estimator <inline-formula id="inline-formula49-1076998611420439">
<mml:math id="mml-inline49-1076998611420439">
<mml:msubsup>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msubsup>
</mml:math>
</inline-formula>. The mathematical details and an algorithm are given in <xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels and Goderniaux (2004b)</xref>.</p>
</sec>
<sec id="section7-1076998611420439">
<title>Estimation strategy (3)</title>
<p>The first derivative is modeled by a smooth function interrupted by a jump on position θ. The strategy for estimating the change-point θ is to separate the smooth curve and the jump factor as introduced by <xref ref-type="bibr" rid="bibr31-1076998611420439">Qiu and Yandell (1998)</xref>. A polynomial of second degree is fitted by the ordinary least square method in a neighborhood <inline-formula id="inline-formula50-1076998611420439">
<mml:math id="mml-inline50-1076998611420439">
<mml:mo stretchy="false">[</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi>h</mml:mi>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula> of <inline-formula id="inline-formula51-1076998611420439">
<mml:math id="mml-inline51-1076998611420439">
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> for all <inline-formula id="inline-formula52-1076998611420439">
<mml:math id="mml-inline52-1076998611420439">
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>T</mml:mi>
</mml:math>
</inline-formula> to obtain the coefficient vector <inline-formula id="inline-formula53-1076998611420439">
<mml:math id="mml-inline53-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. The difference operator <inline-formula id="inline-formula54-1076998611420439">
<mml:math id="mml-inline54-1076998611420439">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo form="prefix" movablelimits="true">max</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mfenced close="⌋" open="⌊">
<mml:mrow>
<mml:msup>
<mml:mi>n</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
<mml:mi>h</mml:mi>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">β</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mfenced close="⌋" open="⌊">
<mml:mrow>
<mml:msup>
<mml:mi>n</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
<mml:mi>h</mml:mi>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> is calculated and the estimator <inline-formula id="inline-formula55-1076998611420439">
<mml:math id="mml-inline55-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>3</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> is given by <inline-formula id="inline-formula56-1076998611420439">
<mml:math id="mml-inline56-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>3</mml:mn>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:mo form="prefix" movablelimits="false">arg</mml:mo>
<mml:mo form="prefix" movablelimits="true">max</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:mi>t</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">:</mml:mo>
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>. If a change in slope of the regression function <italic>m</italic> does not exist (first derivative of <italic>m</italic> is smooth) at θ, the maximum of <inline-formula id="inline-formula57-1076998611420439">
<mml:math id="mml-inline57-1076998611420439">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is close to zero for all <inline-formula id="inline-formula58-1076998611420439">
<mml:math id="mml-inline58-1076998611420439">
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula>.</p>
<p>The three estimation strategies provide an exploratory technique for estimating the change-point θ. Confidence intervals for the estimate <inline-formula id="inline-formula59-1076998611420439">
<mml:math id="mml-inline59-1076998611420439">
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> and estimates for the number of possible change-points are also important features for real data application. This article is focused on comparing the three nonparametric estimation strategies for θ regarding its application in real data analysis. We briefly describe possible approaches for addressing the two issues in this paragraph. Confidence intervals can be calculated by applying the bootstrap principle (<xref ref-type="bibr" rid="bibr3-1076998611420439">Carpenter &amp; Bithell, 2000</xref>; <xref ref-type="bibr" rid="bibr6-1076998611420439">Efron &amp; Tibshirani, 1993</xref>) to the change-point estimators. The asymptotic distributions of the estimators for θ are unknown to date for calculating exact confidence intervals. <xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels and Goderniaux (2004b)</xref> generalized the cross-validation criterion proposed by <xref ref-type="bibr" rid="bibr23-1076998611420439">Mueller and Stadtmueller (1999)</xref> for estimating the number of discontinuities in derivatives. The cross-validation principle is applied to the derivative estimate with a prespecified number of discontinuities. This procedure is repeated for a set of possible discontinuities and the minima of the corresponding cross-validation quantity is the estimate of interest. <xref ref-type="bibr" rid="bibr31-1076998611420439">Qiu and Yandell (1998)</xref> suggested another strategy. They calculated a threshold for the diagnostic function <inline-formula id="inline-formula60-1076998611420439">
<mml:math id="mml-inline60-1076998611420439">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. The number of discontinuities is then given by the not neighboring design points where the function <inline-formula id="inline-formula61-1076998611420439">
<mml:math id="mml-inline61-1076998611420439">
<mml:mi mathvariant="normal">Δ</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> exceeds this threshold.</p>
</sec>
</sec>
<sec id="section8-1076998611420439">
<title>Choosing a Kernel Function</title>
<p>Besides the bandwidth parameter <italic>h</italic>, the accuracy of the estimated regression function <inline-formula id="inline-formula62-1076998611420439">
<mml:math id="mml-inline62-1076998611420439">
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> depends on the kernel function <italic>K</italic>. The choice of the kernel does not have critical influence to the precision of <inline-formula id="inline-formula63-1076998611420439">
<mml:math id="mml-inline63-1076998611420439">
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> (<xref ref-type="bibr" rid="bibr15-1076998611420439">Hall, Lahiri, &amp; Polzehl, 1995</xref>). <xref ref-type="bibr" rid="bibr9-1076998611420439">Gasser, Mueller, and Mammitzsch (1985)</xref> presented optimal kernels with respect to minimizing the mean squared error of <inline-formula id="inline-formula64-1076998611420439">
<mml:math id="mml-inline64-1076998611420439">
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula>. The estimator <inline-formula id="inline-formula65-1076998611420439">
<mml:math id="mml-inline65-1076998611420439">
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> is calculated by applying the kernels proposed by <xref ref-type="bibr" rid="bibr9-1076998611420439">Gasser et al. (1985)</xref>. For example, the optimal kernel function <italic>K</italic> is the Epanechnikov kernel <inline-formula id="inline-formula66-1076998611420439">
<mml:math id="mml-inline66-1076998611420439">
<mml:mi>K</mml:mi>
<mml:mo stretchy="false">(</mml:mo>
<mml:mi>u</mml:mi>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:mrow>
<mml:mfrac>
<mml:mn>3</mml:mn>
<mml:mn>4</mml:mn>
</mml:mfrac>
</mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">−</mml:mo>
<mml:msup>
<mml:mi>u</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> for <inline-formula id="inline-formula67-1076998611420439">
<mml:math id="mml-inline67-1076998611420439">
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mi>u</mml:mi>
<mml:mrow>
<mml:mo>|</mml:mo>
</mml:mrow>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo stretchy="false">[</mml:mo>
<mml:mn>0</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">]</mml:mo>
</mml:math>
</inline-formula> when estimating the regression function <italic>m</italic> by a second-degree polynomial (<xref ref-type="bibr" rid="bibr9-1076998611420439">Gasser et al., 1985</xref>).</p>
</sec>
<sec id="section9-1076998611420439">
<title>Bandwidth Selection</title>
<p>The decision about the bandwidth and therefore about the smoothness of the estimated regression curve <inline-formula id="inline-formula68-1076998611420439">
<mml:math id="mml-inline68-1076998611420439">
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> and its derivative <inline-formula id="inline-formula69-1076998611420439">
<mml:math id="mml-inline69-1076998611420439">
<mml:msup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula> is of great importance in nonparametric regression analyses (<xref ref-type="bibr" rid="bibr14-1076998611420439">Haerdle, 1990</xref>, p. 179). The challenging problem in the selection of an appropriate bandwidth <italic>h</italic> is balancing the two competing issues: the residuals <inline-formula id="inline-formula70-1076998611420439">
<mml:math id="mml-inline70-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, <inline-formula id="inline-formula71-1076998611420439">
<mml:math id="mml-inline71-1076998611420439">
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">∈</mml:mo>
<mml:mo fence="false" stretchy="false">{</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>0</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo fence="false" stretchy="false">}</mml:mo>
</mml:math>
</inline-formula> of the regression model get smaller for a decreasing bandwidth <italic>h</italic>, whereas the variance of <inline-formula id="inline-formula72-1076998611420439">
<mml:math id="mml-inline72-1076998611420439">
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> gets larger (<xref ref-type="bibr" rid="bibr14-1076998611420439">Haerdle, 1990</xref>, p. 179).</p>
<p>Several methods have been discussed for determining the smoothing parameter <italic>h</italic> in regression models with uncorrelated and correlated errors in recent years. The basic principle of all algorithms is minimizing the averaged squared error (ASE) for the considered regression curve and their derivatives, respectively. Several techniques were developed for estimating the ASE in regression models: the generalized cross-validation (GCV) method (<xref ref-type="bibr" rid="bibr13-1076998611420439">Golub, Heath, &amp; Wahba, 1979</xref>), plug-in methods (<xref ref-type="bibr" rid="bibr5-1076998611420439">Chu &amp; Marron, 1991</xref>; <xref ref-type="bibr" rid="bibr30-1076998611420439">Opsomer, Wang, &amp; Yang, 2001</xref>; <xref ref-type="bibr" rid="bibr32-1076998611420439">Ruppert, 1997</xref>; <xref ref-type="bibr" rid="bibr33-1076998611420439">Ruppert, Sheather, &amp; Wand, 1995</xref>), and bootstrap-based methods (<xref ref-type="bibr" rid="bibr7-1076998611420439">Faraway &amp; Jhun, 1990</xref>; <xref ref-type="bibr" rid="bibr10-1076998611420439">Gijbels &amp; Goderniaux, 2004a</xref>; <xref ref-type="bibr" rid="bibr37-1076998611420439">Taylor, 1989</xref>). The basic principles of bandwidth selection for estimating derivatives of a regression function are similar to that of estimating the regression function <italic>m</italic> itself (<xref ref-type="bibr" rid="bibr14-1076998611420439">Haerdle, 1990</xref>, p. 194).<list list-type="order">
<list-item>
<p>The cross-validation approach evaluates the ability to get an estimate for <inline-formula id="inline-formula73-1076998611420439">
<mml:math id="mml-inline73-1076998611420439">
<mml:msup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msup>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> by omitting the data point <inline-formula id="inline-formula74-1076998611420439">
<mml:math id="mml-inline74-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> and <inline-formula id="inline-formula75-1076998611420439">
<mml:math id="mml-inline75-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>. The estimate of the first derivative is compared to the difference quotient at <inline-formula id="inline-formula76-1076998611420439">
<mml:math id="mml-inline76-1076998611420439">
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> instead of the observed outcome variable <inline-formula id="inline-formula77-1076998611420439">
<mml:math id="mml-inline77-1076998611420439">
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:math>
</inline-formula> for <inline-formula id="inline-formula78-1076998611420439">
<mml:math id="mml-inline78-1076998611420439">
<mml:mi>j</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mi>T</mml:mi>
</mml:math>
</inline-formula> (<xref ref-type="bibr" rid="bibr14-1076998611420439">Haerdle, 1990</xref>, p. 194). The estimator for <italic>h</italic> is easy to implement and does not require the estimation of the error variance. <xref ref-type="bibr" rid="bibr15-1076998611420439">Hall, Lahiri, and Polzehl (1995)</xref> introduced the leave-out cross-validation method to account for long-range or short-range dependent errors. The procedure differs in terms of leaving out a set of data points for calculating the cross-validation function. The estimate of <inline-formula id="inline-formula79-1076998611420439">
<mml:math id="mml-inline79-1076998611420439">
<mml:msubsup>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>i</mml:mi>
</mml:mrow>
<mml:mrow>
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:mrow>
</mml:msubsup>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is calculated for data <inline-formula id="inline-formula80-1076998611420439">
<mml:math id="mml-inline80-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mrow>
<mml:mi>i</mml:mi>
<mml:mo stretchy="false">+</mml:mo>
<mml:mi>l</mml:mi>
</mml:mrow>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula>, what stands for omitting <inline-formula id="inline-formula81-1076998611420439">
<mml:math id="mml-inline81-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:mn>2</mml:mn>
<mml:mi>l</mml:mi>
<mml:mo stretchy="false">−</mml:mo>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> data points.</p>
</list-item>
<list-item>
<p>The plug-in method for bandwidth selection aims to estimate the unknown quantities appearing in theoretical considerations of the asymptotical optimal bandwidth (<xref ref-type="bibr" rid="bibr32-1076998611420439">Ruppert, 1997</xref>; <xref ref-type="bibr" rid="bibr33-1076998611420439">Ruppert et al., 1995</xref>). The description of a plug-in bandwidth selector requires an exhaustive mathematical presentation; therefore, we refer to literature (<xref ref-type="bibr" rid="bibr32-1076998611420439">Ruppert, 1997</xref>; <xref ref-type="bibr" rid="bibr33-1076998611420439">Ruppert et al., 1995</xref>) for further reading. The plug-in-based bandwidths are less variable and asymptotical superior compared with cross validation (<xref ref-type="bibr" rid="bibr21-1076998611420439">Loader, 1999</xref>; <xref ref-type="bibr" rid="bibr33-1076998611420439">Ruppert et al., 1995</xref>). Ruppert (1997) proposed a well performing empirical bias bandwidth estimator for derivative estimation.</p>
</list-item>
<list-item>
<p>The application of the bootstrap principle establishes another possibility for estimating a bandwidth parameter by minimizing the ASE. <xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels and Goderniaux (2004b)</xref> discussed the bootstrap principle in selecting the bandwidth <italic>h</italic> in derivative estimation. The algorithm is based on bootstrapping the centered residuals <inline-formula id="inline-formula82-1076998611420439">
<mml:math id="mml-inline82-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">=</mml:mo>
<mml:msub>
<mml:mi>Y</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
<mml:mo stretchy="false">−</mml:mo>
<mml:mover accent="true">
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo accent="true" stretchy="false">-</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> (<inline-formula id="inline-formula83-1076998611420439">
<mml:math id="mml-inline83-1076998611420439">
<mml:mover accent="true">
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo accent="true" stretchy="false">-</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> denotes the mean value for the estimated residuals) estimated by a pilot estimator <inline-formula id="inline-formula84-1076998611420439">
<mml:math id="mml-inline84-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi>m</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula>. Details about the procedure are provided by <xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels and Goderniaux (2004b)</xref>. <xref ref-type="bibr" rid="bibr15-1076998611420439">Hall et al. (1995)</xref> introduced a block bootstrap approach to account for the dependence of errors. The principle of the block bootstrap is similar compared to the bootstrap algorithm above except for the resampling step. Here, the vector of centered residuals <inline-formula id="inline-formula85-1076998611420439">
<mml:math id="mml-inline85-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mi>T</mml:mi>
</mml:msub>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> is grouped into <italic>k</italic> blocks of Length <italic>1</italic>, where <italic>k</italic> is a multiple of 1. Resampling randomly with replacement <italic>k</italic> times the <italic>k</italic> blocks results in a bootstrap version <inline-formula id="inline-formula86-1076998611420439">
<mml:math id="mml-inline86-1076998611420439">
<mml:mo stretchy="false">(</mml:mo>
<mml:msubsup>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mn>1</mml:mn>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msubsup>
<mml:mo stretchy="false">,</mml:mo>
<mml:mo stretchy="false">…</mml:mo>
<mml:mo stretchy="false">,</mml:mo>
<mml:msubsup>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mi>T</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msubsup>
<mml:mo stretchy="false">)</mml:mo>
</mml:math>
</inline-formula> of the residuals. The detailed theory for choosing <italic>h</italic> by block bootstrap is described in <xref ref-type="bibr" rid="bibr15-1076998611420439">Hall et al. (1995)</xref>.</p>
</list-item>
</list>
</p>
</sec>
</sec>
<sec id="section10-1076998611420439">
<title>Monte Carlo Simulation Study</title>
<p>A Monte Carlo simulation study was conducted to compare the performance of the three estimators introduced above. The study is designed to address the following issues:<list list-type="order">
<list-item>
<p>What estimator best detects the meaningful change-point θ in the case of short-range and long-range dependent errors? The convergence of the three estimators of <xref ref-type="bibr" rid="bibr19-1076998611420439">Huh and Carierre (2002)</xref>, <xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels and Goderniaux (2004b)</xref>, <xref ref-type="bibr" rid="bibr31-1076998611420439">Qiu and Yandell 1998</xref>) was shown in the case of independent and identically distributed errors. The finite sample behavior is investigated in case of correlated errors in the simulation study.</p>
</list-item>
<list-item>
<p>What bandwidth selection approach is best suited to obtain an accurate estimate of the change-point θ? The important issue of estimating θ has not yet been addressed in detail in the literature. Previous studies and discussions of the bandwidth parameter are not transferable to our data situation. The authors assumed a simple model with independent and identically distributed errors (<xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels &amp; Goderniaux, 2004b</xref>). The compared bandwidth selection criteria are the GCV, the plug-in estimator proposed by <xref ref-type="bibr" rid="bibr32-1076998611420439">Ruppert (1997)</xref>, and the block bootstrap (<xref ref-type="bibr" rid="bibr15-1076998611420439">Hall et al., 1995</xref>) based on theoretical considerations in literature. The performance of block bootstrapping depends on the block length <italic>1</italic>. It is a function of the range of dependency, as higher the range of dependency as larger should be the block length (<xref ref-type="bibr" rid="bibr15-1076998611420439">Hall et al., 1995</xref>). The block length <italic>1</italic> varies for values of 1, 2, 5, and 10 in the simulation study.</p>
</list-item>
<list-item>
<p>The number of design points <italic>T</italic> is an important parameter in respect to practical application of the nonparametric change-point model. The number of longitudinally measured data is assumed to be 15, 20, 50, 100, and 500, respectively. The long-range and short-range dependent errors are given by the flexible autocovariance function used in <xref ref-type="bibr" rid="bibr15-1076998611420439">Hall et al. (1995)</xref>. The autocovariance is given by <inline-formula id="inline-formula87-1076998611420439">
<mml:math id="mml-inline87-1076998611420439">
<mml:mi>E</mml:mi>
<mml:mo stretchy="false">[</mml:mo>
<mml:msub>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:msub>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
<mml:mo stretchy="false">]</mml:mo>
<mml:mo stretchy="false">=</mml:mo>
<mml:msup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
<mml:mrow>
<mml:msup>
<mml:mi>
</mml:mi>
<mml:mo stretchy="false">∗</mml:mo>
</mml:msup>
</mml:mrow>
<mml:msup>
<mml:mi>e</mml:mi>
<mml:mrow>
<mml:mo stretchy="false">−</mml:mo>
<mml:mi mathvariant="italic">λ</mml:mi>
<mml:mfenced close="|" open="|">
<mml:mrow>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
<mml:mo stretchy="false">−</mml:mo>
<mml:msub>
<mml:mi>t</mml:mi>
<mml:mi>j</mml:mi>
</mml:msub>
</mml:mrow>
</mml:mfenced>
</mml:mrow>
</mml:msup>
</mml:math>
</inline-formula>. The parameter <inline-formula id="inline-formula88-1076998611420439">
<mml:math id="mml-inline88-1076998611420439">
<mml:msup>
<mml:mi mathvariant="italic">σ</mml:mi>
<mml:mn>2</mml:mn>
</mml:msup>
</mml:math>
</inline-formula> is the variance of the standard normal distributed errors <inline-formula id="inline-formula89-1076998611420439">
<mml:math id="mml-inline89-1076998611420439">
<mml:msub>
<mml:mi mathvariant="italic">ϵ</mml:mi>
<mml:mi>i</mml:mi>
</mml:msub>
</mml:math>
</inline-formula>. We study variances of .1 and .5. The range of dependency is given by the parameter λ in the autocovariance function. We model the two extreme cases of λ = 10 (long-range dependent data) and λ = 1,000 (short-range dependent data) as well as values for λ of 50.</p>
</list-item>
</list>
</p>
<p>We consider the regression functions (1) <italic>m</italic>
<sub>1</sub>(<italic>t</italic>) = .5 for <italic>t</italic> &lt; .5 and <italic>m</italic>
<sub>1</sub>(<italic>t</italic>) = exp(<italic>t</italic> − .5) − 0.5 for <italic>t</italic> ≥ .5; (2) <italic>m</italic>
<sub>2</sub>(<italic>t</italic>) = .25 for <italic>t</italic> &lt; .5 and <italic>m</italic>
<sub>2</sub>(<italic>t</italic>) = .7<italic>t</italic>
<sup>2</sup> + .4<italic>t</italic> − .1 for <italic>t</italic> ≥ .5; (3) <italic>m</italic>
<sub>3</sub>(<italic>t</italic>) = .84 for <italic>t</italic> &lt;.5 and <italic>m</italic>
<sub>3</sub>(<italic>t</italic>) = sin (.7π<italic>t</italic> − .1) for <italic>t</italic> ≥ .5; and (4) <italic>m</italic>
<sub>4</sub>(<italic>t</italic>) =.1<italic>t</italic> + .79 for <italic>t</italic> &lt; .5 and <italic>m</italic>
<sub>4</sub>(<italic>t</italic>) = sin (.7π<italic>t</italic> − .1) for <italic>t</italic> ≥ .5. The functions <italic>m</italic>
<sub>1</sub> and <italic>m</italic>
<sub>2</sub> are constant for values of <italic>t</italic> smaller than .5 and increasing values for <italic>t</italic> ≥.5. The two functions mainly differ in the magnitude of the jump in the first derivative. The regression lines <italic>m</italic>
<sub>3</sub> and <italic>m</italic>
<sub>4</sub> have a jump in the first derivative and maxima in the function itself, which provides a challenging estimation problem. Regression line <italic>m</italic>
<sub>3</sub> is constant and <italic>m</italic>
<sub>4</sub> slightly increasing for <italic>t</italic> &lt; .5. The nonparametric estimators HC, GG, and QY are compared to the parametric SR method (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>).</p>
<p>The results are reported in <xref ref-type="fig" rid="fig2-1076998611420439">Figure 2</xref> for the regression functions <italic>m</italic>
<sub>2</sub> and <italic>m</italic>
<sub>3</sub>, a variance of .1, λ = 10 and the bandwidth selections for various values of <italic>T</italic>. Results were comparable to <italic>m</italic>
<sub>2</sub> and <italic>m</italic>
<sub>3</sub> as reported in <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1</xref> for the regression functions <italic>m</italic>
<sub>1</sub>, <italic>m</italic>
<sub>4</sub>, and other parameter settings. The detailed results are available on request from the authors. The estimation methods HC consistently yielded acceptable bias in all simulations applying the plug-in estimator with polynomial of degree 3. The bootstrap method with block length 10 (not reported in <xref ref-type="fig" rid="fig1-1076998611420439">Figure 1</xref>) resulted in acceptable estimates for values of <italic>T</italic> above 50 by comparing the rhl and lhl of the first derivative. The biases of HC were smaller than 2 measurement points in all simulation scenarios. HC underestimated the change-point θ in all cases (not shown in the figure, the absolute values of bias are reported). The bias was somewhat smaller for the bootstrap bandwidth selection with block length 10 (<italic>T</italic> ≥ 50) compared to plug-in bandwidth selection with polynomial of degree 3 (PI_2), whereas the variation (root mean square error [RMSE]) for bootstrap bandwidth selection with block length of 10 (BB_10) was larger. All bandwidth selection methods were asymptotical equivalent for large sets of measurement points (<italic>T</italic> = 500).</p>
<fig id="fig2-1076998611420439" position="float">
<label>Figure 2.</label>
<caption>
<p>Absolute value of bias (dots) and root mean square error (RMSE; range plot) for the four change-point estimators Huh and Carierre (HC), Qiu and Yandell (QY), Gijbels and Goderniaux (GG), and segmented regression (SR) for regression functions (A) <italic>m</italic>
<sub>2</sub> and (B) <italic>m</italic>
<sub>3</sub> (Bias = absolute value of bias; RMSE = root mean square error; I = Plug-in bandwidth selection with polynomial of degree 1; II = Plug-in bandwidth selection with polynomial of degree 3; III = generalized cross-validation bandwidth selection; IV = bootstrap bandwidth selection with block length of 1; V = bootstrap bandwidth selection with block length of 5).</p>
</caption>
<graphic xlink:href="10.3102_1076998611420439-fig2.tif"/>
</fig>

<p>The GG estimator performed well for the regression setting <italic>m</italic>
<sub>1</sub> and <italic>m</italic>
<sub>2</sub>, in contrast to the other assumed regression functions <italic>m</italic>
<sub>3</sub> and <italic>m</italic>
<sub>4</sub>, where the bias was considerable. Regression functions <italic>m</italic>
<sub>1</sub>, <italic>m</italic>
<sub>2</sub>, <italic>m</italic>
<sub>3</sub>, and <italic>m</italic>
<sub>4</sub> are characterized by a single kink at <italic>T</italic> = .5, whereas <italic>m</italic>
<sub>3</sub> and <italic>m</italic>
<sub>4</sub> additionally have a curvilinear shape with a maximum for <italic>T</italic> &gt; .5. It is also notable that the first derivatives for <italic>m</italic>
<sub>3</sub> and <italic>m</italic>
<sub>4</sub> have a steep decrease for <italic>T</italic> &gt; .5. The GG method estimated the maxima point of <italic>m</italic>
<sub>3</sub> and <italic>m</italic>
<sub>4</sub>, although the first derivative is smooth at this interval. Already <xref ref-type="bibr" rid="bibr11-1076998611420439">Gijbels and Goderniaux (2004b)</xref> has mentioned the estimation problems in derivatives with a steep increase and decrease, respectively. GG resulted in acceptable bias and RMSE estimates for large sets of design points.</p>
<p>There was no consistent pattern for good performance of QY. The plug-in estimator with polynomial of degree 3 acceptably worked in some scenarios, whereas it failed in other simulation settings.</p>
<p>The nonparametric estimators were also compared with the parametric SR method. It was not surprising that SR outperforms the nonparametric methods for regression functions <italic>m</italic>
<sub>1</sub> and <italic>m</italic>
<sub>2</sub> with its single kink at <italic>T</italic> = .5. SR failed in the more complex cases of <italic>m</italic>
<sub>3</sub> and <italic>m</italic>
<sub>4</sub> independent of the sample size <italic>T</italic> and is not suitable for estimation problems with nonlinear regression functions.</p>
<p>The estimated bandwidths for several selection methods are presented in <xref ref-type="table" rid="table1-1076998611420439">Table 1</xref> for λ = 10. The plug-in bandwidth estimators yielded bandwidths with slight variability and the bootstrap-estimated bandwidths were more variable, regardless of the chosen block length. The mean bandwidth of GCV was in between the plug-in with polynomial of degree <italic>1</italic> and plug-in with polynomial of degree 3. The bootstrap-estimated bandwidths were smaller than the plug-in with polynomial of degree 3 and were comparable to the GCV.</p>
<table-wrap id="table1-1076998611420439" position="float">
<label>Table 1.</label>
<caption>
<p>Mean and Standard Deviation of Estimated Bandwidths by Various Bandwidth Selection Methods and Sample Sizes T for Regression Functions m1, m2, m3, and m4, and λ = 10</p>
</caption>
<graphic alternate-form-of="table1-1076998611420439" xlink:href="10.3102_1076998611420439-table1.tif"/>
<graphic alternate-form-of="table1-1076998611420439" xlink:href="10.3102_1076998611420439-table1a.tif"/>
<table>
<thead>
<tr>
<th>
<italic>
</italic>
</th>
<th>
</th>
<th colspan="2">PI1</th>
<th colspan="2">PI3</th>
<th colspan="2">GCV</th>
<th colspan="2">BB1</th>
<th colspan="3">BB2</th>
<th colspan="2">BB5</th>
<th colspan="3">BB10</th>
</tr>
<tr>
<th>
<italic>m</italic>
</th>
<th>
<italic>T</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
</th>
<th>
<italic>SD</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
<italic>SD</italic>
</th>
<th>
<italic>M</italic>
</th>
<th>
</th>
<th>
<italic>SD</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<italic>m</italic>
<sub>1</sub>
</td>
<td>15</td>
<td>.266</td>
<td>.001</td>
<td>.352</td>
<td>.021</td>
<td>.340</td>
<td>.047</td>
<td>.263</td>
<td>.040</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
<td>.278</td>
<td>.028</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>20</td>
<td>.254</td>
<td>.001</td>
<td>.357</td>
<td>.019</td>
<td>.342</td>
<td>.037</td>
<td>.271</td>
<td>.030</td>
<td>.272</td>
<td rowspan="4">
</td>
<td>.024</td>
<td>.271</td>
<td>.031</td>
<td>
</td>
<td>
<sup>_b</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>50</td>
<td>.213</td>
<td>.001</td>
<td>.336</td>
<td>.009</td>
<td>.245</td>
<td>.040</td>
<td>.243</td>
<td>.039</td>
<td>.242</td>
<td>.044</td>
<td>.242</td>
<td>.037</td>
<td>.251</td>
<td rowspan="3">
</td>
<td>.039</td>
</tr>
<tr>
<td>
</td>
<td>100</td>
<td>.186</td>
<td>.001</td>
<td>.315</td>
<td>.006</td>
<td>.197</td>
<td>.029</td>
<td>.205</td>
<td>.030</td>
<td>.212</td>
<td>.037</td>
<td>.222</td>
<td>.045</td>
<td>.225</td>
<td>.043</td>
</tr>
<tr>
<td>
</td>
<td>500</td>
<td>.136</td>
<td>.000</td>
<td>.267</td>
<td>.003</td>
<td>.136</td>
<td>.014</td>
<td>.144</td>
<td>.015</td>
<td>.140</td>
<td>.019</td>
<td>.141</td>
<td>.019</td>
<td>.146</td>
<td>.024</td>
</tr>
<tr>
<td>
<italic>m</italic>
<sub>2</sub>
</td>
<td>15</td>
<td>.266</td>
<td>.001</td>
<td>.336</td>
<td>.023</td>
<td>.335</td>
<td>.040</td>
<td>.264</td>
<td>.040</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
<td>.275</td>
<td>.032</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>20</td>
<td>.254</td>
<td>.001</td>
<td>.336</td>
<td>.020</td>
<td>.334</td>
<td>.037</td>
<td>.266</td>
<td>.031</td>
<td>.270</td>
<td rowspan="4">
</td>
<td>.025</td>
<td>.265</td>
<td>.032</td>
<td>
</td>
<td>
<sup>_b</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>50</td>
<td>.215</td>
<td>.001</td>
<td>.323</td>
<td>.014</td>
<td>.232</td>
<td>.039</td>
<td>.232</td>
<td>.040</td>
<td>.229</td>
<td>.037</td>
<td>.237</td>
<td>.040</td>
<td>.250</td>
<td>
</td>
<td>.040</td>
</tr>
<tr>
<td>
</td>
<td>100</td>
<td>.187</td>
<td>.000</td>
<td>.308</td>
<td>.009</td>
<td>.184</td>
<td>.028</td>
<td>.199</td>
<td>.029</td>
<td>.203</td>
<td>.033</td>
<td>.217</td>
<td>.044</td>
<td>.227</td>
<td>
</td>
<td>.042</td>
</tr>
<tr>
<td>
</td>
<td>500</td>
<td>.137</td>
<td>.000</td>
<td>.264</td>
<td>.003</td>
<td>.115</td>
<td>.013</td>
<td>.139</td>
<td>.017</td>
<td>.138</td>
<td>.017</td>
<td>.141</td>
<td>.018</td>
<td>.147</td>
<td>
</td>
<td>.029</td>
</tr>
<tr>
<td>
<italic>m</italic>
<sub>3</sub>
</td>
<td>15</td>
<td>.194</td>
<td>.003</td>
<td>.306</td>
<td>.017</td>
<td>.319</td>
<td>.045</td>
<td>.247</td>
<td>.043</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
<td>.269</td>
<td>0.35</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>20</td>
<td>.180</td>
<td>.002</td>
<td>.305</td>
<td>.014</td>
<td>.319</td>
<td>.037</td>
<td>.260</td>
<td>.036</td>
<td>.264</td>
<td rowspan="4">
</td>
<td>.035</td>
<td>.264</td>
<td>.035</td>
<td>
</td>
<td>
<sup>_b</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>50</td>
<td>.152</td>
<td>.001</td>
<td>.290</td>
<td>.010</td>
<td>.216</td>
<td>.034</td>
<td>.213</td>
<td>.036</td>
<td>.215</td>
<td>.037</td>
<td>.220</td>
<td>.039</td>
<td>.230</td>
<td>
</td>
<td>.043</td>
</tr>
<tr>
<td>
</td>
<td>100</td>
<td>.133</td>
<td>.001</td>
<td>.276</td>
<td>.008</td>
<td>.178</td>
<td>.024</td>
<td>.183</td>
<td>.023</td>
<td>.185</td>
<td>.028</td>
<td>.188</td>
<td>.036</td>
<td>.212</td>
<td>
</td>
<td>.044</td>
</tr>
<tr>
<td>
</td>
<td>500</td>
<td>.097</td>
<td>.000</td>
<td>.235</td>
<td>.003</td>
<td>.125</td>
<td>.012</td>
<td>.130</td>
<td>.013</td>
<td>.130</td>
<td>.015</td>
<td>.128</td>
<td>.015</td>
<td>.134</td>
<td>
</td>
<td>.023</td>
</tr>
<tr>
<td>
<italic>m</italic>
<sub>4</sub>
</td>
<td>15</td>
<td>.196</td>
<td>.003</td>
<td>.305</td>
<td>.017</td>
<td>.325</td>
<td>.046</td>
<td>.254</td>
<td>.041</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
<td>.272</td>
<td>.034</td>
<td>
</td>
<td>
<sup>_a</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>20</td>
<td>.182</td>
<td>.002</td>
<td>.304</td>
<td>.015</td>
<td>.327</td>
<td>.038</td>
<td>.263</td>
<td>.034</td>
<td>.269</td>
<td rowspan="4">
</td>
<td>.031</td>
<td>.265</td>
<td>.033</td>
<td>
</td>
<td>
<sup>_b</sup>
</td>
<td>
</td>
</tr>
<tr>
<td>
</td>
<td>50</td>
<td>.152</td>
<td>.001</td>
<td>.288</td>
<td>.011</td>
<td>.226</td>
<td>.036</td>
<td>.218</td>
<td>.037</td>
<td>.216</td>
<td>.035</td>
<td>.225</td>
<td>.039</td>
<td>.232</td>
<td>
</td>
<td>.042</td>
</tr>
<tr>
<td>
</td>
<td>100</td>
<td>.133</td>
<td>.001</td>
<td>.274</td>
<td>.008</td>
<td>.184</td>
<td>.025</td>
<td>.190</td>
<td>.023</td>
<td>.194</td>
<td>.026</td>
<td>.193</td>
<td>.037</td>
<td>.208</td>
<td>
</td>
<td>.040</td>
</tr>
<tr>
<td>
</td>
<td>500</td>
<td>.097</td>
<td>.000</td>
<td>.233</td>
<td>.003</td>
<td>.129</td>
<td>.012</td>
<td>.135</td>
<td>.014</td>
<td>.134</td>
<td>.016</td>
<td>.135</td>
<td>.016</td>
<td>.135</td>
<td>
</td>
<td>.023</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1076998611420439">
<p>
<italic>Note:</italic> PI1 = Plug-in bandwidth selection with polynomial of degree 1; PI3 = Plug-in bandwidth selection with polynomial of degree 3; GCV = generalized cross validation bandwidth selection; BB1 = bootstrap bandwidth selection with block length of 1; BB2 = bootstrap bandwidth selection with block length of 2; BB5 = bootstrap bandwidth selection with block length of 5; BB10 = bootstrap bandwidth selection with block length of 10</p>
</fn>
<fn id="table-fn2-1076998611420439">
<p>
<sup>a</sup>Block length of 2 and 10 not applicable with <italic>T</italic> = 15</p>
</fn>
<fn id="table-fn3-1076998611420439">
<p>
<sup>b</sup>Estimation procedure failed with 2 blocks of length 10 for <italic>T</italic> = 20. </p>
</fn>
</table-wrap-foot>
</table-wrap>
<p>Summarizing the results of the Monte Carlo simulation study, the estimator of Huh and Carriere (<xref ref-type="bibr" rid="bibr19-1076998611420439">Huh &amp; Carriere, 2002</xref>) performed consistently by applying the plug-in bandwidth with polynomial of degree 3 and the block bootstrap (block length 10 in case of large sample sizes). The plug-in bandwidth method resulted in less variable estimates compared to the others.</p>
</sec>
<sec id="section11-1076998611420439">
<title>Real Data Application: Performance-Related Anxiety</title>
<sec id="section12-1076998611420439">
<title>Musician Performance Data</title>
<p>The sample is comprised of five undergraduate music majors studying at a large university in the United States selected based on high levels of self-reported performance anxiety using the Performance Anxiety Scale (PAS; <xref ref-type="bibr" rid="bibr28-1076998611420439">Nagel, Himle, &amp; Papsdorf, 1981</xref>). The participants' mean PAS score was &gt;1 <italic>SD</italic> above a previously established pretreatment mean (<xref ref-type="bibr" rid="bibr29-1076998611420439">Nagel, Himle, &amp; Papsdorf, 1989</xref>), thereby clearly representing individuals with a high propensity toward performance-related anxiety and impairment.</p>
<p>Participants recorded their data on a Palm <italic>m</italic>105 handheld computer and were instructed in their use before taking them home for a 1-week assessment period (Gloster &amp; Klotsche, under review). Each participant recorded their responses to relevant performance anxiety questions on the handheld computer for 6 days prior to, the day of, and 1 day following a final performance examination in front of a jury of professors. The assessments were scheduled in an accelerating fashion such that the participants recorded 6 times (every 144 minutes) on Days 1 and 2, 7 times (every 120 minutes) on Days 3 through 6, 8 times on Day 7 (the performance day; every 90 minutes plus 10 minutes prior to and following the performance), and 5 times (every 144 minutes) on Day 8. The handheld computer signaled scheduled assessment via a loud set of beeps that repeated every 60 seconds until the participant responded. All data entered into the handheld computer were time stamped.</p>
<p>The data were collected via the handheld computer using the Worry–Anxious–Apprehension–Fear Scale (WAAF). The WAAF is based on the concept of a threat imminence continuum (Craske, 1999) and assesses five domains of anxiety believed to change with increasing threat: cognitions, mental images, emotions, self-perceived autonomic arousal, and avoidance. Each of the five domains consists of four questions. The WAAF total score is the sum of the 20 individual items.</p>
</sec>
<sec id="section13-1076998611420439">
<title>Results</title>
<p>The values of the WAAF total score across time are presented in <xref ref-type="fig" rid="fig3-1076998611420439">Figure 3</xref> for the five participants and averaged across the five individuals (dots). The raw data were investigated for within-day patterns of anxiety. A consistent pattern of anxiety could not be found within a day across the study period. It is remarkable that the first measurement of the WAAF score was on a low level for the first 2 days compared to the other within-day measurements. The regression lines (solid line, <xref ref-type="fig" rid="fig3-1076998611420439">Figure 3</xref>) were individually estimated for every music major by local polynomial regression as introduced in the method section applying the plug-in estimator of <xref ref-type="bibr" rid="bibr32-1076998611420439">Ruppert (1997)</xref>. The overall regression curve was calculated across the five music majors. The dashed vertical line represents the event of the performance examination in front of a jury of professors. The levels of the WAAF total score varied individually. A consistent pattern existed for the score across time as can be seen in <xref ref-type="fig" rid="fig3-1076998611420439">Figure 3</xref>. The scores remained constant or decreased during the baseline period, likely due to familiarization with the study design, followed by an increase of the score before the performance examination and a strong decrease after the performance. The key question, however, is when exactly did the subjective assessments begin to increase above and beyond the baseline level. Identification of this point might suggest the point at which the impending performance becomes subjectively salient.</p>
<fig id="fig3-1076998611420439" position="float">
<label>Figure 3.</label>
<caption>
<p>The individual raw Worry–Anxious–Apprehension–Fear Scale (WAAF) total scores across time for the five music majors and overall (dots: Individual values; solid line: Estimated nonparametric regression curve; dashed line: Event performance examination; dotted line: Estimated change-points).</p>
</caption>
<graphic xlink:href="10.3102_1076998611420439-fig3.tif"/>
</fig>
<p>The estimation method of <xref ref-type="bibr" rid="bibr19-1076998611420439">Huh and Carriere (2002)</xref> was applied to estimate a meaningful change in the WAAF total score across time individually for the five music majors and across the data. The diagnostic function rhl − lhl (rhl of the first derivative minus lhl of the first derivative) is shown for the five participants and overall in <xref ref-type="fig" rid="fig4-1076998611420439">Figure 4</xref>. The estimated change-point was given at the maximum of the diagnostic function rhl − lhl. It is supported by <xref ref-type="fig" rid="fig4-1076998611420439">Figure 4</xref> and the plot of the estimated regression lines in <xref ref-type="fig" rid="fig3-1076998611420439">Figure 3</xref> that there was one change-point for music major 1 (<inline-formula id="inline-formula90-1076998611420439">
<mml:math id="mml-inline90-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 31), music major 2 (<inline-formula id="inline-formula91-1076998611420439">
<mml:math id="mml-inline91-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 42), and two change-points for music major 3 (<inline-formula id="inline-formula92-1076998611420439">
<mml:math id="mml-inline92-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 30, <inline-formula id="inline-formula93-1076998611420439">
<mml:math id="mml-inline93-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 43), music major 4 (<inline-formula id="inline-formula94-1076998611420439">
<mml:math id="mml-inline94-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 21, <inline-formula id="inline-formula95-1076998611420439">
<mml:math id="mml-inline95-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 41), music major 5 (<inline-formula id="inline-formula96-1076998611420439">
<mml:math id="mml-inline96-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 23, <inline-formula id="inline-formula97-1076998611420439">
<mml:math id="mml-inline97-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 41), and overall (<inline-formula id="inline-formula98-1076998611420439">
<mml:math id="mml-inline98-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 27, <inline-formula id="inline-formula99-1076998611420439">
<mml:math id="mml-inline99-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 41). For example, the mean overall WAAF total score across time could be described by a decrease up to <inline-formula id="inline-formula100-1076998611420439">
<mml:math id="mml-inline100-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 27, a flat increase up to <inline-formula id="inline-formula101-1076998611420439">
<mml:math id="mml-inline101-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>2</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 41 followed by a steep increase up to the performance examination. The graph of rhl − lhl for music major 2 had a second local maxima at <inline-formula id="inline-formula102-1076998611420439">
<mml:math id="mml-inline102-1076998611420439">
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
</mml:math>
</inline-formula> = 16. This local maxima was caused by the boundary problem, a less accurate estimated smoothed mean at the boundaries of the observed time interval as suggested by the comparison with the estimated regression line in <xref ref-type="fig" rid="fig4-1076998611420439">Figure 4</xref>. A similar pattern existed for music major 3, but the maxima at <inline-formula id="inline-formula103-1076998611420439">
<mml:math id="mml-inline103-1076998611420439">
<mml:msub>
<mml:mover accent="true">
<mml:mi mathvariant="italic">θ</mml:mi>
<mml:mo accent="true" stretchy="false">ˆ</mml:mo>
</mml:mover>
<mml:mn>1</mml:mn>
</mml:msub>
</mml:math>
</inline-formula> = 30 was associated with a change in slope in contrast to music major 2.</p>
<fig id="fig4-1076998611420439" position="float">
<label>Figure 4.</label>
<caption>
<p>Diagnostic function right-hand limit minus left-hand limit of the first derivative in the estimation algorithm of Huh and Carriére (solid lines) and the selected change-points (vertical dotted lines).</p>
</caption>
<graphic xlink:href="10.3102_1076998611420439-fig4.tif"/>
</fig>
<p>SR analysis (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>) was also applied to the musician performance data. Using multiple sets of starting values, a variety of possible estimates for θ were calculated (data not shown). The ambiguous results obtained when using segmented regression analysis could be explained by the large variance of the data and the nonlinear course of performance anxiety across time as indicated by <xref ref-type="fig" rid="fig3-1076998611420439">Figure 3</xref>. The association between WAAF total score and time was modeled by a segmented linear function in the SR analysis.</p>
</sec>
</sec>
<sec id="section14-1076998611420439">
<title>Discussion</title>
<p>The identification of the change-point at which an observed outcome variable begins to meaningfully change above and beyond baseline fluctuations is an important research question in longitudinal data analysis. We considered a nonparametric modeling framework for estimating the change-point. Three possible exploratory estimation procedures were compared in a Monte Carlo simulation study. The estimator of Huh and Carriere (<xref ref-type="bibr" rid="bibr19-1076998611420439">2002</xref>) yielded small biases and RMSE across many simulation settings. The estimator is based on a comparison of the lhl and rhl of the first derivative of the underlying unknown regression function. This procedure was applied to longitudinal data collected on performance anxiety of five musicians as they approach a performance. The estimation procedure HC yielded reasonable estimates of θ, whereas the parametric SR analyses failed. This method can be applied when a meaningful change-point is of interest in exploratory data analyses.</p>
<p>The measurements of anxiety levels (WAAF total score) were nested within day in our real data example. A systematic within-day pattern of performance anxiety has not been observed. It was not possible to account for the nested within-day observation in the discussed estimation framework, for example, as in random effect models. The dependent errors were taken into account by the bandwidth selection method.</p>
<p>It was shown in the simulation study that the estimator of Huh and Carierre performed well when only a small number of design points (<italic>T</italic> ≥ 15) are available. The estimator yielded ambiguous estimates, however, for design points less than 15 with results that were too variable. The estimator yielded also plausible results in the musician performance data used in this article. It is notable that the variance of the estimated regression functions was large (Participant 1: 47.0; Participant 2: 96.4; Participant 3: 55.4; Participant 4: 102.9; Participant 5: 74.1; and overall: 31.1). In contrast, the SR approach (<xref ref-type="bibr" rid="bibr24-1076998611420439">Muggeo, 2003</xref>) resulted in implausible and variable estimates for the change-point depending on starting values for the estimation algorithm and the hypothesized functional pattern in for each segment.</p>
<p>Other methods on change-point detection in derivatives based on kernel smoothers exist. They include the work of <xref ref-type="bibr" rid="bibr4-1076998611420439">Cheng and Raimondo (2008)</xref> in the time-series framework. They provided an estimator for the change-point θ based on the zero crossing technique. The change-point in the first derivative is estimated by applying a kernel approximation of the third derivative. The method resulted in good simulation results for large sets of design points (<italic>T</italic> = 1,000) while decreasing performance for <italic>T</italic> ≤ 200. One might expect a worse performance in the face of real data applications with limited design points of <italic>T</italic> &lt; 25. The estimation algorithm of <xref ref-type="bibr" rid="bibr4-1076998611420439">Cheng and Raimondo (2008)</xref> relies on an estimate of the third derivative. Higher-order derivatives are hard to estimate and large bandwidths are required for acceptable estimates (<xref ref-type="bibr" rid="bibr31-1076998611420439">Qiu &amp; Yandell, 1998</xref>). This fact provides a limitation in real data analyses with smaller sets of design points. The estimation problem of θ can also be addressed by spline smoothing (<xref ref-type="bibr" rid="bibr1-1076998611420439">Barry, 2002</xref>) and the wavelet approximation (<xref ref-type="bibr" rid="bibr39-1076998611420439">Wang, 1995</xref>) besides nonparametric regression analyses.</p>
<p>
<xref ref-type="bibr" rid="bibr21-1076998611420439">Loader (1999)</xref> compared the cross-validation criteria (<xref ref-type="bibr" rid="bibr13-1076998611420439">Golub et al., 1979</xref>) with the plug-in estimator of <xref ref-type="bibr" rid="bibr33-1076998611420439">Ruppert et al. (1995)</xref> in a regression model with independent and identically distributed errors. The results of an extensive simulation study yielded an ambiguous situation. The theoretical results could not be confirmed in simulated data about the superiority of plug-in-based methods over cross validation in respect to faster convergence and less variable estimates. Furthermore, a pilot estimator has to be calculated in the plug-in approach using estimates of higher-order derivatives assuming its smoothness. The failure of the assumption could yield inefficient estimates (<xref ref-type="bibr" rid="bibr21-1076998611420439">Loader, 1999</xref>). The estimated bandwidths are smaller for the plug-in method applying a polynomial of first degree compared to GCV as reported in literature (<xref ref-type="bibr" rid="bibr21-1076998611420439">Loader, 1999</xref>) in our Monte Carlo simulation study. The plug-in bandwidths get larger for higher-order polynomials. GCV and the bootstrapping procedure produce more variable estimates than the plug-in method. It is reported in literature (<xref ref-type="bibr" rid="bibr21-1076998611420439">Loader, 1999</xref>) that the plug-in estimated bandwidths tend to oversmooth data with inflection points and curvature. This could be the reason for the superiority of the plug-in method applying a polynomial of third degree to the other bandwidth methods. <xref ref-type="bibr" rid="bibr31-1076998611420439">Qiu and Yandell (1998)</xref> already pointed out that the derivatives are hard to estimate and large bandwidths are essential to get acceptable estimates.</p>
<p>The three estimation algorithms are based on local polynomial approximation of the outcome variable <italic>Y</italic> for design points <italic>T</italic>. The local polynomial approximation has benefits over both the kernel smoothed approaches of Nadayara and Watson (<xref ref-type="bibr" rid="bibr27-1076998611420439">Nadaraya, 1964</xref>) and <xref ref-type="bibr" rid="bibr8-1076998611420439">Gasser and Mueller (1979)</xref>. (a) The boundary problem is addressed in local polynomial smoothing (<xref ref-type="bibr" rid="bibr17-1076998611420439">Hastie &amp; Loader, 1993</xref>). The boundary problem denotes the problem of a less accurate estimated smoothed mean at the boundaries of the observed time interval. (b) The estimated regression function <italic>m</italic> is improved when estimated in regions with few observations. (c) Local polynomial smoothing allows for an easy way to estimate the derivatives of the regression function <italic>m</italic>, which plays an important role in the estimation process of the change-point θ proposed in the nonparametric approach laid out in this article.</p>
<p>The large bandwidths represent a limitation of the framework modeling discussed here in the article. The first derivative is not estimated accurately at the boundaries of the considered time interval, although the boundary problem is addressed in local polynomial regression analyses (<xref ref-type="bibr" rid="bibr17-1076998611420439">Hastie &amp; Loader, 1993</xref>). The estimate of the lhl of the first derivative is based on only very few measurements at a time point at the beginning of the time interval, whereas the estimate of the rhl is based on the whole range of measurement points defined by the bandwidth. The fact results in artificial differences between the lhl and rhl of the first derivative at an observed time point close to the starting point and endpoint of the time interval.</p>
<p>Another limitation in real data analyses is the lack of statistical hypothesis testing, whether the observed discontinuity is statistically significant. <xref ref-type="bibr" rid="bibr31-1076998611420439">Qiu and Yandell (1998)</xref> calculated a threshold if the estimated change-point is meaningful, but the corresponding estimator for θ did not perform well in our simulation study. It is recommended to plot the function rhl minus lhl of the first derivative in the method of <xref ref-type="bibr" rid="bibr19-1076998611420439">Huh and Carriere (2002)</xref> in application to real data. The automatic estimation of the maximum of that function could yield false estimates because of the boundary problem discussed above.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>The authors would like to thank the editor and the anonymous reviewer for their helpful suggestions what improved the clarity of this article.</p>
</ack>
<ref-list>
<title>References</title>
<ref id="bibr1-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Barry</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>A Bayesian analysis for derivative change points</article-title>. <source>Communications in Statistics-Theory and Methods</source>, <volume>31</volume>, <fpage>1335</fpage>–<lpage>1348</lpage>.</citation>
</ref>
<ref id="bibr2-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Brown</surname>
<given-names>L. D.</given-names>
</name>
<name>
<surname>Low</surname>
<given-names>M. G.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Asymptotic equivalence of nonparametric regression and white noise</article-title>. <source>Annals of Statistics</source>, <volume>24</volume>, <fpage>2384</fpage>–<lpage>2398</lpage>.</citation>
</ref>
<ref id="bibr3-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Carpenter</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Bithell</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2000</year>). <article-title>Bootstrap confidence intervals: When, which, what? A practical guide for medical statisticians</article-title>. <source>Statistics in Medicine</source>, <volume>19</volume>, <fpage>1141</fpage>–<lpage>1164</lpage>.</citation>
</ref>
<ref id="bibr4-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cheng</surname>
<given-names>M. Y.</given-names>
</name>
<name>
<surname>Raimondo</surname>
<given-names>M.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Kernel methods for optimal change-points estimation in derivatives</article-title>. <source>Journal of Computational and Graphical Statistics</source>, <volume>17</volume>, <fpage>56</fpage>–<lpage>75</lpage>.</citation>
</ref>
<ref id="bibr5-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chu</surname>
<given-names>C. K.</given-names>
</name>
<name>
<surname>Marron</surname>
<given-names>J. S.</given-names>
</name>
</person-group> (<year>1991</year>). <article-title>Comparison of two bandwidth selectors with dependent errors</article-title>. <source>Annals of Statistics</source>, <volume>19</volume>, <fpage>1906</fpage>–<lpage>1918</lpage>.</citation>
</ref>
<ref id="bibr5a-1076998611420439">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Craske</surname>
<given-names>M. G.</given-names>
</name>
</person-group> (<year>1999</year>). <source>Anxiety disorders: Psychological approaches to theory and treatment</source>. <publisher-loc>Boulder, CO</publisher-loc>: <publisher-name>Westview Press</publisher-name>
</citation>
</ref>
<ref id="bibr6-1076998611420439">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Efron</surname>
<given-names>B.</given-names>
</name>
<name>
<surname>Tibshirani</surname>
<given-names>R. J.</given-names>
</name>
</person-group> (<year>1993</year>). . In <source>An introduction to the bootstrap</source> (pp. <fpage/>–<lpage/>). <publisher-loc>London, England</publisher-loc>: <publisher-name>Chapman and Hall</publisher-name>.</citation>
</ref>
<ref id="bibr7-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Faraway</surname>
<given-names>J. J.</given-names>
</name>
<name>
<surname>Jhun</surname>
<given-names>M. S.</given-names>
</name>
</person-group> (<year>1990</year>). <article-title>Bootstrap choice of bandwidth for density estimation</article-title>. <source>Journal of the American Statistical Association</source>, <volume>85</volume>, <fpage>1119</fpage>–<lpage>1122</lpage>.</citation>
</ref>
<ref id="bibr8-1076998611420439">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Gasser</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Mueller</surname>
<given-names>H. G.</given-names>
</name>
</person-group> (<year>1979</year>). <article-title>Kernel estimation of regression functions</article-title>. In <source>Smoothing Techniques for Curve Estimation: Proceedings of a Workshop Held in Heidelberg, April 2–4, 1979</source> (pp. <fpage>23</fpage>–<lpage>68</lpage>). <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr9-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gasser</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Mueller</surname>
<given-names>H. G.</given-names>
</name>
<name>
<surname>Mammitzsch</surname>
<given-names>V.</given-names>
</name>
</person-group> (<year>1985</year>). <article-title>Kernels for nonparametric curve estimation</article-title>. <source>Journal of the Royal Statistical Society Series B–Methodological</source>, <volume>47</volume>, <fpage>238</fpage>–<lpage>252</lpage>.</citation>
</ref>
<ref id="bibr10-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gijbels</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Goderniaux</surname>
<given-names>A. C.</given-names>
</name>
</person-group> (<year>2004a</year>). <article-title>Bandwidth selection for change-point estimation in nonparametric regression</article-title>. <source>Technometrics</source>, <volume>46</volume>, <fpage>76</fpage>–<lpage>86</lpage>.</citation>
</ref>
<ref id="bibr11-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gijbels</surname>
<given-names>I.</given-names>
</name>
<name>
<surname>Goderniaux</surname>
<given-names>A. C.</given-names>
</name>
</person-group> (<year>2004b</year>). <article-title>Data-driven discontinuity detection in derivatives of a regression function</article-title>. <source>Communications in Statistics-Theory and Methods</source>, <volume>33</volume>, <fpage>851</fpage>–<lpage>871</lpage>.</citation>
</ref>
<ref id="bibr12-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gloster</surname>
<given-names>A. T.</given-names>
</name>
<name>
<surname>Richard</surname>
<given-names>D. C. S.</given-names>
</name>
<name>
<surname>Himle</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Koch</surname>
<given-names>E.</given-names>
</name>
<name>
<surname>Anson</surname>
<given-names>H.</given-names>
</name>
<name>
<surname>Lokers</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Thornton</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Accuracy of retrospective memory and covariation estimation in patients with obsessive-compulsive disorder</article-title>. <source>Behaviour Research and Therapy</source>, <volume>46</volume>, <fpage>642</fpage>–<lpage>655</lpage>.</citation>
</ref>
<ref id="bibr13-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Golub</surname>
<given-names>G. H.</given-names>
</name>
<name>
<surname>Heath</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Wahba</surname>
<given-names>G.</given-names>
</name>
</person-group> (<year>1979</year>). <article-title>Generalized cross-validation as a method for choosing a good ridge parameter</article-title>. <source>Technometrics</source>, <volume>21</volume>, <fpage>215</fpage>–<lpage>223</lpage>.</citation>
</ref>
<ref id="bibr14-1076998611420439">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Haerdle</surname>
<given-names>W.</given-names>
</name>
</person-group> (<year>1990</year>). . In <source>Applied nonparametric regression</source> (pp. <fpage/>–<lpage/>). <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr15-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hall</surname>
<given-names>P.</given-names>
</name>
<name>
<surname>Lahiri</surname>
<given-names>S. N.</given-names>
</name>
<name>
<surname>Polzehl</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>On bandwidth choice in nonparametric regression with both short- and long-range dependent errors</article-title>. <source>Annals of Statistics</source>, <volume>23</volume>, <fpage>1921</fpage>–<lpage>1936</lpage>.</citation>
</ref>
<ref id="bibr16-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hartmann</surname>
<given-names>D. P.</given-names>
</name>
<name>
<surname>Gottman</surname>
<given-names>J. M.</given-names>
</name>
<name>
<surname>Jones</surname>
<given-names>R. R.</given-names>
</name>
<name>
<surname>Gardner</surname>
<given-names>W.</given-names>
</name>
<name>
<surname>Kazdin</surname>
<given-names>A. E.</given-names>
</name>
<name>
<surname>Vaught</surname>
<given-names>R. S.</given-names>
</name>
</person-group> (<year>1980</year>). <article-title>Interrupted time-series analysis and its application to behavioral-data</article-title>. <source>Journal of Applied Behavior Analysis</source>, <volume>13</volume>, <fpage>543</fpage>–<lpage>559</lpage>.</citation>
</ref>
<ref id="bibr17-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hastie</surname>
<given-names>T.</given-names>
</name>
<name>
<surname>Loader</surname>
<given-names>C.</given-names>
</name>
</person-group> (<year>1993</year>). <article-title>Local regression—Automatic kernel carpentry</article-title>. <source>Statistical Science</source>, <volume>8</volume>, <fpage>120</fpage>–<lpage>129</lpage>.</citation>
</ref>
<ref id="bibr18-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hidalgo</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Non-parametric estimation with strongly dependent multivariate time series</article-title>. <source>Journal of Time Series Analysis</source>, <volume>18</volume>, <fpage>95</fpage>–<lpage>122</lpage>.</citation>
</ref>
<ref id="bibr19-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Huh</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Carriere</surname>
<given-names>K. C.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Estimation of regression functions with a discontinuity in a derivative with local polynomial fits</article-title>. <source>Statistics &amp; Probability Letters</source>, <volume>56</volume>, <fpage>329</fpage>–<lpage>343</lpage>.</citation>
</ref>
<ref id="bibr20-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kashdan</surname>
<given-names>T. B.</given-names>
</name>
<name>
<surname>Steger</surname>
<given-names>M. F.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Expanding the topography of social anxiety—An experience-sampling assessment of positive emotions, positive events, and emotion suppression</article-title>. <source>Psychological Science</source>, <volume>17</volume>, <fpage>120</fpage>–<lpage>128</lpage>.</citation>
</ref>
<ref id="bibr21-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Loader</surname>
<given-names>C. R.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Bandwidth selection: Classical or plug-in?</article-title>. <source>Annals of Statistics</source>, <volume>27</volume>, <fpage>415</fpage>–<lpage>438</lpage>.</citation>
</ref>
<ref id="bibr22-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Morgan</surname>
<given-names>O. W.</given-names>
</name>
<name>
<surname>Griffiths</surname>
<given-names>C.</given-names>
</name>
<name>
<surname>Majeed</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Interrupted time-series analysis of regulations to reduce paracetamol (acetaminophen) poisoning</article-title>. <source>PLoS Medicine</source>, <volume>4</volume>, <fpage>654</fpage>–<lpage>659</lpage>.</citation>
</ref>
<ref id="bibr23-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mueller</surname>
<given-names>H.-G.</given-names>
</name>
<name>
<surname>Stadtmueller</surname>
<given-names>U.</given-names>
</name>
</person-group> (<year>1999</year>). <article-title>Discontinuous versus smooth regression</article-title>. <source>Annals of Statistics</source>, <volume>27</volume>, <fpage>299</fpage>–<lpage>337</lpage>.</citation>
</ref>
<ref id="bibr24-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Muggeo</surname>
<given-names>V. M. R.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Estimating regression models with unknown break-points</article-title>. <source>Statistics in Medicine</source>, <volume>22</volume>, <fpage>3055</fpage>–<lpage>3071</lpage>.</citation>
</ref>
<ref id="bibr25-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mueller</surname>
<given-names>H. G.</given-names>
</name>
</person-group> (<year>1992</year>). <article-title>Change-points in nonparametric regression-analysis</article-title>. <source>Annals of Statistics</source>, <volume>20</volume>, <fpage>737</fpage>–<lpage>761</lpage>.</citation>
</ref>
<ref id="bibr26-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mueller</surname>
<given-names>H. G.</given-names>
</name>
<name>
<surname>Song</surname>
<given-names>K. S.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Two-stage change-point estimators in smooth regression models</article-title>. <source>Statistics &amp; Probability Letters</source>, <volume>34</volume>, <fpage>323</fpage>–<lpage>335</lpage>.</citation>
</ref>
<ref id="bibr27-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nadaraya</surname>
<given-names>E. A.</given-names>
</name>
</person-group> (<year>1964</year>). <article-title>On estimating regression</article-title>. <source>Theory of Probability and Its Applications</source>, <volume>9</volume>, <fpage>141</fpage>–<lpage>142</lpage>.</citation>
</ref>
<ref id="bibr28-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nagel</surname>
<given-names>J. J.</given-names>
</name>
<name>
<surname>Himle</surname>
<given-names>D. P.</given-names>
</name>
<name>
<surname>Papsdorf</surname>
<given-names>J. D.</given-names>
</name>
</person-group> (<year>1981</year>). <article-title>Coping with performance anxiety</article-title>. <source>NATS Bulletin</source>, <volume>37</volume>, <fpage>26</fpage>–<lpage>33</lpage>
</citation>
</ref>
<ref id="bibr29-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Nagel</surname>
<given-names>J. J.</given-names>
</name>
<name>
<surname>Himle</surname>
<given-names>D. P.</given-names>
</name>
<name>
<surname>Papsdorf</surname>
<given-names>J. D.</given-names>
</name>
</person-group> (<year>1989</year>). <article-title>Cognitive-behavioral treatment of musical performance anxiety</article-title>. <source>Psychology of Music</source>, <volume>17</volume>, <fpage>12</fpage>–<lpage>21</lpage>.
</citation>
</ref>
<ref id="bibr30-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Opsomer</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>Y. D.</given-names>
</name>
<name>
<surname>Yang</surname>
<given-names>Y. H.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Nonparametric regression with correlated errors</article-title>. <source>Statistical Science</source>, <volume>16</volume>, <fpage>134</fpage>–<lpage>153</lpage>.</citation>
</ref>
<ref id="bibr31-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Qiu</surname>
<given-names>P. H.</given-names>
</name>
<name>
<surname>Yandell</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>A local polynomial jump-detection algorithm in nonparametric regression</article-title>. <source>Technometrics</source>, <volume>40</volume>, <fpage>141</fpage>–<lpage>152</lpage>.</citation>
</ref>
<ref id="bibr32-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ruppert</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>1997</year>). <article-title>Empirical-bias bandwidths for local polynomial nonparametric regression and density estimation</article-title>. <source>Journal of the American Statistical Association</source>, <volume>92</volume>, <fpage>1049</fpage>–<lpage>1062</lpage>.</citation>
</ref>
<ref id="bibr33-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ruppert</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Sheather</surname>
<given-names>S. J.</given-names>
</name>
<name>
<surname>Wand</surname>
<given-names>M. P.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>An effective bandwidth selector for local least squares regression</article-title>. <source>Journal of the American Statistical Association</source>, <volume>90</volume>, <fpage>1257</fpage>–<lpage>1270</lpage>.</citation>
</ref>
<ref id="bibr34-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Shiffman</surname>
<given-names>S.</given-names>
</name>
<name>
<surname>Stone</surname>
<given-names>A. A.</given-names>
</name>
<name>
<surname>Hufford</surname>
<given-names>M. R.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Ecological momentary assessment</article-title>. <source>Annual Review of Clinical Psychology</source>, <volume>4</volume>, <fpage>1</fpage>–<lpage>32</lpage>.</citation>
</ref>
<ref id="bibr35-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stone</surname>
<given-names>A. A.</given-names>
</name>
<name>
<surname>Schwartz</surname>
<given-names>J. E.</given-names>
</name>
<name>
<surname>Broderick</surname>
<given-names>J. E.</given-names>
</name>
<name>
<surname>Shiffman</surname>
<given-names>S. S.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Variability of momentary pain predicts recall of weekly pain: A consequence of the peak (or salience) memory heuristic</article-title>. <source>Personality and Social Psychology Bulletin</source>, <volume>31</volume>, <fpage>1340</fpage>–<lpage>1346</lpage>.</citation>
</ref>
<ref id="bibr36-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Stone</surname>
<given-names>A. A.</given-names>
</name>
<name>
<surname>Shiffman</surname>
<given-names>S.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Capturing momentary, self-report data: A proposal for reporting guidelines</article-title>. <source>Annals of Behavioral Medicine</source>, <volume>24</volume>, <fpage>236</fpage>–<lpage>243</lpage>.</citation>
</ref>
<ref id="bibr37-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Taylor</surname>
<given-names>C. C.</given-names>
</name>
</person-group> (<year>1989</year>). <article-title>Bootstrap choice of smoothing parameter in kernel density estimation</article-title>. <source>Biometrika</source>, <volume>76</volume>, <fpage>705</fpage>–<lpage>712</lpage>.</citation>
</ref>
<ref id="bibr38-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wagner</surname>
<given-names>A. K.</given-names>
</name>
<name>
<surname>Soumerai</surname>
<given-names>S. B.</given-names>
</name>
<name>
<surname>Zhang</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Ross-Degnan</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Segmented regression analysis of interrupted time series studies in medication use research</article-title>. <source>Journal of Clinical Pharmacy and Therapeutics</source>, <volume>27</volume>, <fpage>299</fpage>–<lpage>309</lpage>.</citation>
</ref>
<ref id="bibr39-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wang</surname>
<given-names>Y. Z.</given-names>
</name>
</person-group> (<year>1995</year>). <article-title>Jump and sharp cusp detection by wavelets</article-title>. <source>Biometrika</source>, <volume>82</volume>, <fpage>385</fpage>–<lpage>397</lpage>.</citation>
</ref>
<ref id="bibr40-1076998611420439">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yang</surname>
<given-names>Y. H.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Nonparametric regression with dependent errors</article-title>. <source>Bernoulli</source>, <volume>7</volume>, <fpage>633</fpage>–<lpage>655</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>