<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="review-article" dtd-version="2.3" xml:lang="EN">
   <front>
      <journal-meta>
         <journal-id journal-id-type="publisher-id">DIJ</journal-id>
         <journal-id journal-id-type="hwp">spdij</journal-id>
         <journal-title>Drug Information Journal</journal-title>
         <issn pub-type="ppub">0092-8615</issn>
         <publisher>
            <publisher-name>SAGE Publications</publisher-name>
            <publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
         </publisher>
      </journal-meta>
      <article-meta>
         <article-id pub-id-type="doi">10.1177/0092861512442057</article-id>
         <article-id pub-id-type="publisher-id">10.1177_0092861512442057</article-id>
         <article-categories>
            <subj-group subj-group-type="heading">
               <subject>Statistics</subject>
            </subj-group>
         </article-categories>
         <title-group>
            <article-title>Source Data Verification by Statistical Sampling: Issues in Implementation</article-title>
         </title-group>
         <contrib-group>
            <contrib contrib-type="author" corresp="yes" xlink:type="simple">
               <name name-style="western">
                  <surname>Grieve</surname>
                  <given-names>Andrew P.</given-names>
               </name>
               <degrees>BSc, MSc, PhD, DSc</degrees>
               <xref ref-type="aff" rid="aff1-0092861512442057">1</xref>
               <xref ref-type="corresp" rid="corresp1-0092861512442057"/>
            </contrib>
            <aff id="aff1-0092861512442057">Innovation Centre, Aptiv Solutions, Cologne, Germany  This work was conducted while the author was a member of the Statistical Research and Consulting Centre, Pfizer Global R&amp;D, Sandwich, UK</aff>
         </contrib-group>
         <author-notes>
            <corresp id="corresp1-0092861512442057">Andrew P. Grieve, BSc, MSc, PhD, DSc, Innovation Centre, Aptiv Solutions, Robert-Perthel Strasse 77a, 50739 Köln, Germany Email: <email xlink:type="simple">Andy.grieve@AptivSolutions.com</email>
            </corresp>
         </author-notes>
         <pub-date pub-type="epub-ppub">
            <month>5</month>
            <year>2012</year>
         </pub-date>
         <volume>46</volume>
         <issue>3</issue>
         <fpage>368</fpage>
         <lpage>377</lpage>
         <history>
            <date date-type="received">
               <day>28</day>
               <month>6</month>
               <year>2011</year>
            </date>
            <date date-type="accepted">
               <day>21</day>
               <month>2</month>
               <year>2012</year>
            </date>
         </history>
         <permissions>
            <copyright-statement>© The Author(s) 2012</copyright-statement>
            <copyright-year>2012</copyright-year>
            <copyright-holder content-type="society">Drug Information Association</copyright-holder>
         </permissions>
         <abstract>
            <p>Efficiency of the drug development process is a continuing concern for pharmaceutical companies, governments, regulatory authorities, and patients.  While much time and effort have been spent on developments in genetics and on sophisticated statistical designs, there has been less concern about the processes that govern the running of clinical trials.  In this article, I describe a statistical method for source data verification whose implementation can have a large impact on the workload of trial monitors. I investigate the consequences of a less stringent form of source data verification on the quality of data and the inferences that can be drawn from the data.</p>
         </abstract>
         <kwd-group>
            <kwd>source data verification</kwd>
            <kwd>statistical sampling</kwd>
            <kwd>data quality</kwd>
            <kwd>cost savings</kwd>
            <kwd>efficiency</kwd>
         </kwd-group>
      </article-meta>
   </front>
   <body>
      <sec id="section1-0092861512442057">
         <title>1. Introduction</title>
         <p>The search for efficiency in drug development is important to pharmaceutical companies, governments, regulatory authorities, and patients. Efficiency is important for pharmaceutical companies because they are interested in increasing the return on investment and their share price, as can be read in almost any company annual report over the last 10 years. Efficiency is important to governments because they have an interest in the economic welfare of pharmaceutical companies and the health of their citizens, hence, the EU’s Innovative Medicines Initiative.<sup>
               <xref ref-type="bibr" rid="bibr1-0092861512442057">1</xref>
            </sup> Efficiency is important for regulatory authorities because they recognize that the regulatory drug development process is becoming “increasingly challenging, inefficient, and costly.”<sup>
               <xref ref-type="bibr" rid="bibr2-0092861512442057">2</xref>
            </sup> Finally, efficiency is important for patients who expect new medicines for which they are not necessarily prepared to pay vastly increased prices.</p>
         <p>This search for efficiency has been conducted in different ways.  Companies have looked to re-engineer the drug development process and have recognized the difference between the “learning” and “confirming” stages identified by Sheiner.<sup>
               <xref ref-type="bibr" rid="bibr3-0092861512442057">3</xref>
            </sup> Statisticians have argued for an increasing use of innovative, in particular adaptive, designs, and these ideas have been taken up by companies<sup>
               <xref ref-type="bibr" rid="bibr4-0092861512442057">4</xref>
            </sup> and have been recognized by governments<sup>
               <xref ref-type="bibr" rid="bibr1-0092861512442057">1</xref>
            </sup> and regulatory authorities.<sup>
               <xref ref-type="bibr" rid="bibr2-0092861512442057">2</xref>
            </sup> Basic science has a role to play, and there has been increasing use of biomarkers to expedite early-phase (“learning”) drug development in combination with innovative designs.<sup>
               <xref ref-type="bibr" rid="bibr5-0092861512442057">5</xref>
            </sup>
         </p>
         <p>All of these have a role to play and will contribute to increased efficiency, but by how much?  Beltangady and Brown report that Pfizer’s Enhanced Clinical Trial Design initiative was aimed at reducing direct grant costs by US$100 million per year.<sup>
               <xref ref-type="bibr" rid="bibr4-0092861512442057">4</xref>
            </sup>  This is a large amount and is clearly worth saving, but there are simpler ways that can achieve at least as much.</p>
         <p>The results of a survey of members of LIF, the Swedish trade association for the pharmaceutical industry concerning ICH Good Clinical Practice (GCP) activities and their associated costs were reported by Funning et al.<sup>
               <xref ref-type="bibr" rid="bibr6-0092861512442057">6</xref>
            </sup> Of interest to us are the results relating to source data verification (SDV) of phase III clinical trials.  Source data verification is the verification of the accuracy of clinical trial data recorded in clinical record forms (CRFs) compared to their source. These sources may include, but need not be restricted to, physician notes, nurse notes, original laboratory reports, original reports from medical machines, and others. In the Swedish survey, it was reported that 50% of all phase III trial costs arise from ICH GCP activities, and of these 50%, in other words 25% of the total, costs relate to SDV,<sup>
               <xref ref-type="bibr" rid="bibr6-0092861512442057">6</xref>
            </sup> and from figures supplied by LIF, it was estimated that SDV costs, on average, US$90 million per phase III program. Part of the issue is that many companies continue to employ 100% SDV. That is, they verify all data against its source. As might be imagined, this is extremely time consuming; however, it may not be as successful in maintaining the quality of the data as sponsors would think.  It has been reported that as many as 15% of errors remain in databases following human review.<sup>
               <xref ref-type="bibr" rid="bibr7-0092861512442057">7</xref>
            </sup>
         </p>
         <p>Many authors and groups have recently suggested that one solution to the problem is risk-based SDV.<sup>
               <xref ref-type="bibr" rid="bibr8-0092861512442057">8</xref>
               <xref ref-type="bibr" rid="bibr9-0092861512442057"/>
               <xref ref-type="bibr" rid="bibr10-0092861512442057"/>-<xref ref-type="bibr" rid="bibr11-0092861512442057">11</xref>
            </sup> In this article, I concentrate on reducing the amount of SDV by an appropriate use of statistical sampling.  In section 2, I describe a scheme for SDV by statistical sampling, developed for Pfizer Global R&amp;D, based on a well-established acceptance sampling scheme. Such schemes require the specification of an acceptable quality level (AQL), which determines the size of the sample to be taken from a batch of objects to be checked. There are 3 distinct statistical considerations that can be used to determine an appropriate level of AQL for such an acceptance sampling scheme. In section 3, I describe how to learn about the typical quality levels being achieved in a sponsor’s studies across development programs in a baseline assessment. In such an assessment, it is important to ensure that there is a good mix of trials because monitors tend to report that error rates vary considerably by trial design, by therapeutic area, and by the phase of drug development. In section 4, I look at the operating characteristics of the proposed acceptance sampling scheme.  In section 5, I investigate the consequences of errors remaining in the database.  In section 6, I illustrate how these 3 considerations can be brought together to make an appropriate choice of the AQL. Finally, in section 7, I discuss implications of the use of such a scheme.</p>
      </sec>
      <sec id="section2-0092861512442057">
         <title>2. A Scheme for SDV by Statistical Sampling</title>
         <p>Notably, SDV by statistical sampling (SDVSS) is a form of SDV whereby not all, but an appropriately chosen sample, of the available data is verified against the source. The sampling scheme investigated by Pfizer uses a tested and established statistical method: acceptance sampling.<sup>
               <xref ref-type="bibr" rid="bibr12-0092861512442057">12</xref>,<xref ref-type="bibr" rid="bibr13-0092861512442057">13</xref>
            </sup> Separate entries in the CRF are defined as data points for the sampling process.</p>
         <p>The standard approach to acceptance sampling in a production context has 2 major components. Firstly, based on the total number of items in a batch, and a predetermined AQL, the required size of a random sample size and the acceptance number are determined. Secondly, the random sample of items is selected from the batch, and if the number of defects is less than the acceptance number, the whole batch is accepted, or else it is rejected.</p>
         <p>In the context of its application to SDV, there are some modifications to the standard acceptance sampling approach that are necessary. Firstly, in SDV, the batch size is not fixed because the number of data points from which a sample is to be taken is not completely controllable by the monitor. A consequence is that an SDV control system needs to be developed to allow the monitor to determine the appropriate sample size for the data that are available to be monitored. Secondly, in SDV, entire patient visits are generally selected until the required number of data points is achieved or minimally exceeded. This approach is taken for practical reasons because if pure random sampling were allowed, then the savings in effort would be reduced as there is considerable overhead in sampling an individual patient record.</p>
         <p>For a given site, the control system will select a sample of patient visits from among all patient visits that took place between the previous monitoring visit and the next, thereby providing instructions to the monitor about those visits that have to be verified.</p>
         <p>The control system will require access to the following information:</p>
         <list list-type="alpha-lower">
            <list-item>
               <p>the number of data points in each patient visit for this protocol, which is determinable at study setup; </p>
            </list-item>
            <list-item>
               <p>the number and identity of patient visits that took place at this site since last monitoring visit, which should be readily available from a clinical trials management system; and </p>
            </list-item>
            <list-item>
               <p>the AQL that is determined up front and whose choice is an important business decision.</p>
            </list-item>
         </list>
         <p>The control system will then provide</p>
         <list list-type="alpha-lower">
            <list-item>
               <p>the number of data points that need to be verified<sup>
                     <xref ref-type="bibr" rid="bibr12-0092861512442057">12</xref>,<xref ref-type="bibr" rid="bibr13-0092861512442057">13</xref>
                  </sup>; </p>
            </list-item>
            <list-item>
               <p>using this number, a random sample of the available patient visits that need to be verified; </p>
            </list-item>
            <list-item>
               <p>the possibility of selecting an alternative patient visit in the case that a particular patient’s source documents are not available for verification at a monitoring visit; and </p>
            </list-item>
            <list-item>
               <p>the number of errors that is acceptable (the error acceptance/rejection number). This number is corrected for the actual sample size rather than the target sample size, which can occur if the number of data points in the selected sample of patient visits exceeds the required sample size. It may also need to be changed if a patient visit needs to be replaced or a new sample taken.</p>
            </list-item>
         </list>
         <p>Such an acceptance sampling scheme is dependent upon the choice of the AQL. In the following sections, I investigate how statistical considerations can be used to drive the choice of the AQL.</p>
      </sec>
      <sec id="section3-0092861512442057">
         <title>3. Baseline Assessment of Quality</title>
         <p>One approach is to ask all monitors who are due one or more monitoring visits during a specified week to collect data about the SDV carried out during those visits. Monitors should not be asked to make extra site visits or to reschedule visits, nor should they be expected to undertake extra work during each visit, apart from the completion of a simple record.  The baseline assessment should reflect the real-world practice of monitors.</p>
         <p>At each monitoring visit, the following types of data can be collected:</p>
         <list list-type="bullet">
            <list-item>
               <p>protocol identifier,</p>
            </list-item>
            <list-item>
               <p>date of monitoring visit,</p>
            </list-item>
            <list-item>
               <p>type of visit and number of errors found for each patient visit on which SDV was performed, and</p>
            </list-item>
            <list-item>
               <p>time spent at site and time spent on SDV.</p>
            </list-item>
         </list>
         <p>Information on site identity need not be collected as the purpose of this data collection is not so that it can be used to make an assessment of individual site performance. This process is illustrated using a baseline assessment conducted by Pfizer before implementing this SDVSS scheme.</p>
         <p>Data were returned from 1234 individual patient visits from 24 studies. The number of data points in a patient visit ranged from 2 to 333, and the number of errors ranged from 0 to 41.  In <xref ref-type="fig" rid="fig1-0092861512442057">Figure 1</xref>, the data are presented as the empirical error rates by monitoring visit in each project, in other words, pooled over the individual patient visits per monitoring visit. There is clearly considerable variability between the error rates in the individual monitoring visits. However, the majority of error rates are between approximately 1% and 10%.</p>
         <fig id="fig1-0092861512442057" position="float">
            <label>Figure 1.</label>
            <caption>
               <p>Error rates by monitoring visit in each project.</p>
            </caption>
            <graphic alt-version="no" alternate-form-of="fig1-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-fig1.tif" xlink:type="simple"/>
         </fig>
         <p>The data in <xref ref-type="fig" rid="fig1-0092861512442057">Figure 1</xref> have been used to estimate the error rates in individual projects, as well as an overall estimate of the error rate, pooled across all projects. A Bayesian model was used, taking account of both within-project as well as between-project variability and was fitted using WinBUGS (Medical Research Council Biostatistics Unit, Cambridge, UK). The estimates that were obtained are presented graphically in <xref ref-type="fig" rid="fig2-0092861512442057">Figure 2</xref> in the form of the posterior mean for project rates and associated 95% credible intervals and the pooled rate with its 95% credible interval.</p>
         <fig id="fig2-0092861512442057" position="float">
            <label>Figure 2.</label>
            <caption>
               <p>Estimated error rates by project.</p>
            </caption>
            <graphic alt-version="no" alternate-form-of="fig2-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-fig2.tif" xlink:type="simple"/>
         </fig>
         <p>Overall, the majority of the estimated rates for individual projects lie between 0.01 and 0.04, although there are some notable exceptions with very high error rates and correspondingly wide credible intervals.  These “outlying” projects are associated with small numbers of data points, particularly with projects for which less than 1000 data points were available to be verified. The pooled estimate of the error rate across all projects was approximately 3% with an associated 95% credible interval ranging from 2% to 4%.</p>
      </sec>
      <sec id="section4-0092861512442057">
         <title>4. Operating Characteristics</title>
         <p>I noted previously that the implementation of an acceptance sampling approach to SDV is nonstandard, and because of this, the published tables and graphs of the operating characteristics of such schemes are not useful as they assume a fixed batch size. Accordingly, it is necessary to simulate the SDV process to determine its operating characteristics.</p>
         <p>In what follows, I describe the simulation of a typical randomized controlled trial. The trial was a randomized discontinuation design and is shown schematically in <xref ref-type="fig" rid="fig3-0092861512442057">Figure 3</xref>. Of importance here are the numbers of data points to be checked per visit. These are shown at the bottom of the figure, and we can see that for this study, they range from 9 in the nontreatment follow-up phase to 87 for early termination. </p>
         <fig id="fig3-0092861512442057" position="float">
            <label>Figure 3.</label>
            <caption>
               <p>Schematic representation of a randomized discontinuation study.</p>
            </caption>
            <graphic alt-version="no" alternate-form-of="fig3-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-fig3.tif" xlink:type="simple"/>
         </fig>
         <p>The simulation has the following settings:</p>
         <list list-type="simple">
            <list-item>
               <p>
                  <bold>Number of simulations:</bold> 500</p>
            </list-item>
            <list-item>
               <p>
                  <bold>Number of patients:</bold> 100</p>
            </list-item>
            <list-item>
               <p>
                  <bold>Number of monitoring visits:</bold> 30 at monthly intervals</p>
            </list-item>
            <list-item>
               <p>
                  <bold>AQL:</bold> 0.0065, 0.01, 0.025</p>
            </list-item>
            <list-item>
               <p>
                  <bold>True error rates:</bold> 0.01 (0.01) 0.10</p>
            </list-item>
            <list-item>
               <p>
                  <bold>Interarrival rate:</bold> 1.75 days, corresponding to 4 patients per week entering the study</p>
            </list-item>
            <list-item>
               <p>
                  <bold>Correction rate:</bold> 95%, indicating the rate at which centers are able to detect and correct errors when monitoring visits fail testing. This may be too high as some authors report only an 85% success rate from 100% checking.</p>
            </list-item>
         </list>
         <p>The results of the simulations are monitored in terms of the following:</p>
         <list list-type="simple">
            <list-item>
               <p>
                  <bold>
                     <italic>Proportionate saving in effort from using statistical SDV:</italic>
                  </bold> This looks at the number of data points checked as a percentage of the total number of data points that would have been checked if 100% SDV were in operation.</p>
            </list-item>
            <list-item>
               <p>
                  <bold>
                     <italic>Acceptance rates:</italic>
                  </bold> This is the proportion of monitoring visits that would have passed inspection. This measure is plotted against the true error rate.</p>
            </list-item>
            <list-item>
               <p>
                  <bold>
                     <italic>Postmonitoring quality level:</italic>
                  </bold> This measure looks at the data quality postmonitoring and is also plotted as a function of the true error rate.</p>
            </list-item>
         </list>
         <p>These 3 metrics define the “operating characteristics” of this scheme. Formally, the acceptance rates themselves constitute the traditional operating characteristics, but the other metrics are important to persuade the management of sponsor companies that resources can be saved, and at the same time, data quality is maintained at an appropriate level. The distribution of the proportionate savings in effort to be achieved by using statistical SDV for each value of AQL determined from simulations, pooled across the true error rates, is shown in <xref ref-type="fig" rid="fig4-0092861512442057">Figure 4</xref>. </p>
         <fig id="fig4-0092861512442057" position="float">
            <label>Figure 4.</label>
            <caption>
               <p>Proportionate saving in effort by using source data verification by statistical sampling.</p>
            </caption>
            <graphic alt-version="no" alternate-form-of="fig4-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-fig4.tif" xlink:type="simple"/>
         </fig>
         <p>For the 3 AQL values, the proportionate saving in effort is, on average, approximately 95%, so in effect, monitors need to verify only 5% of the available data points. This percentage is governed by the size of the batch of data points that are available to be verified, independent of the AQL that is a feature of BS6001. This percentage can be affected by the interval between monitoring visits. If monitors reduce the interval between visits, the total amount of data from which the sample is taken will be reduced, and the proportionate saving will also be reduced. </p>
         <p>
            <xref ref-type="fig" rid="fig5-0092861512442057">Figure 5</xref> shows the proportion of monitoring visits that will pass the test for each of the values of AQL and for each true error rate for which simulations have been carried out. Here, we see that for the lowest AQL (0.0065), pass rates drop sharply from 80% at a true error rate of 0.01 to 40% at 0.02 to less than 20% at 0.03. In the case of the middle AQL (0.01), there is a similar pattern, although the pass rates are uniformly higher than for 0.0065 and are now approximately 90%, 70%, and 40% for true error rates of 0.01, 0.02, and 0.03, respectively. Finally, in contrast to the previous values, for the highest AQL (0.025), the pass rate does not drop rapidly until the true error rate is 0.04 or greater.</p>
         <fig id="fig5-0092861512442057" position="float">
            <label>Figure 5.</label>
            <caption>
               <p>Monitoring pass rates for each true error rate.</p>
            </caption>
            <graphic alt-version="no" alternate-form-of="fig5-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-fig5.tif" xlink:type="simple"/>
         </fig>
         <p>
            <xref ref-type="fig" rid="fig6-0092861512442057">Figure 6</xref> presents the postmonitoring quality levels achieved for each value of the AQL. From these results, we see that the postmonitoring error rates vary as a function of the true error rate, which is a consequence of the monitoring pass rates also varying as a function of the true error rates. For the lowest AQL (0.0065), the postmonitoring error rates for true rates of 0.01 and 0.02 are comparable at about 0.007 to 0.008, and these are the highest levels. For the medium AQL (0.01), the worst level of postmonitoring error rate occurs when the true error rate is 0.02, or twice the value of the AQL; this is consistent with the theory of acceptance sampling schemes. At this value of the true error rate, the postmonitoring quality is approximately 0.015 on average, that is, 50% higher than the AQL itself. For the largest AQL (0.025), the postmonitoring error rate again peaks at twice the AQL, although in this case, the postmonitoring error rate is also high for true error rates between the AQL and twice the AQL. In absolute terms, the peak postmonitoring error rate is just under 0.03.</p>
         <fig id="fig6-0092861512442057" position="float">
            <label>Figure 6.</label>
            <caption>
               <p>Postmonitoring error rates.</p>
            </caption>
            <graphic alt-version="no" alternate-form-of="fig6-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-fig6.tif" xlink:type="simple"/>
         </fig>
      </sec>
      <sec id="section5-0092861512442057">
         <title>5. The Implication of Errors (Misclassification) for the Sensitivity of Studies to Detect Differences in Responder Rates</title>
         <p>One of the assumptions that we make when considering using a statistical approach to SDV is that the error rates that are knowingly left in the CRFs will be controlled at a sufficiently low level to have a minimal impact on business decisions. The scheme I propose controls errors in all variables at the same rate; in particular, the primary variable is treated like any other. An implication is that any errors left in the data have a small chance of being associated with the primary variable and therefore may have some effect on the conclusions of the study.</p>
         <p>In general, it is difficult to quantify how great will be the impact of a given level of errors being left in the data because if the primary end point were reduction in diastolic blood pressure (DBP) (in mm Hg), the impact of an error will depend on the magnitude of the error. For example, suppose that in one patient, instead of recording the reduction in DBP as 10 mmHg it is recorded as 15 mmHg, and that there are 100 patients, then the impact on the mean reduction is 0.05 mmHg with a corresponding small impact on the standard deviation, and such a small effect is unlikely to be significant. However, if instead of 10 mmHg, a value of 30 mmHg were to be recorded, the impact on mean reduction would be 0.2 mmHg, and this might have an impact.  Of course, very large deviations should be picked up by data management systems and raised as a data query or by the statistician and queried as an outlier.</p>
         <p>It is instructive to look at errors in a different way.  Let us suppose that the primary end point is based on responder rates—in other words, it is binary—and that the population success rates are π<sub>t</sub> in the treatment group and π<sub>c</sub> in the control group. One common measure of treatment effect is the absolute rate reduction δ = π<sub>t</sub> – π<sub>c</sub>. With these assumptions, the standard calculation to determine the sample size (per group) to detect δ with a type I error α and power 1 – β is given by</p>
         <p>
            <disp-formula id="disp-formula1-0092861512442057">
               <mml:math id="mml-disp1-0092861512442057" overflow="scroll">
                  <mml:mi>n</mml:mi>
                  <mml:mo stretchy="false">=</mml:mo>
                  <mml:mrow>
                     <mml:mfrac>
                        <mml:mrow>
                           <mml:mi mathvariant="normal">v</mml:mi>
                           <mml:mi mathvariant="normal">a</mml:mi>
                           <mml:mi mathvariant="normal">r</mml:mi>
                           <mml:mo mathvariant="normal" stretchy="false">(</mml:mo>
                           <mml:mo>δ</mml:mo>
                           <mml:mo mathvariant="normal" stretchy="false">)</mml:mo>
                           <mml:mo mathvariant="normal" stretchy="false">(</mml:mo>
                           <mml:msub>
                              <mml:mi mathvariant="normal">z</mml:mi>
                              <mml:mrow>
                                 <mml:mo>α</mml:mo>
                                 <mml:mrow>
                                    <mml:mo>/</mml:mo>
                                 </mml:mrow>
                                 <mml:mn mathvariant="normal">2</mml:mn>
                              </mml:mrow>
                           </mml:msub>
                           <mml:mo mathvariant="normal" stretchy="false">+</mml:mo>
                           <mml:msub>
                              <mml:mi mathvariant="normal">z</mml:mi>
                              <mml:mo>β</mml:mo>
                           </mml:msub>
                           <mml:mrow>
                              <mml:msup>
                                 <mml:mo mathvariant="normal" stretchy="false">)</mml:mo>
                                 <mml:mn mathvariant="normal">2</mml:mn>
                              </mml:msup>
                           </mml:mrow>
                        </mml:mrow>
                        <mml:msup>
                           <mml:mo>δ</mml:mo>
                           <mml:mn mathvariant="normal">2</mml:mn>
                        </mml:msup>
                     </mml:mfrac>
                  </mml:mrow>
               </mml:math>
               <graphic alt-version="no" alternate-form-of="disp-formula1-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-eq1.tif" xlink:type="simple"/>
            </disp-formula>
         </p>
         <p>where var(δ) = π<sub>t</sub>(1 – π<sub>t</sub>) + π<sub>c</sub>(1 – π<sub>c</sub>),  and <italic>z</italic>
            <sub>γ</sub> is the γ% point of the standard normal distribution.</p>
         <p>Now suppose that, because of SDVSS, an error rate of φ% is allowed in the data. Because of the errors, the responder rates are now
<disp-formula id="disp-formula2-0092861512442057">
               <mml:math id="mml-disp2-0092861512442057" overflow="scroll">
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo stretchy="false">−</mml:mo>
                  <mml:mo>φ</mml:mo>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:msub>
                     <mml:mi mathvariant="italic">π</mml:mi>
                     <mml:mrow>
                        <mml:mi mathvariant="normal">t</mml:mi>
                     </mml:mrow>
                  </mml:msub>
                  <mml:mo stretchy="false">+</mml:mo>
                  <mml:mo>φ</mml:mo>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo stretchy="false">−</mml:mo>
                  <mml:msub>
                     <mml:mi mathvariant="italic">π</mml:mi>
                     <mml:mrow>
                        <mml:mi mathvariant="normal">t</mml:mi>
                     </mml:mrow>
                  </mml:msub>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mtext> </mml:mtext>
                  <mml:mrow>
                     <mml:mi mathvariant="normal">a</mml:mi>
                     <mml:mi mathvariant="normal">n</mml:mi>
                     <mml:mi mathvariant="normal">d</mml:mi>
                     <mml:mtext mathvariant="normal"> </mml:mtext>
                  </mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo stretchy="false">−</mml:mo>
                  <mml:mo>φ</mml:mo>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:msub>
                     <mml:mi mathvariant="italic">π</mml:mi>
                     <mml:mrow>
                        <mml:mi mathvariant="normal">c</mml:mi>
                     </mml:mrow>
                  </mml:msub>
                  <mml:mo stretchy="false">+</mml:mo>
                  <mml:mo>φ</mml:mo>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo stretchy="false">−</mml:mo>
                  <mml:msub>
                     <mml:mi mathvariant="italic">π</mml:mi>
                     <mml:mrow>
                        <mml:mi mathvariant="normal">c</mml:mi>
                     </mml:mrow>
                  </mml:msub>
                  <mml:mo stretchy="false">)</mml:mo>
               </mml:math>
               <graphic alt-version="no" alternate-form-of="disp-formula2-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-eq2.tif" xlink:type="simple"/>
            </disp-formula> in the treated and control groups, respectively, leading to a reduced treatment difference of δ(1 – 2φ).  Correspondingly, the variance of the treatment difference becomes<disp-formula id="disp-formula3-0092861512442057">
               <mml:math id="mml-disp3-0092861512442057" overflow="scroll">
                  <mml:mrow>
                     <mml:mi mathvariant="normal">v</mml:mi>
                     <mml:mi mathvariant="normal">a</mml:mi>
                     <mml:mi mathvariant="normal">r</mml:mi>
                  </mml:mrow>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mi mathvariant="italic">δ</mml:mi>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo stretchy="false">+</mml:mo>
                  <mml:mo>φ</mml:mo>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo stretchy="false">−</mml:mo>
                  <mml:mo>φ</mml:mo>
                  <mml:mo stretchy="false">)</mml:mo>
                  <mml:mo stretchy="false">[</mml:mo>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo stretchy="false">−</mml:mo>
                  <mml:mn>2</mml:mn>
                  <mml:msub>
                     <mml:mi mathvariant="italic">π</mml:mi>
                     <mml:mrow>
                        <mml:mi mathvariant="normal">t</mml:mi>
                     </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                     <mml:msup>
                        <mml:mo stretchy="false">)</mml:mo>
                        <mml:mn>2</mml:mn>
                     </mml:msup>
                  </mml:mrow>
                  <mml:mo stretchy="false">+</mml:mo>
                  <mml:mo stretchy="false">(</mml:mo>
                  <mml:mn>1</mml:mn>
                  <mml:mo stretchy="false">−</mml:mo>
                  <mml:mn>2</mml:mn>
                  <mml:msub>
                     <mml:mi mathvariant="italic">π</mml:mi>
                     <mml:mrow>
                        <mml:mi mathvariant="normal">c</mml:mi>
                     </mml:mrow>
                  </mml:msub>
                  <mml:mrow>
                     <mml:msup>
                        <mml:mo stretchy="false">)</mml:mo>
                        <mml:mn>2</mml:mn>
                     </mml:msup>
                  </mml:mrow>
                  <mml:mo stretchy="false">]</mml:mo>
               </mml:math>
               <graphic alt-version="no" alternate-form-of="disp-formula3-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-eq3.tif" xlink:type="simple"/>
            </disp-formula> which is increased because the second term is positive.<sup>16</sup>
         </p>
         <p>When misclassification rates were first studied by Bross,<sup>
               <xref ref-type="bibr" rid="bibr14-0092861512442057">14</xref>
            </sup> he assumed that the error rates for successes judged to be failures were different from the error rates for failures judged to be successes. If this latter rate is θ%, then the treatment difference is δ(1 – θ – φ). It has also been suggested that the error rates may differ between the samples.<sup>
               <xref ref-type="bibr" rid="bibr15-0092861512442057">15</xref>
            </sup> This latter consideration is not relevant here because under randomization and blindness, there are no grounds to suppose differential error rates between the treated and control groups, which is not necessarily the case in epidemiological studies in the context of interest.<sup>
               <xref ref-type="bibr" rid="bibr15-0092861512442057">15</xref>
            </sup> While separate φ and θ error rates were considered, it seems unlikely that the 2 error rates would differ significantly, and therefore, this added complexity has not been addressed.</p>
         <p>The conjunction of a reduction in the size of difference to be detected and an increase in its variance implies that the power of the study will be reduced from its planned level. Or, put another way, to achieve the same power, the sample size of the study must be increased.  Here are 2 examples:
<disp-quote>
               <p>
                  <bold>
                     <italic>Example 1.</italic>
                  </bold>
               </p>
               <p>Suppose π<sub>t</sub> = 0.4, π<sub>c</sub> = 0.3, δ = 0.1, α = 0.05, 1 – β = 0.80, φ = 0.01, from which we can determine var(δ) = 0.450 and n = 354 per treatment group. With 1% errors, the true difference is 0.098, the variance is 0.452, and the power for a sample size of 354 is 0.783, a value that is marginally smaller than the nominal value. Alternatively, a sample size of 370 per group would be required to achieve the desired power of 0.8, an increase of 16 per group.</p>
               <p>
                  <bold>
                     <italic>Example 2.</italic>
                  </bold>
               </p>
               <p>With the same assumptions as in example 1 but now with 10% errors, the true difference is 0.080, the variance is 0.468, and the power for a sample size of 354 is 0.595. This is considerably lower than the nominal value. The required sample size to achieve the nominal value is now 575, an increase of 221 per group or a 62% increase in sample size.</p>
            </disp-quote>
            <xref ref-type="fig" rid="fig7-0092861512442057">Figure 7</xref> shows the impact of different error rates in the data on the required sample size for a range of π<sub>c</sub> values. All calculations have been carried out for a type I error of α = 0.05, δ = 0.1, and power of 0.8, which tends to be an industry standard, at least for phase II studies. The curves are essentially parallel, indicating that the increase in sample size per group is independent of the control rate so that this can be ignored.  As an approximation, each increase of 1% in the misclassification rate corresponds to an increased sample size of an additional 20 patients per group so that if the misclassification rate were 5%, this would lead to an increased sample size of approximately 100 per group to achieve the required power.</p>
         <fig id="fig7-0092861512442057" position="float">
            <label>Figure 7.</label>
            <caption>
               <p>Sample size as a function of control rate effect for various misclassification rates.</p>
            </caption>
            <graphic alt-version="no" alternate-form-of="fig7-0092861512442057" position="float" xlink:href="10.1177_0092861512442057-fig7.tif" xlink:type="simple"/>
         </fig>
      </sec>
      <sec id="section6-0092861512442057">
         <title>6. Choosing an Appropriate Value for the AQL</title>
         <p>The baseline assessment of 24 studies monitored during a single week shows we could anticipate error rates between 2% and 4%. With these rates in mind, the simulation of the process shows that an AQL of 0.025 would result in a significant percentage of monitoring visits with high error rates being passed, leading to high error rates of the order of 3% remaining in the database. On the other hand, an AQL of 0.0065 would lead to a significant proportion of monitoring visits being failed; the failure rates are 60%, 85%, and 95% at true error rates of 2%, 3%, and 4%, respectively. While this is a true reflection of the consequences of high error rates, the implications for individual centers might be extreme, particularly in the early stages of implementing the scheme. Were such a scheme to be piloted, failure rates in the pilot of the order of 60% to 95% could have serious consequences for the centers, and their responses may bias the sponsor’s perspective on the likely acceptability of the statistical sampling approach.</p>
         <p>From an analysis of the consequence of misclassification rates, a 3% error rate in the database would result in a 12% to 15% increase in the sample size to maintain a power of 80%. On the other hand, an AQL of 0.01 has a uniformly better acceptance/failure profile than an AQL of 0.0065 and results in errors remaining in the database at half the level of an AQL of 0.025. An error rate of 1.5% in the database would require a 5% to 7% increase in the sample size to maintain a power of 80%. Based on these considerations, a reasonable choice of AQL is 1%.</p>
      </sec>
      <sec id="section7-0092861512442057">
         <title>7. Discussion</title>
         <p>The ICH E6 describes the responsibility and extent of monitoring of clinical trials by sponsors.<sup>
               <xref ref-type="bibr" rid="bibr16-0092861512442057">16</xref>
            </sup> We are told that “the sponsor should ensure that the trials are adequately monitored” and that “the sponsor should determine the appropriate extent and nature of monitoring.”<sup>
               <xref ref-type="bibr" rid="bibr16-0092861512442057">16</xref>
            </sup> It is therefore clear that the responsibility for choosing the monitoring remains with the sponsor. The guideline does not require 100% SDV, and furthermore, ICH E6 foresees the possibility of SDVSS: “Statistically controlled sampling may be an acceptable method for selecting the data to be verified.”<sup>
               <xref ref-type="bibr" rid="bibr16-0092861512442057">16</xref>
            </sup> The scheme outlined in this article is therefore not immediately unacceptable in terms of GCP requirements.</p>
         <p>The saving in verification volume is noteworthy and will clearly contribute to overall cost savings.  However, in many ways, there is a more important consequence of saving that proportion of monitors’ time that is spent on SDV. The time that is freed up can be used by monitors to “focus their attention on critical data elements that are vital for the analysis of the study data rather than box checking”<sup>
               <xref ref-type="bibr" rid="bibr11-0092861512442057">11</xref>
            </sup> or determining whether there are protocol violations: “Were the investigators really trained? Did they understand the protocol?”<sup>
               <xref ref-type="bibr" rid="bibr9-0092861512442057">9</xref>
            </sup>
         </p>
         <p>There will always be questions raised about the quality of data if 100% SDV is abandoned, but there has been little research into the consequences of errors remaining in the database. One study using simulations shows that there is little impact of large random transcription errors up to a frequency of 5%.<sup>
               <xref ref-type="bibr" rid="bibr17-0092861512442057">17</xref>
            </sup> The generalizability of this limited study, which was carried out in a model-rich pharmacokinetic context, is questionable in a model-poor phase III clinical trial context. The simulations in section 4 show that in the case of response-type data, there is a loss of power in a superiority trial, and because the bias is towards zero, it is an issue for the sponsor and not for regulators.  On the other hand, regulators have expressed concern that the same bias in noninferiority trials will likely lead to anticonservative inferences. The picture is not entirely clear because the results in section 5 indicate that not only is there a bias towards zero, but there is also an increase in variance that may offset the bias.  However, simple calculations show that the increase in variance will never fully compensate for the bias towards zero, and therefore, in these circumstances, there will be a small anticonservative bias in the inferences.  Of course, given the evidence that 100% SDV will not produce 100% clean data, that is the case of whether statistical sampling is used or not. </p>
      </sec>
   </body>
   <back>
      <fn-group>
         <fn fn-type="conflict" id="fn1-0092861512442057">
            <p>The author declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
         </fn>
         <fn fn-type="financial-disclosure" id="fn2-0092861512442057">
            <p>The author received no financial support for the research, authorship, and/or publication of this article. </p>
         </fn>
      </fn-group>
      <ref-list>
         <title>References</title>
         <ref id="bibr1-0092861512442057">
            <label>1</label>
            <citation citation-type="web" xlink:type="simple">
               <collab collab-type="author" xlink:type="simple">European Commission</collab>. <article-title>The Innovative Medicines Initiative (IMI) Strategic Research Agenda: creating biomedical R&amp;D leadership for Europe to benefit patients and society</article-title>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.imi.europa.eu/sites/default/files/uploads/documents/imi-gb-006v2-15022008-research-agenda_en.pdf" xlink:type="simple">http://www.imi.europa.eu/sites/default/files/uploads/documents/imi-gb-006v2-15022008-research-agenda_en.pdf</ext-link>. <comment>Accessed June 14, 2011</comment>.</citation>
         </ref>
         <ref id="bibr2-0092861512442057">
            <label>2</label>
            <citation citation-type="web" xlink:type="simple">
               <collab collab-type="author" xlink:type="simple">Food and Drug Administration</collab>. <article-title>Challenge and opportunity on the critical path to new medical products</article-title>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.fda.gov/downloads/ScienceResearch/SpecialTopics/CriticalPathInitiative/CriticalPathOpportunitiesReports/ucm113411.pdf" xlink:type="simple">http://www.fda.gov/downloads/ScienceResearch/SpecialTopics/CriticalPathInitiative/CriticalPathOpportunitiesReports/ucm113411.pdf</ext-link>. <comment>Accessed June 14, 2011</comment>.</citation>
         </ref>
         <ref id="bibr3-0092861512442057">
            <label>3</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Sheiner</surname>
                     <given-names>LB</given-names>
                  </name>
               </person-group>. <article-title>Learning versus confirming in clinical drug development</article-title>. <source>Clin Pharmacol Ther</source>. <year>1997</year>;<volume>61</volume>:<fpage>275</fpage>-<lpage>291</lpage>.</citation>
         </ref>
         <ref id="bibr4-0092861512442057">
            <label>4</label>
            <citation citation-type="web" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Beltangady</surname>
                     <given-names>M</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Brown</surname>
                     <given-names>M</given-names>
                  </name>
               </person-group>. <article-title>Enhanced clinical trial design: how it has transformed our late stage drug development</article-title>. <comment>Paper presented at the PhRMA Adaptive Design Working Group KOL Lecture Series; December 12</comment>, <year>2008</year>; <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.biopharmnet.com/doc/doc12004-05.html" xlink:type="simple">http://www.biopharmnet.com/doc/doc12004-05.html</ext-link>. <comment>Accessed June 14, 2011</comment>.</citation>
         </ref>
         <ref id="bibr5-0092861512442057">
            <label>5</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Barker</surname>
                     <given-names>AD</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Sigman</surname>
                     <given-names>CC</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Kelloff</surname>
                     <given-names>GJ</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Hylton</surname>
                     <given-names>NM</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Berry</surname>
                     <given-names>DA</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Esserman</surname>
                     <given-names>LJ</given-names>
                  </name>
               </person-group>. <article-title>I-SPY 2: an adaptive breast cancer trial design in the setting of neoadjuvant chemotherapy</article-title>. <source>Clin Pharmacol Ther</source>. <year>2009</year>;<volume>86</volume>:<fpage>97</fpage>-<lpage>100</lpage>.</citation>
         </ref>
         <ref id="bibr6-0092861512442057">
            <label>6</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Funning</surname>
                     <given-names>S</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Grahnén</surname>
                     <given-names>A</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Eriksson</surname>
                     <given-names>K</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Kettis-Linblad</surname>
                     <given-names>Å</given-names>
                  </name>
               </person-group>. <article-title>Quality assurance within the scope of Good Clinical Practice (GCP): what is the cost of GCP-related activities? A survey with the Swedish Association of the Pharmaceutical Industry (LIF)'s members</article-title>. <source>Qual Assur J</source>. <year>2009</year>;<volume>12</volume>:<fpage>3</fpage>-<lpage>7</lpage>.</citation>
         </ref>
         <ref id="bibr7-0092861512442057">
            <label>7</label>
            <citation citation-type="book" xlink:type="simple">
               <collab collab-type="author" xlink:type="simple">Society for Clinical Data Management</collab>. <article-title>Good clinical data management practices</article-title>. <publisher-loc>Washington DC</publisher-loc>; <year>2005</year>. </citation>
         </ref>
         <ref id="bibr8-0092861512442057">
            <label>8</label>
            <citation citation-type="web" xlink:type="simple">
               <collab collab-type="author" xlink:type="simple">Clinical Trials Transformation Initiative</collab>. <article-title>Effective and efficient monitoring as a component of quality in the conduct of clinical trials: final report of Workstream 2</article-title>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="https://www.trialstransformation.org/WS2%20Final%20Report.pdf" xlink:type="simple">https://www.trialstransformation.org/WS2%20Final%20Report.pdf</ext-link>. <comment>Accessed June 14, 2011</comment>.</citation>
         </ref>
         <ref id="bibr9-0092861512442057">
            <label>9</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Korieth</surname>
                     <given-names>K</given-names>
                  </name>
               </person-group>. <article-title>The high cost and questionable impact of 100% SDV</article-title>. <source>CenterWatch Mon</source>. <year>2011</year>;<volume>18</volume>:<fpage>14</fpage>-<lpage>17</lpage>.</citation>
         </ref>
         <ref id="bibr10-0092861512442057">
            <label>10</label>
            <citation citation-type="web" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Hines</surname>
                     <given-names>S</given-names>
                  </name>
               </person-group>. <article-title>Targeting source document verification</article-title>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.appliedclinicaltrialsonline.com/appliedclinicaltrials/Articles/Targeting-Source-Document-Verification/ArticleStandard/Article/detail/706505" xlink:type="simple">http://www.appliedclinicaltrialsonline.com/appliedclinicaltrials/Articles/Targeting-Source-Document-Verification/ArticleStandard/Article/detail/706505</ext-link>. <comment>Accessed June 14, 2011</comment>. </citation>
         </ref>
         <ref id="bibr11-0092861512442057">
            <label>11</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Tantsyura</surname>
                     <given-names>V</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Grimes</surname>
                     <given-names>I</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Mitchel</surname>
                     <given-names>J</given-names>
                  </name>
                  <etal/>
               </person-group>. <article-title>Risk-based source data verification approaches: pros and cons</article-title>. <source>Drug Inf J</source>. <year>2010</year>;<volume>44</volume>:<fpage>745</fpage>-<lpage>756</lpage>.</citation>
         </ref>
         <ref id="bibr12-0092861512442057">
            <label>12</label>
            <citation citation-type="book" xlink:type="simple">
               <collab collab-type="author" xlink:type="simple">British Standards Institute</collab>. <article-title>Sampling procedures for inspection by attributes, part 1: sampling schemes indexed by acceptance quality limit (AQL) for lot-by-lot inspection, BS6001</article-title>. <publisher-loc>London, UK</publisher-loc>: <year>1999</year>.</citation>
         </ref>
         <ref id="bibr13-0092861512442057">
            <label>13</label>
            <citation citation-type="book" xlink:type="simple">
               <collab collab-type="author" xlink:type="simple">International Standards Organisation</collab>. <article-title>Sampling procedures for inspection by attributes, part 1: sampling schemes indexed by acceptance quality limit (AQL) for lot-by-lot inspection, ISO2859</article-title>. <publisher-loc>Geneva, Switzerland</publisher-loc>: <year>1999</year>. </citation>
         </ref>
         <ref id="bibr14-0092861512442057">
            <label>14</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Bross</surname>
                     <given-names>I</given-names>
                  </name>
               </person-group>. <article-title>Misclassification in 2 x 2 tables</article-title>. <source>Biometrics</source>. <year>1954</year>;<volume>10</volume>:<fpage>478</fpage>-<lpage>486</lpage>.</citation>
         </ref>
         <ref id="bibr15-0092861512442057">
            <label>15</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Goldberg</surname>
                     <given-names>JD</given-names>
                  </name>
               </person-group>. <article-title>The effects of misclassification on the bias in the difference between two proportions and the relative odds in the fourfold table</article-title>. <source>J Am Stat Assoc</source>. <year>1975</year>;<volume>70</volume>:<fpage>561</fpage>-<lpage>567</lpage>.</citation>
         </ref>
         <ref id="bibr16-0092861512442057">
            <label>16</label>
            <citation citation-type="web" xlink:type="simple">
               <collab collab-type="author" xlink:type="simple">International Conference of Harmonisation</collab>. <article-title>E6: guideline for Good Clinical Practice</article-title>. <comment>Available at</comment>: <ext-link ext-link-type="uri" xlink:href="http://www.ema.europa.eu/pdfs/human/ich/013595en.pdf" xlink:type="simple">http://www.ema.europa.eu/pdfs/human/ich/013595en.pdf</ext-link>. <publisher-loc>Geneva, Switzerland</publisher-loc>: <publisher-name>International Standards Organisation</publisher-name>. <comment>Accessed June 14, 2011</comment>.</citation>
         </ref>
         <ref id="bibr17-0092861512442057">
            <label>17</label>
            <citation citation-type="journal" xlink:type="simple">
               <person-group person-group-type="author">
                  <name name-style="western">
                     <surname>Grahnen</surname>
                     <given-names>AE</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Karlsson</surname>
                     <given-names>K</given-names>
                  </name>
                  <name name-style="western">
                     <surname>Bragazzi</surname>
                     <given-names>F</given-names>
                  </name>
               </person-group>. <article-title>Impact of transcription errors on the outcome of a clinical trial</article-title>. <source>Clin Pharmacol Ther</source>. <year>2007</year>;<volume>81</volume>:<fpage>P1</fpage>-<lpage>29</lpage>.</citation>
         </ref>
      </ref-list>
   </back>
</article>