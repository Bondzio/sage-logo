<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BJP</journal-id>
<journal-id journal-id-type="hwp">spbjp</journal-id>
<journal-title>British Journal of Pain</journal-title>
<issn pub-type="ppub">2049-4637</issn>
<issn pub-type="epub">XXXX-XXXX</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/2049463712449961</article-id>
<article-id pub-id-type="publisher-id">10.1177_2049463712449961</article-id>
<title-group>
<article-title>Evaluating the impact of pain education: how do we know we have made a difference?</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Briggs</surname><given-names>Emma</given-names></name>
</contrib>
<aff id="aff1-2049463712449961">King’s College London, Florence Nightingale School of Nursing and Midwifery, London, UK</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-2049463712449961">Emma Briggs, King’s College London, Florence Nightingale School of Nursing and Midwifery, 57 Waterloo Road, London SE1 8WA, UK Email: <email>emma.briggs@kcl.ac.uk</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>6</volume>
<issue>2</issue>
<fpage>85</fpage>
<lpage>91</lpage>
<permissions>
<copyright-statement>© The British Pain Society 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">British Pain Society</copyright-holder>
</permissions>
<abstract>
<title>Summary points</title>
<p>1. Education is a core activity for most healthcare professionals working in pain management and an effective evaluation strategy should assess its impact.</p>
<p>2. Evaluation may have one or more purposes: accountability, development or knowledge generation. Other key principles include making evaluation integral to the education process, reflecting with learners on progress, self-evaluation by the pain educator and involving all the key stakeholders.</p>
<p>3. A wide variety of methods are available, but the choice will be influenced by the nature and amount of the pain education, number of learners, purpose of the evaluation and time and resources available.</p>
<p>4. Patient education can be evaluated through knowledge and attitude questionnaires, concordance with the treatment plan, satisfaction and pain- and disability-related measures.</p>
<p>5. Further research is needed to explore the specific strategies or combination of techniques that are effective for different groups, and build on the theoretical base underpinning effective pain education and evaluation for patients and professionals.</p>
<p>6. The importance of education for the public has also been recently recognised, but this wider educational initiative should also be fully evaluated to assess whether this initiative is making a difference.</p>
</abstract>
<kwd-group>
<kwd>Pain Education</kwd>
<kwd>Evaluation</kwd>
<kwd>Patient Education</kwd>
<kwd>Public Education</kwd>
<kwd>undergraduates</kwd>
<kwd>postgraduates</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-2049463712449961" sec-type="intro">
<title>Introduction</title>
<p>Amongst the pain community, the importance of pain education is widely accepted and recognised. It is a key part of (an often) complex treatment plan for patients, and educating the undergraduate and postgraduate healthcare workforce is an essential strategy for promoting effective practice. The importance of public education has also been recently highlighted with campaigns from The Patients Association<sup><xref ref-type="bibr" rid="bibr1-2049463712449961">1</xref></sup> and priorities identified at the Pain Summit 2011.<sup><xref ref-type="bibr" rid="bibr2-2049463712449961">2</xref></sup> This paper explores the key principles of evaluation in pain education and highlights issues in these three arenas: the education of patients, professionals and the public. Practical suggestions are provided for enhancing the evaluation process in pain education. There are many exciting and innovative educational initiatives locally, nationally and internationally, but there is a need to assess their impact. We need to evaluate whether pain education is making a difference and, if so, by how much?</p>
<p>In many ways, evaluation forms part of everyday practice as we examine the effectiveness of care provided on an individual basis or by exploring particular groups through feedback, audit and research. The American Evaluation Association<sup><xref ref-type="bibr" rid="bibr3-2049463712449961">3</xref></sup> defines evaluation more formally as ‘assessing the strengths and weaknesses of programs, policies, personnel, products, and organizations to improve their effectiveness’. This statement reflects the many levels at which evaluation can occur and comes from one of the national and international bodies on evaluation. Evaluation is a specific discipline with professional organisations, dedicated journals and a theoretical base to underpin the practice of evaluation. The focus here is much more specific, exploring the key principles of educational evaluation in order to ultimately enhance pain practice.</p>
</sec>
<sec id="section2-2049463712449961">
<title>Purpose and key principles of evaluation</title>
<p>Evaluating the impact of pain education in any arena can be enhanced by using some key principles. Throughout this section, the term <italic>learner</italic> is used to describe the person benefiting from education, recognising that this could be a patient, student, qualified professional or the public. Similarly, <italic>educator</italic> is used when referring to those who design and/or facilitate the learning process.</p>
<p>Reflecting on the body of research and development on evaluation, Chelimsky<sup><xref ref-type="bibr" rid="bibr4-2049463712449961">4</xref></sup> identified three purposes of evaluation: accountability (measuring results), development (strengthening the intervention and empowering stakeholders) and knowledge generation (obtaining a deeper understanding of the process, similar to research) (<xref ref-type="table" rid="table1-2049463712449961">Table 1</xref>). Evaluating pain education may have more than one purpose, but the aim needs to be clearly identified along with the various stakeholders involved, such as learners, educators and commissioners. The purpose should also be clear to these groups, and learners in particular need to know how their feedback and input will be used and whether it will benefit them or others, or both. It may be helpful to distinguish between an evaluation that is formative and used mid-way to improve the current learner’s experience and summative evaluation that will help future learners.<sup><xref ref-type="bibr" rid="bibr5-2049463712449961">5</xref></sup></p>
<table-wrap id="table1-2049463712449961" position="float">
<label>Table 1.</label>
<caption>
<p>Three main purposes of pain education (based on Chelimsky’s framework)<sup><xref ref-type="bibr" rid="bibr4-2049463712449961">4</xref></sup>.</p></caption>
<graphic alternate-form-of="table1-2049463712449961" xlink:href="10.1177_2049463712449961-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Purpose of evaluation</th>
<th align="left">Common methods</th>
<th align="left">Pain education examples</th>
</tr>
</thead>
<tbody>
<tr>
<td><bold>Accountability:</bold> measuring results or efficiency for stakeholders. Focus is the outcome and whether change is due to the educational intervention</td>
<td>Typically quantitative, comparative evaluative methods, e.g. pre and post evaluation, quasi-experimental techniques</td>
<td>Assessment of knowledge and attitudes towards pain before and after teaching sessions or pain management programme (PMP)<break/>Survey of public knowledge following a national campaign</td>
</tr>
<tr>
<td><bold>Development:</bold> providing information to empower and strengthen educational practice, a community or organisation. Focus is the process and outcome</td>
<td>Information is collected retrospectively and prospectively. Case studies and stakeholder evaluations. Can include performance results</td>
<td>Mid-session or course evaluation to inform the rest of the teaching or PMP<break/>User feedback on professional education</td>
</tr>
<tr>
<td><bold>Knowledge:</bold> obtaining a deeper understanding of learning or change. Focus is more on the process and less on the outcome</td>
<td>Methods that explore issues to generate understanding or theory, e.g. interviews, focus groups</td>
<td>Examining patient’s experience of an educational programme and the factors that led to change or lack of change</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The evaluation of pain education should be planned before the learning takes place in order to select the optimum techniques and timing. The RUFDATA framework<sup><xref ref-type="bibr" rid="bibr6-2049463712449961">6</xref></sup> was developed to support those starting out and provides a useful aide memoire when planning any evaluation.</p>
<list id="list1-2049463712449961" list-type="bullet">
<list-item><p>What are our <bold>Reasons and Purposes</bold> for evaluation (e.g. planning, developing, accountability)?</p></list-item>
<list-item><p>What will be our <bold>Uses</bold> of our evaluation (e.g. staff development, sharing good practice)?</p></list-item>
<list-item><p>What will be the <bold>Foci</bold> for our evaluations (range of activities, connect priority areas and original aims)?</p></list-item>
<list-item><p>What will be our <bold>Data and Evidence</bold> for our evaluations (e.g. numerical, observational, qualitative)?</p></list-item>
<list-item><p>Who will be the <bold>Audience</bold> for our evaluations (e.g. commissioners, educators, employers)?</p></list-item>
<list-item><p>What will be the <bold>Timing</bold> for our evaluations (should coincide with decision-making cycle)?</p></list-item>
<list-item><p>Who should be the <bold>Agency</bold> conducting the evaluations (e.g. you, external evaluators)?</p></list-item></list>
<p>Many evaluations, such as feedback forms, focus on the outcome and learner satisfaction, but this gives very limited information and does not explore how effective the pain teaching and learning has been. This also implies a passive experience rather than the learner taking an active part in the process, and evaluations should encourage learners to reflect on their learning, evaluating both the process and the outcome. The intended goals are important to include (referred to as convergent evaluation)<sup><xref ref-type="bibr" rid="bibr5-2049463712449961">5</xref></sup> and may ask participants to describe the most useful element or extent to which the planned learning outcomes or own goals were met. Unintended outcomes (divergent evaluation)<sup><xref ref-type="bibr" rid="bibr5-2049463712449961">5</xref></sup> are also valuable and can be explored by asking people to identify additional knowledge or skills gained, what they feel most confident about or the aspect they found most challenging.</p>
<p>The evaluation technique chosen should be aligned with the pain education provided in terms of resources or time spent. For example, a pain education programme will have a detailed evaluation and particular time points but a one-off teaching session to patients may simply involve written or verbal feedback. Ensuring that the evaluation occurs as close as possible to the learning experience and that it is accessible to the participants in terms of readability (for written evaluations) and access (e.g. electronic evaluations) will maximise the return rate and avoid frustration.</p>
<p>The learner’s perspective is central to the evaluation process and the emphasis should be on building on strengths and continuously improving pain education rather than proving that a teaching session, programme or learning technique works.<sup><xref ref-type="bibr" rid="bibr5-2049463712449961">5</xref>,<xref ref-type="bibr" rid="bibr7-2049463712449961">7</xref></sup> In exploring and improving pain education, different sources of evidence can be sought and different stakeholders included. The pain educator has to be the next key stakeholder and self-evaluation and critical reflection are important skills in considering the quality of the learning. Peer feedback from a colleague can also be helpful as part of the evaluation process for individual educators and to encourage learning from each other. It is also a necessary element of university teaching through internal processes and the external examiner system.</p>
<p>Other stakeholders may include colleagues, managers, universities, funding, commissioning or quality assurance bodies. They may require much wider evidence as part of the evaluation including attendance, cost-effectiveness, student assignment results and completion rates, as well as impact data such as patient satisfaction and improvement measures.</p>
</sec>
<sec id="section3-2049463712449961" sec-type="methods">
<title>Evaluation methods</title>
<p>The choice of method will be influenced by answers to questions in the planning phase and ultimately determined by the nature of the pain education, number of learners, the purpose of the evaluation and time and resources available. With a large student group quantitative methods such as a questionnaires or instant audience response systems may be favoured to capture data from the whole group for accountability purposes. Equally, the educator could interview a small number of students in order to perform a development evaluation and enhance their future learning experiences about pain. The sheer number of evaluation methods available is a positive reflection on the fact there is not one ‘best fit’<sup><xref ref-type="bibr" rid="bibr8-2049463712449961">8</xref></sup> and there are a number of tools available to us to gather evidence. <xref ref-type="table" rid="table2-2049463712449961">Table 2</xref> highlights some core evaluation methods, their uses and resources implications. The Evaluation Cookbook,<sup><xref ref-type="bibr" rid="bibr8-2049463712449961">8</xref></sup> upon which <xref ref-type="table" rid="table2-2049463712449961">Table 2</xref> is based, provides some excellent, practical advice on designing and using these strategies and further options.</p>
<table-wrap id="table2-2049463712449961" position="float">
<label>Table 2.</label>
<caption>
<p>Common evaluation methods (based on ref. 8).</p>
</caption>
<graphic alternate-form-of="table2-2049463712449961" xlink:href="10.1177_2049463712449961-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Evaluation method</th>
<th align="left">Description</th>
<th align="left">Resource comments</th>
</tr>
</thead>
<tbody>
<tr>
<td>Checklists</td>
<td>Multiple choice questions on pre-determined standards</td>
<td>Low–moderate preparation time, learner time and analysis time. Can be delivered online</td>
</tr>
<tr>
<td>Confidence logs</td>
<td>Learner’s confidence rating at different time points</td>
<td>Low–moderate preparation time, learner time and analysis time</td>
</tr>
<tr>
<td>Cost-effectiveness</td>
<td>Educator-only analysis of cost vs. outputs and analysing alternative learning approaches</td>
<td>Moderate–high preparation and administration time</td>
</tr>
<tr>
<td>Focus groups</td>
<td>Group interview (6–12 people) used for development evaluation</td>
<td>Low preparation time, moderate learner and analysis time</td>
</tr>
<tr>
<td>Interviews</td>
<td>Individual interviews used for development evaluation</td>
<td>Moderate–high preparation, learner and analysis time</td>
</tr>
<tr>
<td>Nominal group techniques</td>
<td>Individual reflection followed by small group reflection and voting on top issues</td>
<td>Low preparation time. Moderate learner time</td>
</tr>
<tr>
<td>Pre and post testing</td>
<td>Assessing the impact before and after learning, usually on knowledge and attitudes</td>
<td>Moderate–high preparation, learner and analysis time</td>
</tr>
<tr>
<td>Questionnaires</td>
<td>Online or paper survey including a range of questions</td>
<td>Moderate preparation and analysis, low learner time</td>
</tr>
<tr>
<td>System log</td>
<td>Tracking use of online resources</td>
<td>Low–moderate preparation and analysis</td>
</tr>
</tbody>
</table></table-wrap>
<p>Pain education can be delivered to a variety of learners in different settings using different techniques, but there are core principles for an effective evaluation:</p>
<list id="list2-2049463712449961" list-type="bullet">
<list-item><p>clearly identifying the purpose, effective planning, encouraging learners to reflect on their role in learning, self-evaluation by the educator, ensuring a continuous evaluation process and involving key stakeholders.</p></list-item>
</list>
<p>There a number of strategies that can be used, but there are, however, some unique issues to consider in the evaluation of patient, professional and public education.</p>
<sec id="section4-2049463712449961">
<title>Evaluating patient education</title>
<p>Patient education is usually part of a treatment plan; an interprofessional, complex intervention to which there are several interacting components.<sup><xref ref-type="bibr" rid="bibr9-2049463712449961">9</xref></sup> This presents significant challenges in evaluating and researching the educational element and its effectiveness. However, outcome measures have traditionally focused on changes in knowledge, attitudes, concordance or increased medication use and impact on pain scores. There is a large range of educational interventions available including one-to-one counselling, group sessions, audio guides, information leaflets, books on prescription schemes, DVDs/podcasts or innovations such as online support groups and telecare. In reality, strategies are chosen based on the resources available and are often combined to address different learning needs and provide follow-up materials.</p>
<p>The evidence around patient education is slowly growing and research studies have evaluated individual educational interventions or specific combinations of techniques. Cancer pain education is an example of an area where there is a fairly well-developed body of knowledge and systematic reviews on the effectiveness of education.<sup><xref ref-type="bibr" rid="bibr10-2049463712449961">10</xref><xref ref-type="bibr" rid="bibr11-2049463712449961"/>–<xref ref-type="bibr" rid="bibr12-2049463712449961">12</xref></sup> The most recent systematic review and meta-analysis<sup><xref ref-type="bibr" rid="bibr11-2049463712449961">11</xref></sup> pooled results from 15 trials (<italic>n</italic> = 3501), suggesting improved knowledge and attitudes scores (0.5 point on 0–5 scale) and reduced average and worst pain scores (average: 1.1; worst: 0.78 on 0–10 scale). Although these appear to be small changes, the authors highlight that these may be more effective than a co-analgesic such as a paracetamol or gabapentin (based on some previous drug trials). Mixed results were found on measurements around self-efficacy, and there was no effect on medication adherence or reduction in pain interference on daily activities.</p>
<p>Systematic reviews can helpfully summarise and assess the impact of pain education, but the diverse results illustrate the complexities around researching the experience of pain. There are many methodological challenges and current research and knowledge synthesis reviews place an emphasis on the randomised controlled trial. This approach alone may not be enough to evaluate the full impact of pain education as part of a complex intervention. Research involving other designs lack the homogeneity that is required to make reasonable comparisons between educational interventions and patient groups. There is also limited insight into the social and cultural context in which pain and patient education occurs. Future research needs to focus on which groups will have the greatest response to which interventions. This will help clinicians make informed decisions about strategies to use with pain sufferers.</p>
<p>In the meantime, knowledge and attitude questionnaires, concordance with the treatment plan, satisfaction and pain- and disability-related measures are practical ways of evaluating pain education and treatment. The choice of strategy will depend on the nature of the pain and resources, including time available. Our challenge for the future will be to evaluate patient education with the increasing and creative use of technology, such as using Internet-based support and mobile phone applications (apps). The availability and quality of these for current patients may vary considerably.<sup><xref ref-type="bibr" rid="bibr13-2049463712449961">13</xref></sup></p>
</sec>
<sec id="section5-2049463712449961">
<title>Evaluating pain education for healthcare professionals</title>
<p>Conferences, workshops, seminars, study days and formal university courses on pain all provide opportunities for people to feedback to organisers, but they may not always evaluate the impact of the learning. These educational events are real opportunities to examine whether attitudes, knowledge, skills or, in the longer term, practice have changed as a result. There are a number of models that can be helpful in guiding the evaluation of education for healthcare professionals. Kirkpatrick’s<sup><xref ref-type="bibr" rid="bibr14-2049463712449961">14</xref></sup> and Moore et al.’s<sup><xref ref-type="bibr" rid="bibr15-2049463712449961">15</xref></sup> are two frequently cited models in healthcare education that are included here.</p>
<p>Donald Kirkpatrick wrote the Kirkpatrick Evaluation Model in the 1950s and subsequently published several books on the topic. The model was aimed at helping businesses evaluate their training programmes, but it has found some resonance with healthcare education. There are four levels:</p>
<list id="list3-2049463712449961" list-type="bullet">
<list-item><p><bold>Level 1: Reaction</bold> – the degree to which participants react favourably to the education.</p></list-item>
<list-item><p><bold>Level 2: Learning</bold> – the degree to which participants acquire the intended knowledge, skills, attitudes, confidence and commitment based on their participation.</p></list-item>
<list-item><p><bold>Level 3: Behaviour</bold> – the degree to which participants apply what they learned during the education when they are back on the job.</p></list-item>
<list-item><p><bold>Level 4: Results</bold> – the degree to which specific outcomes occur as a result of the education event and subsequent reinforcement.</p></list-item>
</list>
<p>Moore’s framework is more detailed and was proposed as a guide for the design and evaluation of continuing medical education. It consists of seven levels. At the most basic levels are participation rates and satisfaction, increasing to more advanced levels of knowledge and skill acquisition, competence in an educational setting, performance in practice and then impact on patient and community outcomes. <xref ref-type="table" rid="table3-2049463712449961">Table 3</xref> gives examples of the sources of data that can be collected at each of these levels. Most pain educators are passionate about influencing practice and improving pain management by facilitating learning in others. However, evaluating the impact of pain education at those higher levels of performance, patient and community outcomes, is difficult to achieve because of the practicalities, resource implications and complex number of variables.</p>
<table-wrap id="table3-2049463712449961" position="float">
<label>Table 3.</label>
<caption>
<p>Moore et al.’s framework for planning and assessing educational activities<sup><xref ref-type="bibr" rid="bibr15-2049463712449961">15</xref></sup>.</p></caption>
<graphic alternate-form-of="table3-2049463712449961" xlink:href="10.1177_2049463712449961-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Level</th>
<th align="left">Description</th>
<th align="left">Sources of data</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>Participation: number of learners</td>
<td>Attendance records</td>
</tr>
<tr>
<td>2</td>
<td>Satisfaction: degree to which expectations of participants about setting and delivery were met</td>
<td>Questionnaire at the end</td>
</tr>
<tr>
<td>3A</td>
<td>Learning: Declarative – extent to which participants state what the education intended them to know</td>
<td>Pre and post knowledge tests. Self-report of knowledge gain</td>
</tr>
<tr>
<td>3B</td>
<td>Learning: Procedural – extent to which participants state how to do what the education intended them to know how to do</td>
<td>Pre and post knowledge tests. Self-report of knowledge gain</td>
</tr>
<tr>
<td>4</td>
<td>Competence: degree to which participants show in an educational setting how to do what the educational activity intended them to do</td>
<td>Observation in an educational setting. Self-report of competence and intention to change</td>
</tr>
<tr>
<td>5</td>
<td>Performance: degree to which participants do what the educational activity intended them to do in their practice</td>
<td>Observation of performance in patient care setting: patient charts, administrative databases. Self-report of performance</td>
</tr>
<tr>
<td>6</td>
<td>Patient health: degree to which the health status of patients improves due to changes in practice behaviour of participants</td>
<td>Health status measures recorded in patient charts or administrative databases. Patient self-report of health status</td>
</tr>
<tr>
<td>7</td>
<td>Community health: degree to which the health status of a community of patients changes due to changes in practice behaviours of participants</td>
<td>Epidemiological data and reports. Community self report</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>This challenge is reflected in the current research literature. A recent review of interprofessional pain education studies<sup><xref ref-type="bibr" rid="bibr16-2049463712449961">16</xref></sup> compared evaluation outcome measures with an adapted version of Kirkpatrick’s model. The results revealed that 89.3% (<italic>n</italic> = 441) of papers explored levels 1 and 2 (satisfaction and impact on knowledge, and attitudes) and none of the included studies had evaluated the benefits to patients. An examination of the pain education literature available reveals a similar trend, with papers focusing on knowledge increase and attitude shift rather than patient outcomes.</p>
<p>There is a need for further research to investigate the effectiveness of pain education for healthcare professionals, identify the optimal teaching and learning methods, share good practice, increase the number of publications and devise creative strategies to assess the impact of education beyond the classroom and consider those experiencing pain.</p>
</sec>
<sec id="section6-2049463712449961">
<title>Evaluating pain education for the public</title>
<p>The importance of public education around pain is recognised by many patient and professional organisations. The Patients Association<sup><xref ref-type="bibr" rid="bibr1-2049463712449961">1</xref></sup> has an established campaign around pain, and public education has been identified as a priority at the first UK Pain Summit 2011.<sup><xref ref-type="bibr" rid="bibr2-2049463712449961">2</xref></sup> This latter campaign aims to increase the knowledge skills of individuals and communities in order to reduce the impact of chronic pain. Three specific recommendations of education and public health working groups are:</p>
<list id="list4-2049463712449961" list-type="bullet">
<list-item><p>The media needs to create positive messages about coping and positive role models.</p></list-item>
<list-item><p>We need to understand the communities that we are trying to reach and use appropriate platforms.</p></list-item>
<list-item><p>Public information – an information campaign to focus on the commonalities of chronic pain and other long-term conditions.</p></list-item></list>
<p>A 3–5-year programme will be developed to include strategies based on these recommendations. Evaluating the impact of these campaigns is as important as the education itself and should be built into the programme of work. National pain education campaigns need a more complex evaluation that may involve specific outcomes measurements, surveys, analysis of media coverage or resource use (e.g. website activity logs) and a cost–benefit analysis. After designing and implementing such an exciting campaign, it is important to demonstrate the impact it is having.</p>
</sec></sec>
<sec id="section7-2049463712449961" sec-type="conclusions">
<title>Summary and conclusion</title>
<p>Pain education is a core activity of pain management practice and evaluation should be an integral component of any educational strategy. The core principles for an effective evaluation include identifying the purpose, planning, encouraging learners to reflect on their role in learning, self-evaluation by the educator, ensuring a continuous evaluation process and involving key stakeholders. Educators can choose from a wide range of evaluation techniques depending on the resources available and focus of the evaluation, whether it is accountability, development or knowledge generation.</p>
<p>Different areas of pain management practice present different challenges. Patient education can be evaluated through knowledge and attitude questionnaires, concordance with the treatment plan, satisfaction and pain- and disability-related measures. Further research is needed to explore the context of patient education and the specific strategies or combination of techniques that are effective for different groups. The theoretical base underpinning effective pain education and evaluation for professionals also needs further development. Models can be useful to guide the assessment of impact on learners but we need to move beyond satisfaction and aim for the higher levels that will investigate whether online or classroom learning is having a positive effect on behaviour in practice and patient outcomes. Finally, public campaigns around pain in the UK will offer an exciting opportunity to highlight the importance of pain management and engage relevant communities; this wider educational initiative should be fully evaluated to ensure that the programme meets the original objectives.</p>
<p>Making a difference is what motivates most healthcare professionals in pain management. It is only through an appropriate evaluation strategy that we can be confident that pain education has really made a difference to patients, professionals, organisations and the public.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors.</p></fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-2049463712449961">
<label>1.</label>
<citation citation-type="web">
<collab>The Patients Association</collab>. <article-title>Public attitudes to pain</article-title>, <comment><ext-link ext-link-type="uri" xlink:href="http://www.patients-association.com/Default.aspx?tabid=93">www.patients-association.com/Default.aspx?tabid=93</ext-link></comment> (<year>2010</year>, <access-date>accessed 10 February 2012</access-date>).</citation>
</ref>
<ref id="bibr2-2049463712449961">
<label>2.</label>
<citation citation-type="web">
<collab>Chronic Pain Policy Coalition, British Pain Society, Faculty of Pain Medicine, Royal College of General Practitioners</collab>. <article-title>Pain Summit 2011</article-title>. <source>Policy connect</source>, <comment><ext-link ext-link-type="uri" xlink:href="http://www.painsummit.org.uk/documents">www.painsummit.org.uk/documents</ext-link></comment> (<year>2011</year>, <access-date>accessed 10 February 2012</access-date>).</citation>
</ref>
<ref id="bibr3-2049463712449961">
<label>3.</label>
<citation citation-type="web">
<collab>American Evaluation Association</collab>. <article-title>Mission statement</article-title>, <comment><ext-link ext-link-type="uri" xlink:href="http://www.eval.org/aboutus/organization/aboutus.asp">www.eval.org/aboutus/organization/aboutus.asp</ext-link></comment> (<access-date>accessed 10 February 2012</access-date>).</citation>
</ref>
<ref id="bibr4-2049463712449961">
<label>4.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Chelimsky</surname><given-names>E</given-names></name>
</person-group>. <article-title>Thoughts for a new evaluation society</article-title>. <source>Evaluation</source> <year>1997</year>; <volume>3</volume>(<issue>1</issue>): <fpage>97</fpage>–<lpage>109</lpage>.</citation>
</ref>
<ref id="bibr5-2049463712449961">
<label>5.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Moore</surname><given-names>I</given-names></name>
</person-group>. <article-title>A guide to practice: evaluating your teaching. 2009</article-title>. <source>Centre for Promoting Learner Autonomy, Sheffield Hallam University</source>, <comment><ext-link ext-link-type="uri" xlink:href="http://extra.shu.ac.uk/cetl/cpla/resources.html">http://extra.shu.ac.uk/cetl/cpla/resources.html</ext-link></comment> (<year>2009</year>, <access-date>accessed 10 February 2012</access-date>).</citation>
</ref>
<ref id="bibr6-2049463712449961">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saunders</surname><given-names>M</given-names></name>
</person-group>. <article-title>Beginning an evaluation with RUFDATA: theorising a practical approach to evaluation planning</article-title>. <source>Evaluation</source> <year>200</year>; <volume>6</volume>(<issue>1</issue>): <fpage>7</fpage>–<lpage>21</lpage>.</citation>
</ref>
<ref id="bibr7-2049463712449961">
<label>7.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ramsden</surname><given-names>P</given-names></name>
</person-group>. <source>Learning to teach in higher education</source>. <publisher-loc>London</publisher-loc>: <publisher-name>Routledge</publisher-name>, <year>2003</year>.</citation>
</ref>
<ref id="bibr8-2049463712449961">
<label>8.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Oliver</surname><given-names>M</given-names></name>
<name><surname>Conole</surname><given-names>G</given-names></name>
</person-group>. <article-title>Choosing a methodology</article-title>. In: <person-group person-group-type="editor">
<name><surname>Harvey</surname><given-names>J</given-names></name>
</person-group>. <source>Evaluation cookbook</source>. <publisher-name>Learning Technology Dissemination Initiative</publisher-name>, <comment><ext-link ext-link-type="uri" xlink:href="http://www.icbl.hw.ac.uk/ltdi/cookbook/">www.icbl.hw.ac.uk/ltdi/cookbook/</ext-link></comment> (<year>1999</year>, <access-date>accessed 10 February 2012</access-date>).</citation>
</ref>
<ref id="bibr9-2049463712449961">
<label>9.</label>
<citation citation-type="web">
<collab>Medical Research Council</collab>. <article-title>Developing and evaluating complex interventions: new guidance</article-title>, <comment><ext-link ext-link-type="uri" xlink:href="http://www.mrc.ac.uk/complexinterventionsguidance">www.mrc.ac.uk/complexinterventionsguidance</ext-link></comment> (<year>2008</year>, <access-date>accessed 10 February 2012</access-date>).</citation>
</ref>
<ref id="bibr10-2049463712449961">
<label>10.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Allard</surname><given-names>P</given-names></name>
<name><surname>Maunsell</surname><given-names>E</given-names></name>
<name><surname>Labbé</surname><given-names>J</given-names></name>
<name><surname>Dorval</surname><given-names>M</given-names></name>
</person-group>. <article-title>Educational interventions to improve cancer pain control: a systematic review</article-title>. <source>J Palliat Med</source> <year>2001</year>; <volume>4</volume>: <fpage>191</fpage>–<lpage>203</lpage>.</citation>
</ref>
<ref id="bibr11-2049463712449961">
<label>11.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bennett</surname><given-names>MI</given-names></name>
<name><surname>Bagnall</surname><given-names>AM</given-names></name>
<name><surname>Closs</surname><given-names>SJ</given-names></name>
</person-group>. <article-title>How effective are patient-based educational interventions in the management of cancer pain? Systematic review and meta-analysis</article-title>. <source>Pain</source> <year>2009</year>; <volume>143</volume>(<issue>3</issue>): <fpage>192</fpage>–<lpage>199</lpage>.</citation>
</ref>
<ref id="bibr12-2049463712449961">
<label>12.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bennett</surname><given-names>MI</given-names></name>
<name><surname>Bagnall</surname><given-names>AM</given-names></name>
<name><surname>Raine</surname><given-names>G</given-names></name><etal/>
</person-group>. <article-title>Educational interventions by pharmacists to patients with chronic pain: systematic review and meta-analysis</article-title>. <source>Clin J Pain</source> <year>2011</year>; <volume>27</volume>(<issue>7</issue>): <fpage>623</fpage>–<lpage>630</lpage>.</citation>
</ref>
<ref id="bibr13-2049463712449961">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rayen</surname><given-names>A</given-names></name>
</person-group>. <article-title>App to the future</article-title>. <source>Pain News</source> <year>2011</year> (<issue>Winter</issue>): <fpage>18</fpage>–<lpage>19</lpage>.</citation>
</ref>
<ref id="bibr14-2049463712449961">
<label>14.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kirkpatrick</surname><given-names>DL</given-names></name>
</person-group>. <source>Evaluating training programmes – the four levels</source>. <edition>3rd ed.</edition> <publisher-loc>London</publisher-loc>: <publisher-name>Berrett-Koehler Publishers</publisher-name>, <year>2006</year>.</citation>
</ref>
<ref id="bibr15-2049463712449961">
<label>15.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Moore</surname><given-names>DE</given-names></name>
<name><surname>Green</surname><given-names>JS</given-names></name>
<name><surname>Gallis</surname><given-names>HA</given-names></name>
</person-group>. <article-title>Achieving desired results and improved outcomes: integrating planning and assessment throughout learning activities</article-title>. <source>J Contin Educ Health Prof</source> <year>2009</year>; <volume>29</volume>(<issue>1</issue>): <fpage>1</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr16-2049463712449961">
<label>16.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Gillan</surname><given-names>C</given-names></name>
<name><surname>Lovrics</surname><given-names>E</given-names></name>
<name><surname>Halpern</surname><given-names>E</given-names></name>
<name><surname>Wiljer</surname><given-names>D</given-names></name>
<name><surname>Harnett</surname><given-names>N</given-names></name>
</person-group>. <article-title>The evaluation of learner outcomes in interprofessional continuing education: A literature review and an analysis of survey instruments</article-title>. <source>Med Teacher</source> <year>2011</year>; <volume>33</volume>: <fpage>e461</fpage>–<lpage>e470</lpage>.</citation>
</ref>
</ref-list>
<ref-list>
<ref id="bibr17-2049463712449961">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Carter</surname><given-names>K</given-names></name>
<name><surname>Edwards</surname><given-names>J</given-names></name>
<name><surname>Mohammad</surname><given-names>I</given-names></name><etal/>
</person-group>. <article-title>Evaluate work-based learning</article-title>. <source>Educ Prim Care</source> <year>2005</year>; <volume>16</volume>: <fpage>726</fpage>–<lpage>728</lpage>.</citation>
</ref>
<ref id="bibr18-2049463712449961">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Harvey</surname><given-names>J</given-names></name>
</person-group>. <article-title>Evaluation cookbook</article-title>. <source>Learning Technology Dissemination Initiative</source>, <comment><ext-link ext-link-type="uri" xlink:href="http://www.icbl.hw.ac.uk/ltdi/cookbook/">www.icbl.hw.ac.uk/ltdi/cookbook/</ext-link></comment> (<year>1999</year>, <access-date>accessed 10 February 2012</access-date>).</citation>
</ref>
<ref id="bibr19-2049463712449961">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Light</surname><given-names>G</given-names></name>
<name><surname>Cox</surname><given-names>R</given-names></name>
<name><surname>Calkins</surname><given-names>S</given-names></name>
</person-group>. <source>Learning and teaching in higher education: the reflective professional</source>. <edition>2nd ed.</edition> <publisher-loc>London</publisher-loc>: <publisher-name>Sage</publisher-name>, <year>2009</year>.</citation>
</ref>
</ref-list>
<sec id="section8-2049463712449961">
<title>Multiple choice questions</title>
<list id="list5-2049463712449961" list-type="order">
<list-item><p><bold>What are the three main purposes of evaluation (based on Chelimsky<sup><xref ref-type="bibr" rid="bibr4-2049463712449961">4</xref></sup>)?</bold><list id="list6-2049463712449961" list-type="alpha-lower">
<list-item><p>Knowledge, effectiveness, accountability</p></list-item>
<list-item><p>Knowledge, development, sustainability</p></list-item>
<list-item><p>Accountability, development, knowledge</p></list-item>
</list></p></list-item>
<list-item><p><bold>Which is the correct pneumonic that can be helpful in planning evaluation activities?</bold><list id="list7-2049463712449961" list-type="alpha-lower">
<list-item><p>RUFDATA</p></list-item>
<list-item><p>RUDATAS</p></list-item>
<list-item><p>REEVAL</p></list-item>
</list></p></list-item>
<list-item><p><bold>Evaluation should focus on:</bold><list id="list8-2049463712449961" list-type="alpha-lower">
<list-item><p>Outcome in most cases</p></list-item>
<list-item><p>Process in most cases and outcome when necessary</p></list-item>
<list-item><p>Process and outcome in all cases</p></list-item>
</list></p></list-item>
<list-item><p><bold>Pre and post testing of knowledge, attitudes and skills following pain education is a resource-intensive method.</bold><list id="list9-2049463712449961" list-type="alpha-lower">
<list-item><p>True</p></list-item>
<list-item><p>False</p></list-item>
</list></p></list-item>
<list-item><p><bold>Kirkpatrick’s model of evaluation has four levels. Choose the correct levels.</bold><list id="list10-2049463712449961" list-type="alpha-lower">
<list-item><p>Attendance, learning, reaction, results</p></list-item>
<list-item><p>Reaction, learning, behaviour, results</p></list-item>
<list-item><p>Reaction, satisfaction, learning, behaviour</p></list-item>
</list></p></list-item>
</list>
</sec>
<sec id="section9-2049463712449961">
<title>Answers</title>
<p>1: c; 2: a; 3: c; 4: a; 5: b.</p>
</sec>
</back>
</article>