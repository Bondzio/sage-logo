<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">TRJ</journal-id>
<journal-id journal-id-type="hwp">sptrj</journal-id>
<journal-title>Textile Research Journal</journal-title>
<issn pub-type="ppub">0040-5175</issn>
<issn pub-type="epub">1746-7748</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0040517511418565</article-id>
<article-id pub-id-type="publisher-id">10.1177_0040517511418565</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Original articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Automatic detecting anthropometric landmarks based on spin image</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Li</surname><given-names>Yue</given-names></name>
<xref ref-type="aff" rid="aff1-0040517511418565">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Zhong</surname><given-names>Yueqi</given-names></name>
<xref ref-type="aff" rid="aff1-0040517511418565">1</xref>
<xref ref-type="aff" rid="aff2-0040517511418565">2</xref>
<xref ref-type="corresp" rid="corresp1-0040517511418565"/>
</contrib>
</contrib-group>
<aff id="aff1-0040517511418565"><label>1</label>Donghua University, China.</aff>
<aff id="aff2-0040517511418565"><label>2</label>Key Laboratory of Ministry of Education on Textile Fabric Technology, China.</aff>
<author-notes>
<corresp id="corresp1-0040517511418565">Yueqi Zhong, College of Textiles, North Renming Road, Donghua University, Songjiang District, Shanghai 201620, China Email: <email>phdzyq@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2012</year>
</pub-date>
<volume>82</volume>
<issue>6</issue>
<fpage>622</fpage>
<lpage>632</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012 Reprints and permissions: sagepub.co.uk/journalsPermissions.nav</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>In this paper, we present a method to detect anthropometric landmarks on 3D human scans automatically. Our method is a template-based approach, which builds correspondences between instance models and a template model. Spin images are used to construct the similarity measure, which represents the shape features parametrically. A vertex on the instance model will be regarded as the landmark when its similarity measure against a corresponding landmark on the template model is of maximum value. Normal variation and bounding boxes are employed to speed up the comparison of spin images. The algorithm is verified on scanned human bodies with various shapes. Forty-seven landmarks are detected on the instance body. The results show that most landmarks are found accurately. Furthermore, we demonstrate the implementation of using our detected landmarks in automatic body measurement.</p>
</abstract>
<kwd-group>
<kwd>Anthropometric landmark</kwd>
<kwd>automatic detection</kwd>
<kwd>spin image</kwd>
<kwd>normal variation</kwd>
<kwd>bounding boxes</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>With the development of 3D scanning technology, 3D surface range scanned data shows its advantage in many aspects for building more accurate digital models of a real person. 3D surface data of the human body has a lot of applications such as anthropometric studies, medical diagnosis, garment design, computer animation and entertainment. The high resolution surface data which provides accurate details of the body shape makes a remarkable change, especially in the field of anthropometry and garment design. However, the process of automatic landmark positioning on a scanned human body with various shapes is still a challenging problem. In this paper, we present a detection method to automatically extract the landmarks from the surface data by computing the corresponding information between instance models and a template model.</p>
<sec id="sec1-0040517511418565"><title>Previous work</title>
<p>Topics related to feature point detection are intended to fully benefit from 3D body scanning. Dekker et al.<sup><xref ref-type="bibr" rid="bibr1-0040517511418565">1</xref></sup> made the exploration of feature detection operators as the first step to reconstruct the body surface. Zhong et al.<sup><xref ref-type="bibr" rid="bibr2-0040517511418565">2</xref></sup> defined target zones to find out feature points for body segmentation. Both methods required the branching points such as armpits and crotch to be located for further treatment. The extraction of tailor measurements from a model for the lower body, consisting of two stacks of ellipses, was described by Certain et al.<sup><xref ref-type="bibr" rid="bibr3-0040517511418565">3</xref></sup> Methods of body feature extraction in the application of MTM (made-to-measure) were also proposed by other groups.<sup><xref ref-type="bibr" rid="bibr4-0040517511418565">4</xref>–<xref ref-type="bibr" rid="bibr6-0040517511418565">6</xref></sup> Ju et al.<sup><xref ref-type="bibr" rid="bibr7-0040517511418565">7</xref></sup> presented an algorithm to segment the 3D human body scans by analyzing the cross-sections of the scanned body, especially the circumferences of the horizontal slices. However, the number of detected feature points was limited and less accurate. A more conventional way that computed the surface curvature in the regions of interest was introduced by Suikerbuik et al.<sup><xref ref-type="bibr" rid="bibr8-0040517511418565">8</xref></sup> The model surface was segmented into different landmark regions based on surface curvature, by Subburaj et al.,<sup><xref ref-type="bibr" rid="bibr9-0040517511418565">9</xref></sup> and used to identify landmarks on bone models. The result was not accurate when the local surface curvature of some required features was not remarkable.</p>
<p>Allen et al.<sup><xref ref-type="bibr" rid="bibr10-0040517511418565">10</xref></sup> proposed a template-based approach to fit a template human model to instances of human scans by establishing correspondences between them. All the resulting models had the same number of triangles and point-to-point correspondences. A set of anthropometric landmarks which were provided in the CAESAR (Civilian American and European Surface Anthropometry Resource) data-base<sup><xref ref-type="bibr" rid="bibr11-0040517511418565">11</xref></sup> were pre-marked manually on the template model. After solving the correspondence problem by deforming a template model to fit individual scans, the required features were determined. This template-based approach has no limits on the number of the required landmarks. However, the manually pre-marking process, which requires 76 landmarks for each individual in the CAESAR data-base, increases the time of measurement (from a few seconds to more than 30 minutes).</p>
<p>Anguelov et al.<sup><xref ref-type="bibr" rid="bibr12-0040517511418565">12</xref></sup> also proposed a template-based method to register non-rigid surfaces by using a Markov network. This algorithm was applied on 3D human scans<sup><xref ref-type="bibr" rid="bibr13-0040517511418565">13</xref></sup> by embedding an instance mesh into a template mesh. About 200 corresponding points between the template model and instance model were selected. Therefore anthropometric landmarks of the instance model were identified when the pre-identification process was fulfilled. This algorithm was extended, by Azouz et al.<sup><xref ref-type="bibr" rid="bibr14-0040517511418565">14</xref>,<xref ref-type="bibr" rid="bibr15-0040517511418565">15</xref></sup>, to solve the location of a landmark more efficiently. A pair-wise network was regarded as an instance of a probabilistic graphical model. Each node of the model was a random variable corresponding to the position of a landmark. The edges of the network represented correlations between the positions of landmark pairs. The probability associated with alandmark encoded a preference of the local surfaceproperty around this landmark and maintained the spatial relationship between this landmark and other neighboring landmarks in the network. To characterize the local surface feature, they introduced a representative technique called spin image.<sup><xref ref-type="bibr" rid="bibr16-0040517511418565">16</xref></sup> Spin image is traditionally used to describe the local surface feature by compressing the global 3D coordinates to2D local coordinates embedded in an image. Therefore the problem of establishing the correspondences is simplified to the problem of computing thesimilarities of a stack of 2D images. Starck et al.<sup><xref ref-type="bibr" rid="bibr17-0040517511418565">17</xref></sup> formulated the matching problem as a bijection in the2D spherical domain to guarantee a continuous one-to-one surface correspondence without over-folding. Ghosh et al.<sup><xref ref-type="bibr" rid="bibr18-0040517511418565">18</xref></sup> used an ICP-like framework to detect feature correspondences iteratively by using spin image to match a template with a collection of possible target meshes.</p>
<p>In this paper, we propose a method to detect an unlimited number of anthropometric landmarks on a scanned human body. The algorithm we applied is based on template fitting. Illuminated by the work of Ghosh et al.,<sup><xref ref-type="bibr" rid="bibr18-0040517511418565">18</xref></sup> we use spin image and Euclidian distance to establish correspondences for automatic landmark detection. However, different from their whole mesh registration, our method focuses on a simpler and more effective way to detect landmarks automatically. We also use these detected landmarks to achieve more subsequent implementations such asbody segmentation and skeleton extraction for body measurement.</p>
</sec>
<sec id="sec2-0040517511418565"><title>Computing spin images</title>
<p>We formulate the problem of detecting corresponding landmarks as the problem of finding the best matches to those identified landmarks on a template mesh withsimilar shape features. In another word, it is the similarity of the feature that characterizes the local surface properties and the spatial relationship of the landmarks. In the following section we explain how the similarity is computed.</p>
<sec id="sec3-0040517511418565"><title>Spin images</title>
<p>We use spin images to characterize the feature of local surface. The concept of a spin image was first introduced by Johnson<sup><xref ref-type="bibr" rid="bibr16-0040517511418565">16</xref></sup> who used it to solve the surface matching problem. A spin image is a two-dimensional histogram computed at an oriented point of a surface mesh. Two cylindrical coordinates can be defined with respect to the 3D position and the surface normal of the oriented point. The dense points cloud on the mesh is projected on this 2D image. By accumulating 2D points in discrete bins, spin images are generated. Darker pixels in the image indicate those particular bins contain more projected points. <xref ref-type="fig" rid="fig1-0040517511418565">Figure 1</xref> shows the coordinate system used to compute the spin image at an oriented point. The radial coordinate, <italic>α</italic>, is defined as the perpendicular distance to the line through the surface normal, and the elevation coordinate, <italic>β</italic>, is defined as the signed perpendicular distance to the target plane defined by vertex normal and its position.
<fig id="fig1-0040517511418565" position="float"><label>Figure 1.</label><caption><p>Cylindrical coordinates system.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig1.tif"/></fig></p>
<p>As a representation of shape feature, spin image is generated in an object-centered coordinate system, which means the description of the surface is view-independent. Hence the surfaces can be directly compared without pre-alignment. In practice, the 3D positions of vertices in the mesh are mapped into 2D using the spin map for each oriented point basis. By accumulating 2D points in discrete bins, spin image with appropriate bin size can be generated accordingly. Here, ‘bin size’ refers to the number of bins contained in each image. It is determined according to the mesh resolution of the body scan.<sup><xref ref-type="bibr" rid="bibr16-0040517511418565">16</xref></sup> As shown in <xref ref-type="fig" rid="fig2-0040517511418565">Figure 2</xref>, to facilitate our explanation, the generated spin image has 30 × 30 bin size per image. Projecting the dense points cloud to limited bins of 2D images actually compresses the abundant surface data. This will facilitate the computation when comparing the similarity. Furthermore, it can reduce the effect of poor surface data on the mesh such as clutter (extra data) and occlusion (missing data).
<fig id="fig2-0040517511418565" position="float"><label>Figure 2.</label><caption><p>Spin images for an oriented point on the surface of a human model. Left to right: a) An oriented vertex of the human model; b) The 2D points map in terms of the base vertex; c) The 2D spin image.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig2.tif"/></fig></p>
<p><xref ref-type="fig" rid="fig3-0040517511418565">Figure 3</xref> shows examples of spin images generated on two different human scans. For each scan we choose five landmarks for instance. Notice that the spin images corresponding to the same landmark from different human scans are intrinsically similar with each other.
<fig id="fig3-0040517511418565" position="float"><label>Figure 3.</label><caption><p>Spin images computed for five landmarks on two different human scans.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig3.tif"/></fig></p>
</sec>
<sec id="sec4-0040517511418565"><title>Matching algorithm</title>
<p>To set up the landmark correspondence between instance model and template model, the matched pair should have the similar shape feature and spatial position. With these two criteria, we formulate the similarity measure, <italic>S</italic>, as:
<disp-formula id="disp-formula1-0040517511418565"><label>(1)</label><graphic xlink:href="10.1177_0040517511418565-eq1.tif"/></disp-formula></p>
<p>In <xref ref-type="disp-formula" rid="disp-formula1-0040517511418565">equation 1</xref>, the first part measures the similarity of the surface feature. The ranged space of weight, <italic>β</italic>, is valued between 0 and 1. The second part calculates if the selected point is close enough to the corresponding landmark, where <italic>d</italic> stands for the Euclidean distance. The selection of <italic>β</italic> depends on the distinctiveness of the local surface feature, which will be discussed in detail later. <italic>C</italic> indicates the similarity of the generated spin images. It combines the linear correlation coefficient <italic>R</italic> and its variance affected by the amount of overlapped pixels between spin images:<sup><xref ref-type="bibr" rid="bibr19-0040517511418565">19</xref></sup>
<disp-formula id="disp-formula2-0040517511418565"><label>(2)</label><graphic xlink:href="10.1177_0040517511418565-eq2.tif"/></disp-formula>
where <italic>N</italic> is the number of overlapped pixels used in thecomputation of <italic>R. λ</italic> weights the variance against the expected value of the correlation. In practice, we define <italic>λ</italic> as:
<disp-formula id="disp-formula3-0040517511418565"><label>(3)</label><graphic xlink:href="10.1177_0040517511418565-eq3.tif"/></disp-formula>
where <italic>n</italic> is the number of spin images generated from the template scan, and <italic>N<sub>i</sub></italic> is the number of pixels in <italic>i</italic>th spin image.</p>
</sec>
</sec>
<sec id="sec5-0040517511418565"><title>Pre-marking</title>
<p>We choose a normal size, hole-free male body as our template mesh for the explanation. The posture of the subject before whole body scanning is demanded as standing with arms spread slightly apart from torso, as illustrated in <xref ref-type="fig" rid="fig3-0040517511418565">Figure 3</xref>. In this paper, 29 landmarks that provide the most useful anthropometric data forthe clothing related applications are selected. To reduce the objective error in the collection of anthropometric data, we use the unified description of the definition introduced by Simmons et al.<sup><xref ref-type="bibr" rid="bibr20-0040517511418565">20</xref></sup> Furthermore, we pick four extra landmarks from the template model to assist the automatic detection. They are points on fingertips and heels. As shown in <xref ref-type="fig" rid="fig4-0040517511418565">Figure 4</xref>, the red spheres are the main landmarks and the blue spheres are the assistant feature points. The anthropometric nomenclature of these landmarks is given in <xref ref-type="table" rid="table1-0040517511418565">Table 1</xref>.
<fig id="fig4-0040517511418565" position="float"><label>Figure 4.</label><caption><p>Positions of 29 pre-defined anthropometric landmarks.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig4.tif"/></fig>
<table-wrap id="table1-0040517511418565" position="float"><label>Table 1.</label><caption><p>The anthropometric nomenclature of landmarks</p></caption>
<graphic alternate-form-of="table1-0040517511418565" xlink:href="10.1177_0040517511418565-table1.tif"/>
<table frame="hsides">
<tbody align="left">
<tr>
<td> 1 Crown</td>
<td>16 Navel</td></tr>
<tr>
<td> 2 Right neck</td>
<td>17 Right trochanterion</td></tr>
<tr>
<td> 3 Left neck</td>
<td>18 Left trochanterion</td></tr>
<tr>
<td> 4 Suprasternal</td>
<td>19 Right buttock</td></tr>
<tr>
<td> 5 Cervicale</td>
<td>20 Left buttock</td></tr>
<tr>
<td> 6 Right acromion</td>
<td>21 Right carpus</td></tr>
<tr>
<td> 7 Left acromion</td>
<td>22 Left carpus</td></tr>
<tr>
<td> 8 Right bustpoint</td>
<td>23 Crotch</td></tr>
<tr>
<td> 9 Left bustpoint</td>
<td>24 Right patella</td></tr>
<tr>
<td>10 Right armpit</td>
<td>25 Left patella</td></tr>
<tr>
<td>11 Left armpit</td>
<td>26 Right gastrocnemius</td></tr>
<tr>
<td>12 Right olecrannon</td>
<td>27 Left gastrocnemius</td></tr>
<tr>
<td>13 Left olecranon</td>
<td>28 Right malleolus</td></tr>
<tr>
<td>14 Right waist</td>
<td>29 Left malleolus</td></tr>
<tr>
<td>15 Left waist</td>
<td/></tr>
</tbody>
</table>
</table-wrap></p>
<p>After manual pre-marking, spin images with proper parameters generated from these points are stored for future comparison. The optimal selection of parameters for spin images is determined by the average length of the mesh edges.<sup><xref ref-type="bibr" rid="bibr16-0040517511418565">16</xref></sup> In practice, the resolution of the models is around 7000–8000 points. We find that choosing 100 × 100 bin size to generate spin images seems to be appropriate in shape comparison.</p>
</sec>
<sec id="sec6-0040517511418565"><title>Detecting landmarks</title>
<p>Since spin images are generated according to the oriented points, the features of the landmarks can be categorized by the variation of the surface normal. In this sense, 29 landmarks on the surface of a template scan are divided into three types that represent different shape characteristics.</p>
<p>Geometrically, most of the landmarks have a palpable geometric feature and vary only in size and spatial location. These anatomical landmarks on each single body segment are usually constrained by a spatial relationship that is the same for each model. We therefore propose an additional characteristic for narrowing thescope of landmark identification: invariable relative position of a landmark with respect to other known landmark. This can be achieved via bounding boxes built according to the key landmarks.</p>
<sec id="sec7-0040517511418565"><title>Extraction of landmark regions</title>
<p>The equation we apply to evaluate the variation of the surface normal is defined as:
<disp-formula id="disp-formula4-0040517511418565"><label>(4)</label><graphic xlink:href="10.1177_0040517511418565-eq4.tif"/></disp-formula>
where <italic>m</italic> indicates the number of neighbors for the <italic>i</italic>thvertex, <italic>n<sub>i</sub></italic> is the normal of the <italic>i</italic>th vertex and <italic>n<sub>j</sub></italic> is the normal of its <italic>j</italic>th neighbor. The neighborhood is defined by satisfying two requirements: (a) the Euclidean distance <italic>d</italic> between these two vertices mustbe less than 20 cm; (b) the variation of the surface normal between these two vertices must be less than90°.</p>
<p>After computing the variation of the surface normal for each vertex on the template scan, three viewpoint-independent surface types can be formed accordingly. Different types refer to a different valueof <italic>β</italic> in computing the similarity measure <italic>S</italic> when we match the corresponding landmarks between the template model and the instance model. <xref ref-type="fig" rid="fig5-0040517511418565">Figure 5</xref> illustrates the normal variation on each vertex mapped with color values on the template surface. The definition of three types of surface, the categories of 29 landmarks and the selection of weight <italic>β</italic> in <xref ref-type="disp-formula" rid="disp-formula1-0040517511418565">equation 1</xref> are given in <xref ref-type="table" rid="table2-0040517511418565">Table 2</xref>.
<fig id="fig5-0040517511418565" position="float"><label>Figure 5.</label><caption><p>Variation map of surface normal on the template scan.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig5.tif"/></fig>
<table-wrap id="table2-0040517511418565" position="float"><label>Table 2.</label><caption><p>Different landmark types based on the variation of surface normal</p></caption>
<graphic alternate-form-of="table2-0040517511418565" xlink:href="10.1177_0040517511418565-table2.tif"/>
<table frame="hsides"><thead>
<tr><th>Normal variation</th>
<th><italic>v<sub>i</sub></italic> &gt;65°</th>
<th>55°&lt; <italic>v<sub>i</sub></italic> ≤65°</th>
<th><italic>v<sub>i</sub></italic> ≤55°</th></tr></thead>
<tbody align="left">
<tr>
<td>Type</td>
<td>A</td>
<td>B</td>
<td>C</td></tr>
<tr>
<td>Landmarks (The reference numbers are given in <xref ref-type="table" rid="table1-0040517511418565">Table 1</xref>.)</td>
<td>1, 2, 3, 6, 7, 10, 11, 23</td>
<td>4, 12, 13, 14, 15, 17, 18, 24, 25</td>
<td>5, 8, 9, 16, 19, 20, 21, 22, 26, 27, 28, 29</td></tr>
<tr>
<td>Value of <italic>β</italic></td>
<td>0.7</td>
<td>0.5</td>
<td>0.3</td></tr>
</tbody>
</table>
</table-wrap></p>
<p>The categorized normal variation implies that thelandmarks on the template model will fall into the same types of landmark region on the instance model. This actually builds the regional correspondence between two models. The searching of landmarks on the instance model will then be carried on the same type of region.</p>
</sec>
<sec id="sec8-0040517511418565"><title>Detecting key landmarks</title>
<p>The key landmarks which are classified into type A areoriented points on the body surface with uniqueness in shape characteristics in their vicinity. From our observation, the key landmarks can always be detected precisely. Hence they are detected on the instance scans primarily.</p>
<p>The similarity measure <italic>S</italic> in <xref ref-type="disp-formula" rid="disp-formula1-0040517511418565">equation 1</xref> gives us a way of evaluating spin images. Two spin images with the highest similarity measure are likely to come from two vertices with the same surface feature. In the detecting process, the instance model is calibrated to the same height as the template model, and then it is restored with the same calibration parameters in a reverse manner after landmark detection. For a known landmark on the template model, the spin image will be compared with the spin images extracted from the same type of landmark region on the instance model. The detailed steps follow:
<list id="list1-0040517511418565" list-type="order" prefix-word="Step">
<list-item><p>Select a key landmark <italic>q<sub>i</sub></italic> from the template model, and load its spin image Ω<sub><italic>qi</italic></sub>;</p></list-item>
<list-item><p>For type A vertex <italic>p<sub>j</sub></italic> from the instance model, if the Euclidean distance <italic>d</italic> between <italic>q<sub>i</sub></italic> and <italic>p<sub>j</sub></italic> is less than 20 cm, and the angle between oriented vertex <italic>q<sub>i</sub></italic> and <italic>p<sub>j</sub></italic> is less than 90°, load its spin image Ω<sub><italic>pj</italic></sub>;</p></list-item>
<list-item><p>Use <xref ref-type="disp-formula" rid="disp-formula1-0040517511418565">equation 1</xref> to compute the similarity measure <italic>S</italic>;</p></list-item>
<list-item><p>The type A vertex from the instance model with maximum <italic>S</italic> will be regarded as the matched key landmarks.</p></list-item>
</list></p>
<p>In practice, eight key landmarks can be easily extracted with these four steps of treatment, as shown in <xref ref-type="fig" rid="fig6-0040517511418565">Figure 6</xref> (the red spheres).
<fig id="fig6-0040517511418565" position="float"><label>Figure 6.</label><caption><p>Five bounding boxes based on the detected key landmarks.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig6.tif"/></fig></p>
</sec>
<sec id="sec9-0040517511418565"><title>Bounding boxes</title>
<p>After detecting the key landmarks, we use them to build bounding boxes for further landmark detection. The bounding boxes are defined in terms of the body parts, i.e., torso, left leg, right leg, left arm and right arm respectively. Technically, they are employed to calculate a more accurate relative position when we compute with the similarity measure.</p>
<p>To build these five bounding boxes that enclose fiveparts of human body, we have to detect four extra key landmarks to define the end of each branch. They are the vertices of the left fingertip, right fingertip, left heel and right heel (blue spheres in <xref ref-type="fig" rid="fig4-0040517511418565">Figure 4</xref>). From the variation map of the surface normal (<xref ref-type="fig" rid="fig5-0040517511418565">Figure 5</xref>), wecan classify them into type A landmarks. With these 12 key landmarks, five bounding boxes are built for the template and instance models respectively. From our observation, the thickness of the box in the<italic>z</italic>-axis has little effect on the constraint of spatial relationship. Hence, we use a uniform thickness which determined by the maximum and minimum values at the <italic>z</italic>-axis of all the vertices on the mesh for all the boxes. The corresponding landmarks which determinethe maximum and minimum values at each axis for five bounding boxes are listed in <xref ref-type="table" rid="table3-0040517511418565">Table 3</xref>. The subscript number indicates the index of each landmark, asdescribed in <xref ref-type="table" rid="table1-0040517511418565">Table 1</xref>. <xref ref-type="fig" rid="fig6-0040517511418565">Figure 6</xref> shows an example of building these five bounding boxes based on the detected key landmarks.
<table-wrap id="table3-0040517511418565" position="float"><label>Table 3.</label><caption><p>Bounding boxes based on corresponding landmarks</p></caption>
<graphic alternate-form-of="table3-0040517511418565" xlink:href="10.1177_0040517511418565-table3.tif"/>
<table frame="hsides"><thead align="center">
<tr><th>Box</th>
<th>X<sub>max</sub></th>
<th>X<sub>min</sub></th>
<th>Y<sub>max</sub></th>
<th>Y<sub>min</sub></th>
<th>Z<sub>max</sub></th>
<th>Z<sub>min</sub></th></tr></thead>
<tbody align="center">
<tr>
<td>Torso</td>
<td>x<sub>6</sub></td>
<td>x<sub>7</sub></td>
<td>(y<sub>2</sub>+y<sub>3</sub>)/2</td>
<td>y<sub>23</sub></td>
<td>z<sub>max</sub></td>
<td>z<sub>min</sub></td></tr>
<tr>
<td>Left arm</td>
<td>x<sub>left finger tip</sub></td>
<td>x<sub>11</sub></td>
<td>y<sub>7</sub></td>
<td>y<sub>left finger tip</sub></td>
<td>z<sub>max</sub></td>
<td>z<sub>min</sub></td></tr>
<tr>
<td>Right arm</td>
<td>x<sub>10</sub></td>
<td>x<sub>right finger tip</sub></td>
<td>y<sub>6</sub></td>
<td>y<sub>right finger tip</sub></td>
<td>z<sub>max</sub></td>
<td>z<sub>min</sub></td></tr>
<tr>
<td>Left leg</td>
<td>x<sub>7</sub></td>
<td>x<sub>23</sub></td>
<td>y<sub>23</sub></td>
<td>y<sub>left heel</sub></td>
<td>z<sub>max</sub></td>
<td>z<sub>min</sub></td></tr>
<tr>
<td>Right leg</td>
<td>x<sub>23</sub></td>
<td>x<sub>6</sub></td>
<td>y<sub>23</sub></td>
<td>y<sub>right heel</sub></td>
<td>z<sub>max</sub></td>
<td>z<sub>min</sub></td></tr>
</tbody>
</table>
</table-wrap></p>
<p>To facilitate our explanation, we use the right arm as an example, as shown in <xref ref-type="fig" rid="fig7-0040517511418565">Figure 7</xref>. The bounding box for the right arm is built based on three key landmarks, i.e., right finger tip, right acromion and right armpit. Inthe detecting process, the position of the landmark <italic>p</italic> on the template model can be mapped onto the instance model at the position <italic>p</italic>″ by maintaining the same coordinates in the bounding boxes of the template and instance model:
<disp-formula><graphic xlink:href="10.1177_0040517511418565-eq6.tif"/></disp-formula>
<disp-formula id="disp-formula5-0040517511418565"><label>(5)</label><graphic xlink:href="10.1177_0040517511418565-eq5.tif"/></disp-formula>
<disp-formula><graphic xlink:href="10.1177_0040517511418565-eq7.tif"/></disp-formula>
where <italic>X</italic><sub>min_<italic>I</italic></sub>, <italic>X</italic><sub>max_<italic>I</italic></sub>, <italic>Y</italic><sub>min_<italic>I</italic></sub>, <italic>Y</italic><sub>max_<italic>I</italic></sub>, <italic>Z</italic><sub>min_<italic>I</italic></sub> and <italic>Z</italic><sub>max_<italic>I</italic></sub> are the minimum and maximum value of the bounding box for instance model, and <italic>X</italic><sub>min_<italic>I</italic></sub>, <italic>X</italic><sub>max_<italic>I</italic></sub>, <italic>Y</italic><sub>min_<italic>I</italic></sub>, <italic>Y</italic><sub>max_<italic>I</italic></sub>, <italic>Z</italic><sub>min_<italic>I</italic></sub> and <italic>Z</italic><sub>max_<italic>I</italic></sub> are the minimum and maximum value of the bounding box for the template model.
<fig id="fig7-0040517511418565" position="float"><label>Figure 7.</label><caption><p>Using the bounding box of the right arm to detect the landmark on the right wrist.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig7.tif"/></fig></p>
<p>Although <italic>p</italic>″may or may not be a vertex on the instance model, we always use it as the benchmark for similarity comparison. In this sense, the spin image stored for landmark <italic>p</italic> should also be transferred to point <italic>p</italic>″. Now let <italic>p<sub>i</sub></italic> be the candidate vertex on the instance model, the searching is still limited in the region where the Euclidean distance <italic>d</italic> between <italic>p<sub>i</sub></italic> and <italic>p</italic>″ is less than 20 cm, and the angle between oriented vertex <italic>p<sub>i</sub></italic> and <italic>p</italic>″ is less than 90°. By comparing the similarity measure <italic>S</italic>, we can detect all the landmarks that are classified into type B and C in <xref ref-type="table" rid="table2-0040517511418565">Table 2</xref>.</p>
</sec>
</sec>
<sec id="sec10-0040517511418565"><title>Experimental results and discussions</title>
<p>We applied our algorithm to over 200 human scans including both male (117) and female (112) subjects in a wide variety of human body shapes. <xref ref-type="fig" rid="fig8-0040517511418565">Figure 8</xref> and <xref ref-type="fig" rid="fig9-0040517511418565">Figure 9</xref> illustrates the girth values and height value of the subjects measured in terms of the landmarks detected by our proposed method. Obviously, the measures of these body shapes are quite different. This means our proposed method can satisfy a wide variation of body shapes.
<fig id="fig8-0040517511418565" position="float"><label>Figure 8.</label><caption><p>Shape differences among female subjects.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig8.tif"/></fig>
<fig id="fig9-0040517511418565" position="float"><label>Figure 9.</label><caption><p>Shape differences among male subjects.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig9.tif"/></fig></p>
<p>To demonstrate the result of our proposed method, several typical body shapes including obese shape (fromleft to right, the 5th male body in the first tworows of <xref ref-type="fig" rid="fig10-0040517511418565">Figure 10</xref>) are illustrated, as shown in <xref ref-type="fig" rid="fig10-0040517511418565">Figure 10</xref>.
<fig id="fig10-0040517511418565" position="float"><label>Figure 10.</label><caption><p>Examples of our experiment with 47 detected landmarks on each scan. The first model at each row is the template model for landmark detection, in front view and back view respectively.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig10.tif"/></fig></p>
<p>The 29 landmarks are represented as red spheres, and there are 18 extra landmarks represented as blue spheres. For these extra landmarks, four of them are the aforementioned extra key landmarks to define the end of each branch (left fingertip, right fingertip, left heel and right heel). The other 14 extra landmarks are employed for precise body segmentation (<xref ref-type="fig" rid="fig11-0040517511418565">Figure 11</xref>) in automatic body measurement. Technically, it is wise to segment the human body into torso and limbs prior to circumference and/or distance measurement. This makes the automatic body measurement easier and more accurate than taking the whole body as the subject, since each segmented branch is a cylinder-like object. For instance, if we want to measure the waist girth, a cutting plane passing through the most concave position on torso will not take the left and right arm into account. <xref ref-type="fig" rid="fig11-0040517511418565">Figure 11</xref> demonstrates the automatic body segmentation and measurement based on the landmarks detected by our proposed method. The skeleton for the instance model can also be extracted by setting the joints as the center of the concerned girth. The detailed algorithm of body measurement and skeleton extraction can also be found in our previous investigation.<sup><xref ref-type="bibr" rid="bibr2-0040517511418565">2</xref></sup>
<fig id="fig11-0040517511418565" position="float"><label>Figure 11.</label><caption><p>Automatic body segmentation and measurement.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig11.tif"/></fig></p>
<p>In computer animation, the skeleton is usually employed as the benchmark to drive the skin in various motions, usually equipped with the motion captured data. In this type of application, the pre-marked landmarks for a motion captured subject (template) can beeasily transferred onto an instance model. Hence themotion can be transferred accordingly, and can beemployed in 3D virtual dressing system to provide more realistic visual effects in animation such as runwaywalk.</p>
<p>The Euclidean distances between the predicted landmark positions and their corresponding position placed manually according to their anthropometry definitions are calculated to verify the accuracy of our algorithm. <xref ref-type="fig" rid="fig12-0040517511418565">Figure 12</xref> shows the average error for each landmark.
<fig id="fig12-0040517511418565" position="float"><label>Figure 12.</label><caption><p>Average errors of 29 auto-detected landmarks against manually pointed landmarks.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig12.tif"/></fig></p>
<p>Obviously, for all of the 29 landmarks, the maximum error is less than 1.4 cm, which coincides with the human experience when performing the landmarking and measurement manually.<sup><xref ref-type="bibr" rid="bibr21-0040517511418565">21</xref></sup></p>
<p>Aside from anthropometric landmark positioning, our proposed method can be employed in a more flexible manner in determining the customized landmarks. For instance, in 3D bra design or comfort evaluation, it is important to locate the breast area on the 3D human body. <xref ref-type="fig" rid="fig13-0040517511418565">Figure 13</xref> illustrates examples of breast zone detected for different female body shapes. 19 landmarks around the target area were pre-defined on the first template model.
<fig id="fig13-0040517511418565" position="float"><label>Figure 13.</label><caption><p>Results of breast zone detection for female scans based on our algorithm. Left to right: (a) template model with 19 customized landmarks; (b, c, d) automatically detected landmarks on different female scans.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig13.tif"/></fig></p>
<p>To evaluate the performance of our proposed method, we implemented our algorithm on a PC with Core Dual CPUs at 2.8 GHz, and 1 GB Memory. The running time for the detection of all 47 landmarks (including the extra landmarks used for skeleton extraction and body segmentation) on an instance scan is usually under one half minute. The scans we picked have around 7000–8000 points. We also apply our algorithm for detecting scans with different mesh resolution. Results demonstrate that the time spent per-landmark depends on the number of points we searched in the region of interest. It can be controlled by using different searching parameters such as searching radius and permissible variation of normal. Since the dimension of spin images (bin size) is set to approximately the mesh resolution,<sup><xref ref-type="bibr" rid="bibr16-0040517511418565">16</xref></sup> landmarks detection using spin images is sensitive to mesh quality. We find that the higher the density of points, the higher the accuracy of the result and the higher the running time of the algorithm. The running times for the detection of all 47 landmarks on scans with different mesh resolution are shown in <xref ref-type="fig" rid="fig14-0040517511418565">Figure 14</xref>.
<fig id="fig14-0040517511418565" position="float"><label>Figure 14.</label><caption><p>Running times for detection runs on scans with different mesh resolution.</p></caption><graphic xlink:href="10.1177_0040517511418565-fig14.tif"/></fig></p>
<p>Although most of the landmarks can be detected accurately, the landmarks located on non-significant regions may have less accuracy such as the navel (thefourth scan in the first row in <xref ref-type="fig" rid="fig10-0040517511418565">Figure 10</xref>). Such inaccuracy of detection is caused by the fact that those landmarks are not clearly identifiable points with surface feature descriptor and it might be caused by poor surface reconstruction in body scanning. Another observed limitation is that once the pre-marked landmark offsets from its anthropometric position, the detected landmark on the instance model cannot overcome this historic error, which commonly exists in template-based approaches.</p>
</sec>
<sec id="sec11-0040517511418565" sec-type="conclusions"><title>Conclusion</title>
<p>In this paper, we present a method for automatically detecting anthropometric landmarks on 3D scanned models. To describe the local surface feature, spinimages at each oriented point are adopted. Thelandmark detection is performed by comparing the similarity measure defined upon the spin images between known landmarks on the template model and the candidate vertex on the instance model. Normal variation and bounding boxes are employed to speed up the comparison. Experiment results show accurate landmark detection can be achieved in a short time by using our proposed method. In addition, the body segmentation and skeleton extraction as two common applications can be easily carried by using these detected landmarks.</p>
</sec>
</body>
<back>
<sec id="sec12-0040517511418565"><title>Funding</title>
<p>This work was supported by the Natural Science Foundation of China (grant number 60973072) and the Fundamental Research Funds for the Central Universities (grant number 2011D10102).</p>
</sec>
<ref-list>
<title>References</title>
<ref id="bibr1-0040517511418565"><label>1</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Dekker</surname><given-names>L</given-names></name><name><surname>Douros</surname><given-names>I</given-names></name><name><surname>Buxton</surname><given-names>BF</given-names></name><name><surname>Treleaven</surname><given-names>P</given-names></name></person-group>. <article-title>Building symbolic information for 3D human body modeling from range data</article-title>. <source>Second International Conference on 3-D Imaging and Modeling (3DIM '99)</source>, <comment>3dim</comment>, pp.<fpage>0388</fpage>, <year>1999</year>.</citation></ref>
<ref id="bibr2-0040517511418565"><label>2</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>Y</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name></person-group>. <article-title>Automatic segmenting and measurement on scanned human body</article-title>. <source>Int J Cloth Sci Technol</source> <year>2006</year>; <volume>18</volume>(<issue>1</issue>): <fpage>19</fpage>–<lpage>30</lpage>.</citation></ref>
<ref id="bibr3-0040517511418565"><label>3</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Certain</surname><given-names>A</given-names></name><name><surname>Stuetzle</surname><given-names>W</given-names></name></person-group>. <source>Automatic body measurement for mass customization of garments</source>. <article-title>Second International Conference on 3-D Imaging and Modeling (3DIM '99)</article-title>, <comment>3dim</comment>, pp. <fpage>0405</fpage>, <year>1999</year>.</citation></ref>
<ref id="bibr4-0040517511418565"><label>4</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Huang</surname><given-names>Y</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>T</given-names></name></person-group>. <article-title>Body scanning and modeling for custom fit garments</article-title>. <source>J Textile Apparel Technol Management</source> <year>2002</year>; <volume>2</volume>: <fpage>1</fpage>–<lpage>10</lpage>.</citation></ref>
<ref id="bibr5-0040517511418565"><label>5</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Pargas</surname><given-names>RP</given-names></name><name><surname>Staples</surname><given-names>NJ</given-names></name><name><surname>Davis</surname><given-names>JS</given-names></name></person-group>. <article-title>Automatic measurement extraction for apparel from a three-dimensional body scan</article-title>. <source>Optics Lasers Eng</source> <year>1997</year>; <volume>28</volume>: <fpage>157</fpage>–<lpage>172</lpage>.</citation></ref>
<ref id="bibr6-0040517511418565"><label>6</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Leong</surname><given-names>I</given-names></name><name><surname>Fang</surname><given-names>J</given-names></name><name><surname>Tsai</surname><given-names>M</given-names></name></person-group>. <article-title>Automatic body feature extraction from a marker-less scanned human body</article-title>. <source>Computer-Aided Design</source> <year>2007</year>; <volume>39</volume>: <fpage>568</fpage>–<lpage>582</lpage>.</citation></ref>
<ref id="bibr7-0040517511418565"><label>7</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Ju</surname><given-names>X</given-names></name><name><surname>Werghi</surname><given-names>N</given-names></name><name><surname>Siebert</surname><given-names>JP</given-names></name></person-group>. <article-title>Automatic segmentation of 3D human body scans 2000. Computer Graphics and Imaging 2000 CGIM 2000.</article-title>. <fpage>239</fpage>–<lpage>244</lpage>. <comment>Nov 19–23</comment>.</citation></ref>
<ref id="bibr8-0040517511418565"><label>8</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Suikerbuik</surname><given-names>R</given-names></name><name><surname>Tangelder</surname><given-names>H</given-names></name></person-group>. <source>Automatic feature detection in 3D human body scans</source>. <comment>Proceedings of SAE Digital Human Modeling Conference, 04-DHM-52</comment>.</citation></ref>
<ref id="bibr9-0040517511418565"><label>9</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Subburaj</surname><given-names>K</given-names></name><name><surname>Ravi</surname><given-names>B</given-names></name><name><surname>Agarwal</surname><given-names>M</given-names></name></person-group>. <article-title>Automated identification of anatomical landmarks on 3D bone models reconstructed from CT scan images</article-title>. <source>Computerized Medical Imaging Graphics</source> <year>2009</year>; <volume>33</volume>: <fpage>359</fpage>–<lpage>368</lpage>.</citation></ref>
<ref id="bibr10-0040517511418565"><label>10</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Allen</surname><given-names>B</given-names></name><name><surname>Curless</surname><given-names>B</given-names></name><name><surname>Popovic</surname><given-names>Z</given-names></name></person-group>. <article-title>The space of human body shapes: Reconstruction and parameterizations from range scans</article-title>. <source>ACM Trans Graphics</source> <year>2003</year>; <volume>22</volume>(<issue>3</issue>): <fpage>587</fpage>–<lpage>594</lpage>.</citation></ref>
<ref id="bibr11-0040517511418565"><label>11</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Robinette</surname><given-names>KM</given-names></name><name><surname>Daanen</surname><given-names>H</given-names></name><name><surname>Paquet</surname><given-names>E</given-names></name></person-group>. <article-title>CAESAR project: a 3-D surface anthropometry survey. Second International Conference on 3-D Digital Imaging and Modeling (3DIM' 99)</article-title>, pp.<fpage>380</fpage>–<lpage>386</lpage>, <year>1999</year>.</citation></ref>
<ref id="bibr12-0040517511418565"><label>12</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Srinivasan</surname><given-names>P</given-names></name><name><surname>Koller</surname><given-names>D</given-names></name><name><surname>Thrun</surname><given-names>S</given-names></name><name><surname>Pang</surname><given-names>H</given-names></name><name><surname>Daves</surname><given-names>J</given-names></name></person-group>. <article-title>The correlated correspondence algorithm for unsupervised registration of non-rigid surface</article-title>. <source>Adv Neural Inf Proc Systems</source> <year>2005</year>; <volume>17</volume>: <fpage>33</fpage>–<lpage>40</lpage>.</citation></ref>
<ref id="bibr13-0040517511418565"><label>13</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Anguelov</surname><given-names>D</given-names></name><name><surname>Srinivassan</surname><given-names>P</given-names></name><name><surname>Koller</surname><given-names>D</given-names></name><name><surname>Thrun</surname><given-names>S</given-names></name><name><surname>Rodgers</surname><given-names>J</given-names></name><name><surname>Davis</surname><given-names>J</given-names></name></person-group>. <article-title>SCAPE: Shape completion and animation of people</article-title>. <source>ACM Trans Graphics</source> <year>2005</year>; <volume>24</volume>(<issue>3</issue>): <fpage>408</fpage>–<lpage>416</lpage>.</citation></ref>
<ref id="bibr14-0040517511418565"><label>14</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Azouz</surname><given-names>ZB</given-names></name><name><surname>Shu</surname><given-names>C</given-names></name><name><surname>Mantel</surname><given-names>A</given-names></name></person-group>. <article-title>Automatic locating of anthropometric landmarks on 3D human models</article-title> In: <source>Third International Symposium on 3D Data Processing, Visualization, and Transmission (3DPVT’06), 3dpvt</source>, <year>2006</year>, pp. <fpage>750</fpage>–<lpage>757</lpage>.</citation></ref>
<ref id="bibr15-0040517511418565"><label>15</label><citation citation-type="confproc"><person-group person-group-type="author"><name><surname>Wuhrer</surname><given-names>S</given-names></name><name><surname>Azouz</surname><given-names>ZB</given-names></name><name><surname>Shul</surname><given-names>C</given-names></name></person-group>. <article-title>Semi-automatic prediction of landmarks on human models in varying poses</article-title>. In: <conf-name>2010 Canadian Conference on Computer and Robot Vision</conf-name>, <year>2010</year>, pp. <fpage>136</fpage>–<lpage>142</lpage>.</citation></ref>
<ref id="bibr16-0040517511418565"><label>16</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>AE</given-names></name><name><surname>Hebert</surname><given-names>M</given-names></name></person-group>. <article-title>Using spin images for efficient object recognition in cluttered 3D scenes</article-title>. <source>IEEE Trans Pattern Analysis Machine Intelligence</source> <year>1999</year>; <volume>21</volume>(<issue>5</issue>): <fpage>433</fpage>–<lpage>449</lpage>.</citation></ref>
<ref id="bibr17-0040517511418565"><label>17</label><citation citation-type="confproc"><person-group person-group-type="author"><name><surname>Starck</surname><given-names>J</given-names></name><name><surname>Hilton</surname><given-names>A</given-names></name></person-group>. <article-title>Spherical matching for temporalcorrespondence of non-rigid surfaces</article-title>. In: <conf-name>International Conference on Computer Vision – ICCV</conf-name>, <year>2005</year>, pp. <fpage>1387</fpage>–<lpage>1394</lpage>.</citation></ref>
<ref id="bibr18-0040517511418565"><label>18</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>D</given-names></name><name><surname>Sharf</surname><given-names>A</given-names></name><name><surname>Amenta</surname><given-names>N</given-names></name></person-group>. <article-title>Feature-driven deformation for dense correspondence. Medical Imaging 2009:Visualization Image-Guided Procedures and Modeling</article-title>, <year>2009</year>.</citation></ref>
<ref id="bibr19-0040517511418565"><label>19</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>AE</given-names></name><name><surname>Hebert</surname><given-names>M</given-names></name></person-group>. <article-title>Surface matching for object recognition in complex three-dimensional scene</article-title>. <source>Image Vision Computing</source> <year>1998</year>; <volume>16</volume>: <fpage>635</fpage>–<lpage>651</lpage>.</citation></ref>
<ref id="bibr20-0040517511418565"><label>20</label><citation citation-type="journal"><person-group person-group-type="author"><name><surname>Simmons</surname><given-names>KP</given-names></name><name><surname>Istook</surname><given-names>CL</given-names></name></person-group>. <article-title>Body measurement techniques</article-title>. <source>J Fashion Market Manage</source> <year>2003</year>; <volume>7</volume>(<issue>3</issue>): <fpage>306</fpage>–<lpage>332</lpage>.</citation></ref>
<ref id="bibr21-0040517511418565"><label>21</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>CC</given-names></name><name><surname>Bradtmiller</surname><given-names>B</given-names></name><name><surname>Clausen</surname><given-names>CE</given-names></name><name><surname>Churchill</surname><given-names>T</given-names></name><name><surname>McConville</surname><given-names>JT</given-names></name><name><surname>Tebbets</surname><given-names>IO</given-names></name><etal/></person-group>. <source>Anthropometric survey of US Army personnel. Methods &amp; summary statistics. Technical Report TR-89-044</source>. <publisher-loc>Natick, MA</publisher-loc>: <publisher-name>US Army Natick Research Development and Engineering Center</publisher-name>, <year>1989</year>.</citation></ref>
</ref-list>
</back>
</article>