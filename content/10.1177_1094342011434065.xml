<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342011434065</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342011434065</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>OpenMP task scheduling strategies for multicore NUMA systems</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Olivier</surname>
<given-names>Stephen L</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342011434065">1</xref>
<xref ref-type="corresp" rid="corresp1-1094342011434065"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Porterfield</surname>
<given-names>Allan K</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342011434065">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Wheeler</surname>
<given-names>Kyle B</given-names>
</name>
<xref ref-type="aff" rid="aff3-1094342011434065">3</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Spiegel</surname>
<given-names>Michael</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342011434065">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Prins</surname>
<given-names>Jan F</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342011434065">1</xref>
</contrib>
</contrib-group>
<aff id="aff1-1094342011434065">
<label>1</label>Department of Computer Science, University of North Carolina at Chapel Hill, USA</aff>
<aff id="aff2-1094342011434065">
<label>2</label>Renaissance Computing Institute (RENCI), USA</aff>
<aff id="aff3-1094342011434065">
<label>3</label>Department 1423: Scalable System Software, Sandia National Laboratories, USA</aff>
<author-notes>
<corresp id="corresp1-1094342011434065">Corresponding author: Stephen L Olivier, University of North Carolina at Chapel Hill, Campus Box 3175, Chapel Hill, NC 27599, USA. Email: <email>olivier@cs.unc.edu</email>
</corresp>
<fn fn-type="other" id="fn1-1094342011434065">
<p>
<italic>Stephen L Olivier</italic> is a PhD candidate in the Department of Computer Science at the University of North Carolina at Chapel Hill. His general research interests include multicore technologies, programming languages, and performance analysis for high performance computing. His dissertation research focuses on efficient run time scheduling techniques for task-parallel computation, using the Qthreads library as a vehicle for their implementation. Stephen holds the UNC Computer Science Alumni Fellowship and was previously a National Defense Science and Engineering (NDSEG) Fellow. He is also a contributor to the OpenMP Language Committee.</p>
</fn>
<fn fn-type="other" id="fn2-1094342011434065">
<p>
<italic>Allan K Porterfield</italic> has been a HPC Senior Scientist at the Renaissance Computing Institute (RENCI) in Chapel Hill, North Carolina since 2006. Current projects include MAESTRO/Qthreads, a lightweight threading run-time to support dynamic programming models on a wide variety of modern processors. Previously, he spent 17 years at Tera/Cray Inc. working in the compiler group for the MTA/XMT architecture. He was primarily responsible for the instruction simulator and the linking tools, but other duties spanned the entire suite of compiler tools. Dr Porterfield received his PhD from Rice University in 1989 under Dr Ken Kennedy.</p>
</fn>
<fn fn-type="other" id="fn3-1094342011434065">
<p>
<italic>Kyle B Wheeler</italic> received his PhD from the Department of Computer Science and Engineering at the University of Notre Dame in 2009. Prior to that, he received an MS in Computer Science from the University of Notre Dame in 2005, and a BS in Computer Science from Ohio University. He is a Senior Member of Technical Staff at Sandia National Laboratories, and has been working in the scalable system software field for almost a decade, and focused on lightweight threading environments for the last six years. His current research focuses on multi-node task scheduling, particularly applying adaptive scheduling techniques to that scheduling regime, and additionally developing novel task-based collective synchronization designs using fine-grained synchronization primitives. Kyle is the primary author of the Qthreads tasking library, and is the author of the shared-memory implementation of the Portals4 communication interface.</p>
</fn>
<fn fn-type="other" id="fn4-1094342011434065">
<p>
<italic>Michael Spiegel</italic> is a postdoctoral research associate at the Renaissance Computing Institute (RENCI) in Chapel Hill, North Carolina. He successfully defended his dissertation on “Cache-Conscious Concurrent Data Structures” in April 2011 from the Department of Computer Science at the University of Virginia. His current research activities focus on large-scale bioinformatics algorithms and the design and implementation of efficient memory-aware parallel run-time systems. Michael is one of the principal contributors to the OpenMx project, an open source R library for extensible structural equation modeling. He is interested in embedding concurrency and parallelism into the core undergraduate computer science curriculum.</p>
</fn>
<fn fn-type="other" id="fn5-1094342011434065">
<p>
<italic>Jan F Prins</italic> is a Professor in the Department of Computer Science at the University of North Carolina at Chapel Hill. He obtained his PhD in 1987 in Computer Science from Cornell University. His research interests center on parallel computing, including algorithm design, computer architecture, and programming languages. He collaborates widely on applications of parallel computing in bioinformatics and computational biology, and in the physical sciences and engineering. He was a visiting professor at the Institute for Theoretical Computer Science at ETH Zurich, in the area of scientific computing. His research has been sponsored by AFOSR, ARO, DARPA, DOE, EPA, NIH, NSA, NSF, ONR and by industry, including IBM, Microsoft, and HPC companies.</p>
</fn>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>26</volume>
<issue>2</issue>
<issue-title>Issues in Large Scale Computing Environments: Heterogeneous Computing and Operating Systems - two subjects, one special issue</issue-title>
<fpage>110</fpage>
<lpage>124</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The recent addition of task parallelism to the OpenMP shared memory API allows programmers to express concurrency at a high level of abstraction and places the burden of scheduling parallel execution on the OpenMP run-time system. Efficient scheduling of tasks on modern multi-socket multicore shared memory systems requires careful consideration of an increasingly complex memory hierarchy, including shared caches and non-uniform memory access (NUMA) characteristics. In order to evaluate scheduling strategies, we extended the open source Qthreads threading library to implement different scheduler designs, accepting OpenMP programs through the ROSE compiler. Our comprehensive performance study of diverse OpenMP task-parallel benchmarks compares seven different task-parallel run-time scheduler implementations on an Intel Nehalem multi-socket multicore system: our proposed hierarchical work-stealing scheduler, a per-core work-stealing scheduler, a centralized scheduler, and LIFO and FIFO versions of the Qthreads round-robin scheduler. In addition, we compare our results against the Intel and GNU OpenMP implementations.</p>
<p>Our hierarchical scheduling strategy leverages different scheduling methods at different levels of the hierarchy. By allowing one thread to steal work on behalf of all of the threads within a single chip that share a cache, the scheduler limits the number of costly remote steals. For cores on the same chip, a shared LIFO queue allows exploitation of cache locality between sibling tasks as well as between a parent task and its newly created child tasks. In the performance evaluation, our Qthreads hierarchical scheduler is competitive on all benchmarks tested. On five of the seven benchmarks, it demonstrates speedup and absolute performance superior to both the Intel and GNU OpenMP run-time systems. Our run-time also demonstrates similar performance benefits on AMD Magny Cours and SGI Altix systems, enabling several benchmarks to successfully scale to 192 CPUs of an SGI Altix.</p>
</abstract>
<kwd-group>
<kwd>Task parallelism</kwd>
<kwd>run-time systems</kwd>
<kwd>work stealing</kwd>
<kwd>scheduling</kwd>
<kwd>multicore</kwd>
<kwd>OpenMP</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342011434065">
<title>1 Introduction</title>
<p>Task-parallel programming models offer a simple way for application programmers to specify parallel tasks in a problem-centric form that easily scales with problem size, leaving the scheduling of these tasks onto processors to be performed at run time. Task parallelism is well suited to the expression of nested parallelism in recursive divide-and-conquer algorithms and of unstructured parallelism in irregular computations.</p>
<p>An efficient task scheduler must meet challenging and sometimes conflicting goals: exploit cache and memory locality, maintain load balance, and minimize overhead costs. When there is an inequitable distribution of work among processors, load imbalance arises. Without redistribution of work, some processors become idle. Load balancing operations, when successful, redistribute the work more equitably across processors. However, load balancing operations can also contribute to overhead costs. Load balancing operations between sockets increase memory access time due to more cold cache misses and more high-latency remote memory accesses. This paper proposes an approach to mitigate these issues and advances understanding of their impact through the following contributions:<list list-type="order">
<list-item>
<p>
<bold>A hierarchical scheduling strategy targeting modern multi-socket multicore shared memory systems</bold> whose NUMA architecture is not well supported by either work-stealing schedulers with one queue per core or by centralized schedulers. Our approach combines work stealing and shared queues for low-overhead load balancing and exploitation of shared caches.</p>
</list-item>
<list-item>
<p>
<bold>A detailed performance study on a current generation multi-socket multicore Intel system.</bold> Seven run-time implementations supporting task-parallel OpenMP programs are compared: five schedulers that we added to the open source Qthreads library, the GNU GCC OpenMP run-time, and the Intel OpenMP run-time. In addition to speedup results demonstrating superior performance by our run-time on many of the diverse benchmarks tested, we examine several secondary metrics that illustrate the benefits of hierarchical scheduling over work-stealing schedulers with one queue per core.</p>
</list-item>
<list-item>
<p>
<bold>Additional performance evaluations on a two-socket multicore AMD system and a 192-processor SGI Altix.</bold> These evaluations demonstrate the performance portability and scalability of our run-time implementations.</p>
</list-item>
</list>
</p>
<p>This paper extends work originally presented in <xref ref-type="bibr" rid="bibr26-1094342011434065">Olivier et al. (2011)</xref>. The remainder of the paper is organized as follows: Section 2 provides relevant background information, Section 3 describes existing task scheduler designs and our hierarchical approach, Section 4 presents the results of our experimental evaluation, and Section 5 discusses related work. We conclude in Section 6 with some final observations.</p>
</sec>
<sec id="section2-1094342011434065">
<title>2 Background</title>
<p>Broadly supported by both commercial and open source compilers, OpenMP allows incremental parallelization of serial programs for execution on shared memory parallel computers. Version 3.0 of the OpenMP specification for FORTRAN and C/C++ adds explicit task parallelism to complement its existing data parallel constructs (<xref ref-type="bibr" rid="bibr27-1094342011434065">OpenMP Architecture Review Board, 2008</xref>; <xref ref-type="bibr" rid="bibr3-1094342011434065">Ayguadé et al., 2009</xref>). The OpenMP <bold>task</bold> construct generates a task from a statement or structured block. Task synchronization is provided by the <bold>taskwait</bold> construct, and the semantics of the OpenMP <bold>barrier</bold> construct have also been overloaded to require completion of all outstanding tasks.</p>
<p>Execution of OpenMP programs combines the efforts of the compiler and an OpenMP run-time library. Intel and GCC both have integrated OpenMP compiler and run-time implementations. Using the ROSE compiler (<xref ref-type="bibr" rid="bibr24-1094342011434065">Liao et al., 2010</xref>), we have created an equivalent method to compile and run OpenMP programs with the Qthreads (<xref ref-type="bibr" rid="bibr33-1094342011434065">Wheeler et al., 2008</xref>) library. The ROSE compiler is a source-to-source translator that supports OpenMP 3.0 with a simple compiler flag. In one compile step, it produces an intermediate C++ file and invokes the GNU C++ compiler to compile that file with additional libraries to produce an executable. ROSE performs syntactic and semantic analysis on OpenMP directives, transforming them into run-time library calls in the intermediate program. The ROSE common OpenMP run-time library (XOMP) maps the run-time calls to functions in the Qthreads library.</p>
<sec id="section3-1094342011434065">
<title>2.1 Qthreads</title>
<p>Qthreads (<xref ref-type="bibr" rid="bibr33-1094342011434065">Wheeler et al., 2008</xref>) is a cross-platform general-purpose parallel run-time library designed to support lightweight threading and synchronization in a flexible integrated locality framework. Qthreads directly supports programming with lightweight threads and a variety of synchronization methods, including non-blocking atomic operations and potentially blocking full/empty bit (FEB) operations like those developed for the HEP machine (<xref ref-type="bibr" rid="bibr29-1094342011434065">Smith, 1981</xref>). The Qthreads lightweight threading concept and its implementation are intended to match future hardware environments by providing efficient software support for massive multithreading.</p>
<p>In the Qthreads execution model, lightweight threads (qthreads) are created in user-space with a small context and small fixed-size stack. Unlike heavyweight threads such as pthreads, qthreads do not support expensive features like per-thread identifiers, per-thread signal vectors, or preemptive multitasking. Qthreads are scheduled onto a small set of worker pthreads. Logically, a qthread is the smallest schedulable unit of work, such as a set of loop iterations or an OpenMP task, and execution of a program generates many more qthreads than it has worker pthreads. Each worker pthread is pinned to a processor core and assigned to a locality domain, termed a <bold>shepherd</bold>. Whereas Qthreads previously allowed only one worker pthread per shepherd, we added support for multiple worker pthreads per shepherd. This support enables us to map shepherds to different architectural components, e.g. one shepherd per core, one shepherd per shared L3 cache, or one shepherd per processor socket.</p>
<p>The default scheduler in the Qthreads run-time uses a cooperative multitasking approach. When qthreads block, e.g. performing an FEB operation, a context switch is triggered. Because this context switch is done in user-space via function calls and requires neither signals nor saving a full set of registers, it is less expensive than an operating system or interrupt-based context switch. This technique allows qthreads to execute uninterrupted until data is needed that is not yet available, and allows the scheduler to attempt to hide communication latency by switching to other qthreads. Obviously, this only hides communication latencies that take longer than a context switch.</p>
<p>The Qthreads API includes several threaded loop interfaces, built on top of the core threading components. The API provides three basic parallel loop behaviors: one to create a separate qthread for each iteration, one that divides the iteration space evenly among all shepherds, and one that uses a queue-like structure to distribute sub-ranges of the iteration space to enable self-scheduled loops. We used the Qthreads queueing implementation as a starting point for our scheduling work.</p>
<p>We added support for the ROSE XOMP calls to Qthreads allowing it to be used as the run-time for OpenMP programs. Although Qthreads XOMP/OpenMP support is not fully complete, it accepts every OpenMP program accepted by ROSE. We implement OpenMP threads as worker pthreads. Unlike many OpenMP implementations, default loop scheduling is self-guided rather than static, though the latter can be explicitly requested. For task parallelism, we implement each OpenMP task as a qthread. (We use the term <bold>task</bold> rather than qthread throughout the remainder of the paper, both for simplicity and because the scheduling concepts we explore are applicable to other task-parallel languages and libraries.) We used the Qthreads FEB synchronization mechanism as a base layer upon which to implement <bold>taskwait</bold> and <bold>barrier</bold> sychronization.</p>
</sec>
</sec>
<sec id="section4-1094342011434065">
<title>3 Task scheduler design</title>
<p>The stock Qthreads scheduler, called <bold>
<italic>Q</italic>
</bold> in Section 4, was engineered for parallel loop computation. Each processor executes chunks of loop iterations packaged as qthreads. Round-robin distribution of the iterations among the shepherds and self-scheduling are used in combination to maintain load balance. A simple lock-free per-shepherd FIFO queue stores iterations as they wait to be executed.</p>
<p>Task-parallel programs generate a dynamically unfolding sequence of interdependent tasks, often represented by a directed acyclic graph (DAG). A task executing on the same thread as its parent or sibling tasks may benefit from temporal locality if they operate on the same data. In particular, such locality properties are a feature of divide-and-conquer algorithms. To efficiently schedule tasks as lightweight threads in Qthreads, the run-time must support more general dynamic load balancing while exploiting available locality among tasks. We implemented a modified Qthreads scheduler, <italic>L</italic>, to use LIFO rather than FIFO queues at each shepherd to improve the use of locality. However, the round-robin distribution of tasks between shepherds does not provide fully dynamic load balancing.</p>
<sec id="section5-1094342011434065">
<title>3.1 Work-stealing and centralized schedulers</title>
<p>To better meet the dual goals of locality and load balance, we implemented work stealing. Blumofe et al. proved that work stealing is optimal for multithreaded scheduling of DAGs with minimal overhead costs (<xref ref-type="bibr" rid="bibr7-1094342011434065">Blumofe and Leiserson 1994</xref>), and they implemented it in their Cilk run-time scheduler (<xref ref-type="bibr" rid="bibr6-1094342011434065">Blumofe et al., 1995</xref>). Our initial implementation of work stealing in Qthreads, <italic>WS</italic>, mimics Cilk’s scheduling discipline: each shepherd schedules tasks depth-first locally through LIFO queue operations. An idle shepherd obtains more work by stealing the oldest tasks from the task queue of a busy shepherd. We implemented two different probing schemes to find a victim shepherd, observing equivalent performance: choosing randomly and commencing search at the nearest shepherd ID to the thief. In the work-stealing scheduler, interruptions to busy shepherds are minimized because the burden of load balancing is placed on the idle shepherds. Locality is preserved because newer tasks, whose data is still hot in the processor’s cache, are the first to be scheduled locally and the last in line to be stolen.</p>
<p>The cost of work-stealing operations on multi-socket multicore systems varies significantly based on the relative locations of the thief and victim, e.g. whether they are running on cores on the same chip or on different chips. Stealing between cores on different chips reduces performance by incurring higher overhead costs, additional cold cache misses, remote locking, remote memory access costs, and coherence misses due to false sharing. Another limitation of work stealing is that it does not make the best possible use of caches shared among cores. In contrast, <xref ref-type="bibr" rid="bibr12-1094342011434065">Chen et al. (2007)</xref> showed that a depth-first schedule close to serial order makes better use of a shared cache than work stealing, assuming serial execution of an application makes good use of the cache. Blelloch et al. had shown that such a schedule can be achieved using a shared LIFO queue (<xref ref-type="bibr" rid="bibr5-1094342011434065">Blelloch et al., 1999</xref>). We implemented a centralized shared LIFO queue, <italic>CQ</italic>, for Qthreads, but it is a poor match for multi-socket multicore systems since not all cores, but only cores on the same chip, share the same cache. Moreover, the centralized queue implementation is not scalable, as contention drives up the overhead costs.</p>
</sec>
<sec id="section6-1094342011434065">
<title>3.2 Hierarchical scheduling</title>
<p>To overcome the limitations of both work stealing and shared queues, we developed a hierarchical approach: multithreaded shepherds, <italic>MTS</italic>. We create one shepherd for all the cores on the same chip. These cores share a cache, typically L3, and all are proximal to a local memory attached to that socket. Within each shepherd, we map one pthread worker to each core. Among workers in each shepherd, a shared LIFO queue provides depth-first scheduling close to serial order to exploit the shared cache. Thus, load balancing happens naturally among the workers on a chip and concurrent tasks have possible overlapping localities that can be captured in the shared cache.</p>
<p>Between shepherds, work stealing is used to maintain load balance. Each time the shepherd’s task queue becomes empty, only the first worker to find the queue empty sets a flag and commences stealing. The other workers in the shepherd spin on cached copies of the flag until the steal is complete and the stealing thread resets the flag. The thief thread steals enough tasks from another shepherd’s queue to supply the workers in its shepherd with work. The number of tasks stolen per steal is a tunable parameter, but stealing one per worker in the shepherd ensures that at least immediately following the steal all threads have a task to execute. In practice, we have observed this heuristic to be effective, and Section 4.4 shows how performance varies for different choices of this parameter. If fewer tasks are available then the thief steals all the available tasks on the victim’s queue. The stolen tasks are dequeued and collected into a small linked list, then enqueued at the thief’s queue. If the steal attempt fails because no tasks are available, then the thief thread selects a new victim and begins another steal attempt.</p>
<p>Centralized task queueing for workers within each shepherd reduces the need for remote stealing by providing local load balance. By allowing only one representative worker to steal at time, in bulk for all workers in the shepherd, communication overheads are reduced. While a shared queue can be a performance bottleneck, the number of cores per chip is bounded, and intra-chip locking operations are fast.</p>
</sec>
</sec>
<sec id="section7-1094342011434065">
<title>4 Evaluation</title>
<p>To evaluate the performance of our hierarchical scheduler and the other Qthreads schedulers, we present results from the Barcelona OpenMP Tasks Suite (BOTS), version 1.1, available online (<xref ref-type="bibr" rid="bibr15-1094342011434065">Duran and Teruel, 2010</xref>). The suite comprises a set of task-parallel applications from various domains with varying computational characteristics (<xref ref-type="bibr" rid="bibr16-1094342011434065">Duran et al., 2009</xref>). Our experiments used the following benchmark components and inputs:<list list-type="bullet">
<list-item>
<p>
<italic>Alignment</italic>: Aligns sequences of proteins using dynamic programming (100 sequences);</p>
</list-item>
<list-item>
<p>
<italic>Fib</italic>: Computes the <italic>n</italic>th Fibonacci number using brute-force recursion (<inline-formula id="inline-formula1-1094342011434065">
<mml:math id="mml-inline1-1094342011434065">
<mml:mi>n</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>50</mml:mn>
</mml:math>
</inline-formula>);</p>
</list-item>
<list-item>
<p>
<italic>Health</italic>: Simulates a national health care system over a series of timesteps (144 cities);</p>
</list-item>
<list-item>
<p>
<italic>NQueens</italic>: Finds solutions of the <italic>n</italic>-queens problem using backtrack search (<inline-formula id="inline-formula2-1094342011434065">
<mml:math id="mml-inline2-1094342011434065">
<mml:mi>n</mml:mi>
<mml:mo stretchy="false">=</mml:mo>
<mml:mn>14</mml:mn>
</mml:math>
</inline-formula>);</p>
</list-item>
<list-item>
<p>
<italic>Sort</italic>: Sorts a vector using parallel mergesort with sequential quicksort and insertion sort (128M integers);</p>
</list-item>
<list-item>
<p>
<italic>SparseLU</italic>: Computes the LU factorization of a sparse matrix (<inline-formula id="inline-formula3-1094342011434065">
<mml:math id="mml-inline3-1094342011434065">
<mml:mn>10000</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:mn>10000</mml:mn>
</mml:math>
</inline-formula> matrix, <inline-formula id="inline-formula4-1094342011434065">
<mml:math id="mml-inline4-1094342011434065">
<mml:mn>100</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:mn>100</mml:mn>
</mml:math>
</inline-formula> submatrix blocks);</p>
</list-item>
<list-item>
<p>
<italic>Strassen</italic>: Computes a dense matrix multiply using Strassen’s method (<inline-formula id="inline-formula5-1094342011434065">
<mml:math id="mml-inline5-1094342011434065">
<mml:mn>8192</mml:mn>
<mml:mo stretchy="false">×</mml:mo>
<mml:mn>8192</mml:mn>
</mml:math>
</inline-formula> matrix).</p>
</list-item>
</list>
</p>
<p>For the <italic>Fib</italic>, <italic>Health</italic>, and <italic>NQueens</italic> benchmarks, the default manual cut-off configurations provided in BOTS are enabled to prune the generation of tasks below a prescribed point in the task hierarchy. For <italic>Sort</italic>, cut-offs are set to transition at 32K integers from parallel mergesort to sequential quicksort and from parallel merge tasks to sequential merge calls. For <italic>Strassen</italic>, the cut-off giving the best performance for each implementation is used. Other BOTS benchmarks are not presented here: <italic>UTS</italic> and <italic>FFT</italic> use very fine-grained tasks without cut-offs, yielding poor performance on all run times, and <italic>floorplan</italic> raises compilation issues in ROSE.</p>
<p>For both the <italic>Alignment</italic> and <italic>SparseLU</italic> benchmarks, BOTS provides two different source files. Simplified code given in <xref ref-type="fig" rid="fig1-1094342011434065">Figure 1</xref>
 illustrates the distinction between the two versions of <italic>Alignment</italic>. In the first (<italic>Alignment</italic>-<bold>single</bold>) the loop nest that generates the tasks is executed sequentially by a single thread. This version creates only task parallelism. In the second (<italic>Alignment</italic>-<bold>for</bold>) the outer loop is executed in parallel, creating both loop-level parallelism and task parallelism. Likewise, the two versions of <italic>SparseLU</italic> are one in which tasks are generated within single-threaded loop executions and another in which tasks are generated within parallel loop executions.</p>
<fig id="fig1-1094342011434065" position="float">
<label>Figure 1.</label>
<caption>
<p>Simplified code for the two versions of <italic>Alignment</italic>: <bold>single</bold> (left) and for (right).</p>
</caption>
<graphic alternate-form-of="fig1-1094342011434065" xlink:href="10.1177_1094342011434065-fig1.tif"/>
</fig>
<p>We ran the battery of tests on seven scheduler implementations: five versions of Qthreads (all compiled with GCC 4.4.4 -O2), the GNU GCC OpenMP implementation (<xref ref-type="bibr" rid="bibr17-1094342011434065">Free Software Foundation Inc., 2010</xref>), and the Intel ICC OpenMP implementation, as summarized in <xref ref-type="table" rid="table1-1094342011434065">Table 1</xref>
. The Qthreads implementations are as follows:<list list-type="bullet">
<list-item>
<p>
<italic>Q</italic> is the original version of Qthreads and defines each core to be a separate locality domain or shepherd. It uses a non-blocking FIFO queue to schedule tasks within each shepherd (individual core). Each shepherd only obtains tasks from its local queue, although tasks are distributed across shepherds on a round-robin basis for load balance.</p>
</list-item>
<list-item>
<p>
<italic>L</italic> incorporates a simple double-ended locking LIFO queue to replace the original non-blocking FIFO queue. Concurrent access at both ends is required for work stealing, though <italic>L</italic> retains round-robin task distribution for load balance rather than work stealing.</p>
</list-item>
<list-item>
<p>
<italic>CQ</italic> uses a single shepherd and centralized shared queue to distribute tasks among all of the cores in the system. This should provide adequate load balance, but contention for the queue limits scalability as task size shrinks.</p>
</list-item>
<list-item>
<p>
<italic>WS</italic> provides a shepherd (and individual queue) for each core, and idle shepherds steal tasks from the shepherds running on the other cores. Initial task placement is not round-robin between queues, but onto the local queue of the shepherd where it is generated, exploiting locality among related tasks.</p>
</list-item>
<list-item>
<p>
<italic>MTS</italic> assigns one shepherd to every processor memory locality (shared L3 cache on chip and attached DIMMs). Each core on a chip hosts a worker thread that shares its shepherd’s queue. Only one core is allowed to actively steal tasks on behalf of the queue at a time and tasks are stolen in chunks large enough (tunable) to keep all of the cores busy.</p>
</list-item>
</list>
</p>
<table-wrap id="table1-1094342011434065" position="float">
<label>Table 1.</label>
<caption>
<p>Scheduler implementations evaluated: five Qthreads implementations, ICC, and GCC.</p>
</caption>
<graphic alternate-form-of="table1-1094342011434065" xlink:href="10.1177_1094342011434065-table1.tif"/>
<table>
<thead>
<tr>
<th align="center" colspan="6">Qthreads Implementations, compiled Rose/GCC -O2 –g</th>
</tr>
<tr>
<th>Version Name</th>
<th>Scheduler Implementation</th>
<th>Number of Shepherds</th>
<th>Task Placement</th>
<th>Internal Queue Access</th>
<th>External Queue Access</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<italic>Q</italic>
</td>
<td>Stock</td>
<td>one per core</td>
<td>round-robin</td>
<td>FIFO (non-blocking)</td>
<td>none</td>
</tr>
<tr>
<td>
<italic>L</italic>
</td>
<td>LIFO</td>
<td>one per core</td>
<td>round-robin</td>
<td>LIFO (blocking)</td>
<td>none</td>
</tr>
<tr>
<td>
<italic>CQ</italic>
</td>
<td>Centralized Queue</td>
<td>one</td>
<td>N/A</td>
<td>LIFO (blocking)</td>
<td>N/A</td>
</tr>
<tr>
<td>
<italic>WS</italic>
</td>
<td>Work-Stealing</td>
<td>one per core</td>
<td>local</td>
<td>LIFO (blocking)</td>
<td>FIFO stealing</td>
</tr>
<tr>
<td>
<italic>MTS</italic>
</td>
<td>MultiThreaded Shepherds</td>
<td>one per chip</td>
<td>local</td>
<td>LIFO (blocking)</td>
<td>FIFO stealing</td>
</tr>
<tr>
<td>ICC</td>
<td align="center" colspan="5">Intel 11.1 OpenMP, compiled -O2 -xHost -ipo -g</td>
</tr>
<tr>
<td>GCC</td>
<td align="center" colspan="5">GCC 4.4.4 OpenMP, compiled -O2 –g</td>
</tr>
</tbody>
</table>
</table-wrap>
<sec id="section8-1094342011434065">
<title>4.1 Overall performance on Intel Nehalem</title>
<p>The first hardware test system for our experiments is a Dell PowerEdge M910 quad-socket blade with four Intel x7550 2.0 GHz 8-core Nehalem-EX processors installed for a total of 32 cores. The processors are fully connected using Intel QuickPath Interconnect (QPI) links, as shown in <xref ref-type="fig" rid="fig2-1094342011434065">Figure 2</xref>
. Each processor has an 18 MB shared L3 cache and each core has a private 256 KB L2 cache as well as 32 KB L1 data and instruction caches. The blade has 64 dual-rank 2 GB DDR3 memory sticks (16 per processor chip) for a total of 132 GB. It runs CentOS Linux with a 2.6.35 kernel. Although the x7550 processor supports HyperThreading (Intel’s simultaneous multithreading technology), we pinned only one thread to each physical core for our experiments.</p>
<fig id="fig2-1094342011434065" position="float">
<label>Figure 2.</label>
<caption>
<p>Topology of the four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig2-1094342011434065" xlink:href="10.1177_1094342011434065-fig2.tif"/>
</fig>
<p>All executables using the Qthreads and GCC run-times were compiled with GCC 4.4.4 with -g and -O2 optimization, for consistency. Executables using the Intel run-time were compiled with ICC 11.1 and -O2 -xHost -ipo optimization. Reported results are from the best of 10 runs.</p>
<p>Overall, the GCC and ICC compilers produce executables with similar serial performance, as shown in <xref ref-type="table" rid="table2-1094342011434065">Table 2</xref>
. These serial execution times provide a basis for us to compare the relative speedup of the various benchmarks. If the -ipo and -xHost flags are not used with ICC on <italic>SparseLU</italic>, the GCC serial executable runs 3<inline-formula id="inline-formula6-1094342011434065">
<mml:math id="mml-inline6-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> faster than the ICC executable compiled with -O2 alone. The significance of this difference will be clearer in the presentation of parallel performance on <italic>SparseLU</italic> in Section 4.2. Several other benchmarks also run slower with those ICC flags omitted, though not by such a large margin.</p>
<table-wrap id="table2-1094342011434065" position="float">
<label>Table 2.</label>
<caption>
<p>Sequential and parallel execution times using ICC, GCC, and the Qthreads <italic>MTS</italic> scheduler (time in seconds). For Alignment and SparseLU, the best time between the two parallel variations (single and for) is shown.</p>
</caption>
<graphic alternate-form-of="table2-1094342011434065" xlink:href="10.1177_1094342011434065-table2.tif"/>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>
<italic>Alignment</italic>
</th>
<th>
<italic>Fib</italic>
</th>
<th>
<italic>Health</italic>
</th>
<th>
<italic>NQueens</italic>
</th>
<th>
<italic>Sort</italic>
</th>
<th>
<italic>SparseLU</italic>
</th>
<th>
<italic>Strassen</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>ICC -O2 -xHost -ipo Serial</td>
<td>28.33</td>
<td>100.4</td>
<td>15.07</td>
<td>49.35</td>
<td>20.14</td>
<td>117.3</td>
<td>169.3</td>
</tr>
<tr>
<td>GCC -O2 Serial</td>
<td>28.06</td>
<td>83.46</td>
<td>15.31</td>
<td>45.24</td>
<td>19.83</td>
<td>119.7</td>
<td>162.7</td>
</tr>
<tr>
<td>ICC 32 threads</td>
<td>0.9110</td>
<td>4.036</td>
<td>1.670</td>
<td>1.793</td>
<td>1.230</td>
<td>7.901</td>
<td>10.13</td>
</tr>
<tr>
<td>GCC 32 threads</td>
<td>0.9973</td>
<td>5.283</td>
<td>7.460</td>
<td>1.766</td>
<td>1.204</td>
<td>4.517</td>
<td>10.13</td>
</tr>
<tr>
<td>Qthreads <italic>MTS</italic> 32 workers</td>
<td>1.024</td>
<td>3.189</td>
<td>1.122</td>
<td>1.591</td>
<td>1.080</td>
<td>4.530</td>
<td>10.72</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Qthreads <italic>MTS</italic> 32 core performance is faster than or comparable to the performance of ICC and GCC. In absolute execution time, <italic>MTS</italic> runs faster than ICC for five of the seven benchmarks by up to 74.4%. It is over 6.6<inline-formula id="inline-formula7-1094342011434065">
<mml:math id="mml-inline7-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> faster for one benchmark than GCC and up to 65.6% faster on four of the six others. On two benchmarks <italic>MTS</italic> runs slower: for <italic>Alignment</italic> it is 12.4% slower than ICC and 2.7% slower than GCC, and for <italic>Strassen</italic> it is 5.8% slower than both (although <italic>WS</italic> equaled GCC’s performance—see the discussion on <italic>Strassen</italic> in Section 4.2). Thus, even as a research prototype, ROSE/Qthreads provides competitive OpenMP task execution.</p>
</sec>
<sec id="section9-1094342011434065">
<title>4.2 Individual performance on Intel Nehalem</title>
<p>Individual benchmark performance on multiple implementations of the OpenMP run-time demonstrates features of particular applications where Qthreads generates better scheduling and where it needs further development. Examining where the run-times differ in achieved speedup reveals the strengths and weaknesses of each scheduling approach.</p>
<p>The <italic>Health</italic> benchmark, <xref ref-type="fig" rid="fig3-1094342011434065">Figure 3</xref>
, shows significant diversity in performance and speedup. GNU performance is slightly superlinear for four cores (4.5<inline-formula id="inline-formula8-1094342011434065">
<mml:math id="mml-inline8-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>), but peaks with only 8 cores active (6.3<inline-formula id="inline-formula9-1094342011434065">
<mml:math id="mml-inline9-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>) and by 32 cores the speedup is only 2<inline-formula id="inline-formula10-1094342011434065">
<mml:math id="mml-inline10-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. Intel also has scaling issues and performance flattens to 9<inline-formula id="inline-formula11-1094342011434065">
<mml:math id="mml-inline11-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> at 16 cores. Stock Qthreads <italic>Q</italic> scales slightly better (9.4<inline-formula id="inline-formula12-1094342011434065">
<mml:math id="mml-inline12-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>), but just switching to the LIFO queue <italic>L</italic> to improve locality between tasks allows speedup on 32 cores to reach 11.5<inline-formula id="inline-formula13-1094342011434065">
<mml:math id="mml-inline13-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. Since the individual tasks are relatively small, <italic>CQ</italic> experiences contention on its task queue that limits speedup to 7.7<inline-formula id="inline-formula14-1094342011434065">
<mml:math id="mml-inline14-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> on 16 cores, with performance degrading to 6.1<inline-formula id="inline-formula15-1094342011434065">
<mml:math id="mml-inline15-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> at 32 cores. When work stealing, <italic>WS</italic>, is added to Qthreads the performance improves slightly and speedup reaches 11.6<inline-formula id="inline-formula16-1094342011434065">
<mml:math id="mml-inline16-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. <italic>MTS</italic> further improves locality and load balance on each processor by sharing a queue across the cores on each chip, and speedup increases to 13.6<inline-formula id="inline-formula17-1094342011434065">
<mml:math id="mml-inline17-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> on 32 cores. This additional scalability allows Qthread <italic>MTS</italic> a 17.3% faster execution time on 32 cores than any other implementation, much faster than ICC (48.7%) and GCC(116.1%). <italic>Health</italic> provides an excellent example of how both work stealing and queue sharing within a system can independently and together improve performance, though the failure of any run-time to reach 50% efficiency on 32 cores shows that there is room for improvement.</p>
<fig id="fig3-1094342011434065" position="float">
<label>Figure 3.</label>
<caption>
<p>
<italic>Health</italic> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig3-1094342011434065" xlink:href="10.1177_1094342011434065-fig3.tif"/>
</fig>
<p>The benefits of hierarchical scheduling can also be seen in <xref ref-type="fig" rid="fig4-1094342011434065">Figure 4</xref>
. <italic>Sort</italic>, for which we used a manual cutoff of 32K integers to switch between parallel and serial sorts, achieved speed up of about 16<inline-formula id="inline-formula18-1094342011434065">
<mml:math id="mml-inline18-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> for 32 cores on ICC and GCC, but just 11.4<inline-formula id="inline-formula19-1094342011434065">
<mml:math id="mml-inline19-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> for the base version of Qthreads, <italic>Q</italic>. The switch to a LIFO queue, <italic>L</italic>, improved speedup to 13.6<inline-formula id="inline-formula20-1094342011434065">
<mml:math id="mml-inline20-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> by facilitating data sharing between a parent and child. Independent changes to add work stealing, <italic>WS</italic>, and improve load balance, <italic>CQ</italic>, both improved speedup to 16<inline-formula id="inline-formula21-1094342011434065">
<mml:math id="mml-inline21-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. By combining the best features of both work stealing and multiple threads sharing a queue, <italic>MTS</italic> increased speedup to 18.4<inline-formula id="inline-formula22-1094342011434065">
<mml:math id="mml-inline22-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> and achieved a 13.8% and 11.4% reduction in overall execution time compared to ICC and GCC OpenMP versions respectively.</p>
<fig id="fig4-1094342011434065" position="float">
<label>Figure 4.</label>
<caption>
<p>
<italic>Sort</italic> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig4-1094342011434065" xlink:href="10.1177_1094342011434065-fig4.tif"/>
</fig>
<p>Locality effects allow <italic>NQueens</italic> to achieve slightly superlinear speedup for four and eight cores using Qthreads. As seen in <xref ref-type="fig" rid="fig5-1094342011434065">Figure 5</xref>
, speedup is near-linear for 16 threads and only somewhat sublinear for 32 threads on all OpenMP implementations. By adding load balancing mechanisms to Qthreads, its speedup improved significantly (24.3<inline-formula id="inline-formula23-1094342011434065">
<mml:math id="mml-inline23-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> to 28.4<inline-formula id="inline-formula24-1094342011434065">
<mml:math id="mml-inline24-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>). <italic>CQ</italic> and <italic>WS</italic> both improved load balance beyond what the LIFO queue (<italic>L</italic>) provides and little is gained by combining them together in <italic>MTS</italic>. The additional scaling of these three versions results in an execution time 12.6% faster than ICC and 10.9% faster than GCC.</p>
<fig id="fig5-1094342011434065" position="float">
<label>Figure 5.</label>
<caption>
<p>
<italic>NQueens</italic> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig5-1094342011434065" xlink:href="10.1177_1094342011434065-fig5.tif"/>
</fig>
<p>
<italic>Fib</italic>, <xref ref-type="fig" rid="fig6-1094342011434065">Figure 6</xref>
, uses a cut-off to stop the creation of very small tasks, and thus has enough work in each task to amortize the costs of queue access. <italic>CQ</italic> yields performance 2–3% faster than <italic>MTS</italic> and the other versions of Qthreads, since load balance is good and no time is spent looking for work. The load balancing versions of Qthreads (26.1<inline-formula id="inline-formula25-1094342011434065">
<mml:math id="mml-inline25-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>–26.7<inline-formula id="inline-formula26-1094342011434065">
<mml:math id="mml-inline26-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>) scale better than Intel at 24.9<inline-formula id="inline-formula27-1094342011434065">
<mml:math id="mml-inline27-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. Both systems beat GCC substantially at only 15.8<inline-formula id="inline-formula28-1094342011434065">
<mml:math id="mml-inline28-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. Overall, the scheduling improvements resulted in <italic>MTS</italic> running 26.5% faster than ICC and 28.8% faster than GCC but 2.0% slower than <italic>CQ</italic>.</p>
<fig id="fig6-1094342011434065" position="float">
<label>Figure 6.</label>
<caption>
<p>
<italic>Fib</italic> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig6-1094342011434065" xlink:href="10.1177_1094342011434065-fig6.tif"/>
</fig>
<p>The next two applications, <italic>Alignment</italic> and <italic>SparseLU</italic>, each have two versions. For <italic>Alignment</italic>, <xref ref-type="fig" rid="fig7-1094342011434065">Figures 7</xref> and <xref ref-type="fig" rid="fig8-1094342011434065">8</xref>

, speedup was near-linear for all versions, and execution times between GCC and Qthreads were close (GCC +2.7% single initial task version; Qthreads +0.5% parallel loop version). ICC scales better than GCC or Qthreads <italic>MTS</italic>, <italic>WS</italic>, <italic>CQ</italic>, with 12.4% lower execution time. Since <italic>Alignment</italic> has no taskwait synchronizations, we speculate that ICC scales better on this benchmark because it maintains fewer bookkeeping data structures in the absence of synchronization.</p>
<fig id="fig7-1094342011434065" position="float">
<label>Figure 7.</label>
<caption>
<p>
<italic>Alignment</italic>- <bold>single</bold> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig7-1094342011434065" xlink:href="10.1177_1094342011434065-fig7.tif"/>
</fig>
<fig id="fig8-1094342011434065" position="float">
<label>Figure 8.</label>
<caption>
<p>
<italic>Alignment</italic>- <bold>for</bold> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig8-1094342011434065" xlink:href="10.1177_1094342011434065-fig8.tif"/>
</fig>
<p>On both <italic>SparseLU</italic> versions, ICC serial performance improved nearly 3<inline-formula id="inline-formula29-1094342011434065">
<mml:math id="mml-inline29-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> using the -ipo and -xHost flags rather than using -O2 alone. The flags also improved parallel performance, but by only 60%, so the improvement does not scale linearly. On <italic>SparseLU</italic>- single, <xref ref-type="fig" rid="fig9-1094342011434065">Figure 9</xref>
, the performance of GCC and the various Qthreads versions is effectively equivalent, with speedup reaching 26.2<inline-formula id="inline-formula30-1094342011434065">
<mml:math id="mml-inline30-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. Due to the aforementioned scaling issues, ICC speedup reaches only 14.8<inline-formula id="inline-formula31-1094342011434065">
<mml:math id="mml-inline31-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. The execution times differ by 0.3% between GCC and <italic>MTS</italic>, with both about 74.4% faster than ICC. On <italic>SparseLU</italic>- for, <xref ref-type="fig" rid="fig10-1094342011434065">Figure 10</xref>
, the GCC OpenMP runs were stopped after exceeding the sequential time; thus data is not reported. ICC again scales poorly (14.8<inline-formula id="inline-formula32-1094342011434065">
<mml:math id="mml-inline32-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>), and Qthreads speedup improves due to the LIFO work queue and work stealing, reaching 22.2<inline-formula id="inline-formula33-1094342011434065">
<mml:math id="mml-inline33-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. <italic>MTS</italic> execution time is 46.3% faster than ICC.</p>
<fig id="fig9-1094342011434065" position="float">
<label>Figure 9.</label>
<caption>
<p>
<italic>SparseLU</italic>- <bold>single</bold> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig9-1094342011434065" xlink:href="10.1177_1094342011434065-fig9.tif"/>
</fig>
<fig id="fig10-1094342011434065" position="float">
<label>Figure 10.</label>
<caption>
<p>
<italic>SparseLU</italic>- for on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig10-1094342011434065" xlink:href="10.1177_1094342011434065-fig10.tif"/>
</fig>
<p>
<italic>Strassen</italic>, <xref ref-type="fig" rid="fig11-1094342011434065">Figure 11</xref>
, performs recursive matrix multiplication using Strassen’s method and is challenging for implementations with multiple workers accessing a queue. We used the cut-off setting that gave the best performance for each implementation: coarser (128) for <italic>CQ</italic> and <italic>MTS</italic> and the default setting (64) for the others. The execution times of GCC and <italic>WS</italic> are within 1% of each other on 32 cores, and Intel scales slightly better (16.7<inline-formula id="inline-formula34-1094342011434065">
<mml:math id="mml-inline34-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> vs 16.1<inline-formula id="inline-formula35-1094342011434065">
<mml:math id="mml-inline35-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>). For <italic>MTS</italic>, in which only 8 threads share a queue (rather than 32 as in <italic>CQ</italic>) the speedup reaches 15.2<inline-formula id="inline-formula36-1094342011434065">
<mml:math id="mml-inline36-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. For <italic>CQ</italic>, however, the performance hit due to queue contention is substantial, as speedup peaks at 9.7<inline-formula id="inline-formula37-1094342011434065">
<mml:math id="mml-inline37-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. <italic>Q</italic> performance suffers from the FIFO ordering: not enough parallel work is expressed at any one time, and speedup never exceeds 4<inline-formula id="inline-formula38-1094342011434065">
<mml:math id="mml-inline38-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>.</p>
<fig id="fig11-1094342011434065" position="float">
<label>Figure 11.</label>
<caption>
<p>
<italic>Strassen</italic> on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig11-1094342011434065" xlink:href="10.1177_1094342011434065-fig11.tif"/>
</fig>
</sec>
<sec id="section10-1094342011434065">
<title>4.3 Variability</title>
<p>One interesting feature of a work-stealing run-time is an idle thread’s ability to search for work and the effect this has on performance in regions of limited parallelism or load imbalance. <xref ref-type="table" rid="table3-1094342011434065">Table 3</xref>
 gives the standard deviation of 10 runs as a percentage of the fastest time for each configuration tested with 32 threads. Both Qthreads implementations with work stealing (<italic>WS</italic> and <italic>MTS</italic>) have very small variation in execution time for three of the nine programs. For eight of the nine benchmarks, both <italic>WS</italic> and <italic>MTS</italic> show less variability than ICC.</p>
<table-wrap id="table3-1094342011434065" position="float">
<label>Table 3.</label>
<caption>
<p>Variability in performance on four-socket Intel Nehalem using ICC, GCC, <italic>MTS</italic>, and <italic>WS</italic> schedulers (standard deviation as a percentage of the fastest time).</p>
</caption>
<graphic alternate-form-of="table3-1094342011434065" xlink:href="10.1177_1094342011434065-table3.tif"/>
<table>
<thead>
<tr>
<th>Configuration</th>
<th align="center">
<italic>Alignment</italic> (single)</th>
<th align="center">
<italic>Alignment</italic> (for)</th>
<th align="center">
<italic>Fib</italic>
</th>
<th>
<italic>Health</italic>
</th>
<th align="center">
<italic>NQueens</italic>
</th>
<th align="center">
<italic>Sort</italic>
</th>
<th align="center">
<italic>SparseLU</italic> (single)</th>
<th align="center">
<italic>SparseLU</italic> (for)</th>
<th align="center">
<italic>Strassen</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>ICC 32 threads</td>
<td>4.4</td>
<td>2.0</td>
<td>3.7</td>
<td>2.0</td>
<td>3.2</td>
<td>4.0</td>
<td>1.1</td>
<td>3.9</td>
<td>1.8</td>
</tr>
<tr>
<td>GCC 32 threads</td>
<td>0.11</td>
<td>0.34</td>
<td>2.8</td>
<td>0.35</td>
<td>0.77</td>
<td>1.8</td>
<td>0.49</td>
<td align="center">N/A</td>
<td>1.4</td>
</tr>
<tr>
<td>Qthreads <italic>MTS</italic> 32 workers</td>
<td>0.28</td>
<td>1.5</td>
<td>3.3</td>
<td>1.3</td>
<td>0.78</td>
<td>1.9</td>
<td>0.15</td>
<td>0.16</td>
<td>1.9</td>
</tr>
<tr>
<td>Qthreads <italic>WS</italic> 32 shepherds</td>
<td>0.035</td>
<td>1.8</td>
<td>2.0</td>
<td>0.29</td>
<td>0.60</td>
<td>0.90</td>
<td>0.060</td>
<td>0.24</td>
<td>3.0</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table4-1094342011434065" position="float">
<label>Table 4.</label>
<caption>
<p>Tasks stolen and tasks per steal using the <italic>MTS</italic> scheduler. Average of 10 runs.</p>
</caption>
<graphic alternate-form-of="table4-1094342011434065" xlink:href="10.1177_1094342011434065-table4.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>
<italic>Alignment</italic> (single)</th>
<th>
<italic>Alignment</italic> (for)</th>
<th>
<italic>Fib</italic>
</th>
<th>
<italic>Health</italic>
</th>
<th>
<italic>NQueens</italic>
</th>
<th>
<italic>Sort</italic>
</th>
<th>
<italic>SparseLU</italic> (single)</th>
<th>
<italic>SparseLU</italic> (for)</th>
<th>
<italic>Strassen</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tasks Stolen</td>
<td>5900</td>
<td>450</td>
<td>2181</td>
<td>159386</td>
<td>423</td>
<td>5214</td>
<td>93117</td>
<td>38198</td>
<td>1355</td>
</tr>
<tr>
<td>Tasks Per Steal</td>
<td>5.8</td>
<td>4.1</td>
<td>3.4</td>
<td>5.5</td>
<td>4.1</td>
<td>4.6</td>
<td>5.1</td>
<td>2.8</td>
<td>6.0</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In three cases (<italic>Alignment-</italic>
<bold>single</bold>, <italic>Health</italic>, <italic>SparseLU</italic>-<bold>single</bold>), Qthreads <italic>WS</italic> variability was much lower than <italic>MTS</italic>. Since <italic>MTS</italic> enables only one worker thread per shepherd at a time to steal a chunk of tasks, it is reasonable to expect this granularity to be reflected in execution time variations. Overall, we see less variability with <italic>WS</italic> than <italic>MTS</italic> in six of the nine benchmarks. We speculate that normally having all the threads looking for work leads to finding the last work quickest and therefore less variation in total execution time. However, for some programs (<italic>Alignment</italic>-<bold>for</bold>, <italic>SparseLU</italic>- for, <italic>Strassen</italic>), stealing multiple tasks and moving them to an idle shepherd results in faster execution during periods of limited parallelism. <italic>WS</italic> also shows less variability than GCC in six of the eight programs for which we have data. There is no data for <italic>SparseLU</italic>- <bold>for</bold> on GCC, as explained in the previous section.</p>
</sec>
<sec id="section11-1094342011434065">
<title>4.4 Performance analysis of <italic>MTS</italic>
</title>
<p>Limiting the number of inter-chip load balancing operations is central to the design of our hierarchical scheduler (<italic>MTS</italic>). Consider the number of remote (off-chip) steal operations performed by <italic>MTS</italic> and by the flat work-stealing scheduler <italic>WS</italic>, shown in <xref ref-type="table" rid="table5-1094342011434065">Table 5</xref>
 These counts exclude the number of on-chip steals performed by <italic>WS</italic>, and recall that <italic>MTS</italic> uses work stealing only between chips. We observe that <italic>WS</italic> steals more than <italic>MTS</italic> in almost all cases, and in some cases by an order of magnitude. <italic>Health</italic> and <italic>Sort</italic> are two benchmarks where <italic>MTS</italic> wins clearly in terms of speedup. <italic>WS</italic> steals remotely over twice as many times as <italic>MTS</italic> on <italic>Sort</italic> and nearly twice as many times as <italic>MTS</italic> on <italic>Health</italic>. The number of failed steals is also significantly higher with <italic>WS</italic> than with <italic>MTS</italic>. A failed steal occurs when a thief’s lock-free probe of a victim indicates that work is available but upon acquisition of the lock to the victim’s queue the thief finds no work to steal because another thread has stolen it or the victim has executed the tasks itself. Thus, both failed and completed steals contribute to overhead costs.</p>
<table-wrap id="table5-1094342011434065" position="float">
<label>Table 5.</label>
<caption>
<p>Number of remote steal operations during execution by Qthreads <italic>MTS</italic> and <italic>WS</italic> schedulers. In a failed steal, the thief acquires the lock on the victim’s queue after a positive probe for work but ultimately finds no work available for stealing. On-chip steals performed by the <italic>WS</italic> scheduler are excluded. Average of 10 runs.</p>
</caption>
<graphic alternate-form-of="table5-1094342011434065" xlink:href="10.1177_1094342011434065-table5.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th align="center" colspan="2">
<italic>MTS</italic>
</th>
<th align="center" colspan="2">
<italic>WS</italic>
</th>
</tr>
<tr>
<th>Benchmark</th>
<th>Steals</th>
<th>Failed</th>
<th>Steals</th>
<th>Failed</th>
</tr>
</thead>
<tbody>
<tr>
<td>
<italic>Alignment</italic> (single)</td>
<td>1016</td>
<td>88</td>
<td>3695</td>
<td>255</td>
</tr>
<tr>
<td>
<italic>Alignment</italic> (for)</td>
<td>109</td>
<td>122</td>
<td>1431</td>
<td>286</td>
</tr>
<tr>
<td>
<italic>Fib</italic>
</td>
<td>633</td>
<td>331</td>
<td>467</td>
<td>984</td>
</tr>
<tr>
<td>
<italic>Health</italic>
</td>
<td>28948</td>
<td>10323</td>
<td>295637</td>
<td>47538</td>
</tr>
<tr>
<td>
<italic>NQueens</italic>
</td>
<td>102</td>
<td>141</td>
<td>1428</td>
<td>389</td>
</tr>
<tr>
<td>
<italic>Sort</italic>
</td>
<td>1134</td>
<td>404</td>
<td>19330</td>
<td>3283</td>
</tr>
<tr>
<td>
<italic>SparseLU</italic> (single)</td>
<td>18045</td>
<td>8133</td>
<td>68927</td>
<td>24506</td>
</tr>
<tr>
<td>
<italic>SparseLU</italic> (for)</td>
<td>13486</td>
<td>11889</td>
<td>68099</td>
<td>32205</td>
</tr>
<tr>
<td>
<italic>Strassen</italic>
</td>
<td>227</td>
<td>157</td>
<td>14042</td>
<td>823</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table6-1094342011434065" position="float">
<label>Table 6.</label>
<caption>
<p>Memory performance data for Health using <italic>MTS</italic> and <italic>WS</italic>. Average of 10 runs on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="table6-1094342011434065" xlink:href="10.1177_1094342011434065-table6.tif"/>
<table>
<thead>
<tr>
<th>Metric</th>
<th align="center">
<italic>MTS</italic>
</th>
<th align="center">
<italic>WS</italic>
</th>
<th align="center">%Diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>L3 Misses</td>
<td>1.16e+06</td>
<td>2.58e+06</td>
<td>38</td>
</tr>
<tr>
<td>Bytes from Memory</td>
<td>8.23e+09</td>
<td>9.21e+09</td>
<td>5.6</td>
</tr>
<tr>
<td>Bytes on QPI</td>
<td>2.63e+10</td>
<td>2.98e+10</td>
<td>6.2</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The <italic>MTS</italic> scheduler aggregates inter-chip load balancing by permitting only one worker at a time to initiate bulk stealing from remote shepherds. <xref ref-type="fig" rid="fig12-1094342011434065">Figure 12</xref>
 shows how this improves performance on <italic>Health</italic>, one of the benchmarks sensitive to load balancing granularity. If only one task is stolen at a time, subsequent steals are needed to provide all workers with tasks, adding to overhead costs. There are eight cores per socket on our test machine, thus eight workers per shepherd, and a target of eight tasks stolen per steal request. This coincides with the peak performance: when the target number of tasks stolen corresponds to the number of workers in the shepherd, all workers in the shepherd are able to draw work immediately from the queue as a result of the steal.</p>
<fig id="fig12-1094342011434065" position="float">
<label>Figure 12.</label>
<caption>
<p>Performance on <italic>Health</italic> using <italic>MTS</italic> based on choice of the chunk size for stealing. Average of 10 runs on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="fig12-1094342011434065" xlink:href="10.1177_1094342011434065-fig12.tif"/>
</fig>
<p>Frequently, the number of tasks available to steal is less than the target number to be stolen. <xref ref-type="table" rid="table5-1094342011434065">Table 5</xref> shows the total number of tasks stolen and the average number of tasks stolen per steal operation. Across all benchmarks, the range of tasks stolen per steal is 2.8 to 6.0. The numbers skew downward due to a scarcity of work during start-up and near termination, when only one or few tasks are available at a time. Note the lower number both of total steals and tasks per steal for the for versions of <italic>Alignment</italic> and <italic>SparseLU</italic> compared to the single versions. Loop parallel initialization provides good initial load balance so that fewer steals are needed, and those that do occur sporadically are near termination and synchronization phases.</p>
<p>Another benefit of the <italic>MTS</italic> scheduler is better L3 cache performance, since all workers in a shepherd share the on-chip L3 cache. The <italic>WS</italic> scheduler exhibits poorer cache performance and, consequently, more reads to main memory. <xref ref-type="table" rid="table5-1094342011434065">Tables 7</xref> and <xref ref-type="table" rid="table6-1094342011434065">8</xref>

 show the relevant metrics for <italic>Health</italic> and <italic>Sort</italic> as measured using hardware performance counters, averaged over 10 runs. They also show more traffic on the Quick Path Interconnect (QPI) between chips for <italic>WS</italic> than for <italic>MTS</italic>. QPI traffic occurs when data is requested and transferred from either remote memory or remote L3 cache, i.e. attached to a different socket of the machine. Not only are remote accesses higher latency, but they also result in remote cache invalidations of shared cache lines and subsequent coherence misses. Increased QPI traffic in <italic>WS</italic> reflects more remote steals and more accesses to data in remote L3 caches and remote memory. In summary, <italic>MTS</italic> gains advantage by exploiting locality among tasks executed by threads on cores of the same chip, making good use of the shared L3 cache to access memory less frequently and avoid high latency remote accesses and coherence misses.</p>
<table-wrap id="table7-1094342011434065" position="float">
<label>Table 7.</label>
<caption>
<p>Memory performance data for <italic>Sort</italic> using <italic>MTS</italic> and <italic>WS</italic>. Average of 10 runs on four-socket Intel Nehalem.</p>
</caption>
<graphic alternate-form-of="table7-1094342011434065" xlink:href="10.1177_1094342011434065-table7.tif"/>
<table>
<thead>
<tr>
<th>Metric</th>
<th align="center">
<italic>MTS</italic>
</th>
<th align="center">
<italic>WS</italic>
</th>
<th align="center">%Diff</th>
</tr>
</thead>
<tbody>
<tr>
<td>L3 Misses</td>
<td>1.03e+7</td>
<td>3.42e+07</td>
<td>54</td>
</tr>
<tr>
<td>Bytes from Memory</td>
<td>2.27e+10</td>
<td>2.53e+10</td>
<td>5.5</td>
</tr>
<tr>
<td>Bytes on QPI</td>
<td>4.35e+10</td>
<td>4.87e+10</td>
<td>5.6</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table8-1094342011434065" position="float">
<label>Table 8.</label>
<caption>
<p>Sequential execution times using ICC and GCC on the AMD Magny Cours.</p>
</caption>
<graphic alternate-form-of="table8-1094342011434065" xlink:href="10.1177_1094342011434065-table8.tif"/>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>
<italic>Alignment</italic>
</th>
<th>
<italic>Fib</italic>
</th>
<th>
<italic>Health</italic>
</th>
<th>
<italic>NQueens</italic>
</th>
<th>
<italic>Sort</italic>
</th>
<th>
<italic>SparseLU</italic>
</th>
<th>
<italic>Strassen</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>ICC</td>
<td>23.93</td>
<td>107.9</td>
<td>10.18</td>
<td>60.56</td>
<td>18.51</td>
<td>156.0</td>
<td>214.9</td>
</tr>
<tr>
<td>GCC</td>
<td>29.77</td>
<td>105.0</td>
<td>10.67</td>
<td>58.16</td>
<td>17.72</td>
<td>153.4</td>
<td>211.1</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section12-1094342011434065">
<title>4.5 Performance on AMD Magny Cours</title>
<p>We also evaluate the Qthreads schedulers against ICC and GCC on a 2-socket AMD Magny Cours system, one node of a cluster at Sandia National Laboratories. Each socket hosts an Opteron 6136 multi-chip module: two quad-core chips that share a package connected via two internal HyperTransport (HT) links. The remaining two HT links per chip are connected to the chips in the other socket, as shown in <xref ref-type="fig" rid="fig13-1094342011434065">Figure 13</xref>
. Each chip contains a memory controller with 8 GB attached DDR3 memory, a 5 MB shared L3 cache, and four 2.4 MHz cores with 64 Kb L1 and 512 Kb L2 caches. Thus, there are a total of 16 cores and 32 GB memory, evenly divided among four HyperTransport-connected NUMA nodes (one per chip, two per socket). The system runs Cray compute-node Linux kernel 2.6.27, and we used the GCC 4.6.0 with -O3 optimization and ICC 12.0 with -O3 -ipo -msse3 -simd optimization.</p>
<fig id="fig13-1094342011434065" position="float">
<label>Figure 13.</label>
<caption>
<p>Topology of the two-socket/four-chip AMD Magny Cours.</p>
</caption>
<graphic alternate-form-of="fig13-1094342011434065" xlink:href="10.1177_1094342011434065-fig13.tif"/>
</fig>
<p>We ran the same benchmarks with the same parameters as the Intel Nehalem evaluation. Sequential execution times are reported in <xref ref-type="table" rid="table8-1094342011434065">Table 8</xref>. Again, interprocedural optimization (-ipo) in ICC was essential to match the GCC performance; execution time was more than 500 seconds without it. The greatest remaining difference between the sequential times is for <italic>Alignment</italic>, where GCC is 20% slower than ICC.</p>
<p>Speedup results using 16 threads are given in <xref ref-type="fig" rid="fig14-1094342011434065">Figure 14</xref>
, for Qthreads configurations with one shepherd per chip, <italic>MTS (4Q)</italic>; one shepherd per socket, <italic>MTS (2Q)</italic>; one shepherd per core (flat work stealing), <italic>WS</italic>; ICC; and GCC. At least one of the Qthreads variants matches or beats ICC and GCC on all but one of the benchmarks. Moreover, the Qthreads schedulers achieve near-linear to slightly superlinear speedup on six of the nine benchmarks: the two versions of <italic>Alignment</italic>, <italic>Fib</italic>, <italic>NQueens</italic>, and the two versions of <italic>SparseLU</italic>. Of those, speedup using ICC is 22% and 23% lower than Qthreads on the two versions of <italic>Alignment</italic>, 10% and 18% lower on the two versions of <italic>SparseLU</italic>, 9% lower on <italic>NQueens</italic> and 7% lower on <italic>Fib</italic>. GCC is 42% lower than Qthreads on Fib, 9% and 27% lower on the two versions of <italic>SparseLU</italic>, and close on <italic>NQueens</italic> and both versions of <italic>Alignment</italic>.</p>
<fig id="fig14-1094342011434065" position="float">
<label>Figure 14.</label>
<caption>
<p>BOTS benchmarks on 2-socket AMD Magny Cours using 16 threads.</p>
</caption>
<graphic alternate-form-of="fig14-1094342011434065" xlink:href="10.1177_1094342011434065-fig14.tif"/>
</fig>
<p>On three of the benchmarks, no run-time achieves ideal speedup. <italic>Strassen</italic> is the only benchmark on which ICC and GCC outperform Qthreads, and even ICC falls short of 10<inline-formula id="inline-formula39-1094342011434065">
<mml:math id="mml-inline39-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. On <italic>Sort</italic>, the best performance is with Qthreads <italic>WS</italic>, <italic>MTS (4Q)</italic>, and GCC, all at roughly 8<inline-formula id="inline-formula40-1094342011434065">
<mml:math id="mml-inline40-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. Speedup is lower with Qthreads <italic>MTS (2Q)</italic> and still lower with <italic>CQ</italic>, indicating that centralized queueing beyond the chip level is counterproductive. ICC speedup lags behind the other schedulers on this benchmark. Speedup on <italic>Health</italic> peaks at 3.3<inline-formula id="inline-formula41-1094342011434065">
<mml:math id="mml-inline41-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> on this system using the Qthreads schedulers, with even worse speedup using ICC and GCC.</p>
<p>The variability in execution times is shown in <xref ref-type="table" rid="table9-1094342011434065">Table 9</xref>
. The standard deviations for all of the benchmarks on the <italic>MTS</italic> and <italic>WS</italic> Qthreads implementations are below 2% of the best-case execution time. On all but two of the benchmarks, the <italic>MTS</italic> standard deviation is less than 1%.</p>
<table-wrap id="table9-1094342011434065" position="float">
<label>Table 9.</label>
<caption>
<p>Variability in performance on AMD Magny Cours using 16 threads (standard deviation as a percentage of the fastest time).</p>
</caption>
<graphic alternate-form-of="table9-1094342011434065" xlink:href="10.1177_1094342011434065-table9.tif"/>
<table>
<thead>
<tr>
<th>Configuration</th>
<th align="center">
<italic>Alignment</italic> (single)</th>
<th align="center">
<italic>Alignment</italic> (for)</th>
<th align="center">
<italic>Fib</italic>
</th>
<th align="center">
<italic>Health</italic>
</th>
<th align="center">
<italic>NQueens</italic>
</th>
<th align="center">
<italic>Sort</italic>
</th>
<th align="center">
<italic>SparseLU</italic> (single)</th>
<th align="center">
<italic>SparseLU</italic> (for)</th>
<th align="center">
<italic>Strassen</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>ICC</td>
<td>2.2</td>
<td>0.80</td>
<td>1.3</td>
<td>14</td>
<td>1.1</td>
<td>8.2</td>
<td>0.62</td>
<td>0.31</td>
<td>2.5</td>
</tr>
<tr>
<td>GCC</td>
<td>0.035</td>
<td>0.27</td>
<td>5.4</td>
<td>0.38</td>
<td>0.96</td>
<td>3.5</td>
<td>0.016</td>
<td>0.025</td>
<td>1.1</td>
</tr>
<tr>
<td>Qthreads <italic>MTS (4Q)</italic>
</td>
<td>0.25</td>
<td>0.63</td>
<td>1.5</td>
<td>0.17</td>
<td>0.13</td>
<td>1.1</td>
<td>0.012</td>
<td>0.16</td>
<td>0.98</td>
</tr>
<tr>
<td>Qthreads <italic>MTS (2Q)</italic>
</td>
<td>0.46</td>
<td>0.68</td>
<td>1.4</td>
<td>0.069</td>
<td>0.24</td>
<td>0.30</td>
<td>0.015</td>
<td>0.081</td>
<td>0.87</td>
</tr>
<tr>
<td>Qthreads <italic>WS</italic>
</td>
<td>0.21</td>
<td>1.3</td>
<td>1.5</td>
<td>0.15</td>
<td>0.13</td>
<td>1.8</td>
<td>0.036</td>
<td>0.094</td>
<td>1.4</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The Magny Cours results demonstrate that the competitive, and in some cases superior, performance of our Qthreads schedulers against ICC and GCC is not confined to the Intel architecture. At first glance, differences in performance using the various Qthreads configurations seem less pronounced than they were on the four-socket Intel machine. However, those differences were strongest on the Intel machine at 32 threads, and the AMD system only has 16 threads. Some architectural differences go beyond the difference in core count. <italic>MTS</italic> is designed to leverage locality in shared L3 cache, but the Magny Cours has much less L3 cache per core than the Intel system (1.25 MB/core versus 2.25 MB/core). Less available cache also accounts for worse performance on the data-intensive <italic>Sort</italic> and <italic>Health</italic> benchmarks.</p>
</sec>
<sec id="section13-1094342011434065">
<title>4.6 Performance on SGI Altix</title>
<p>We evaluate scalability beyond 32 threads on an SGI Altix 3700. Each of the 96 nodes contains two 1.6 MHz Intel Itanium2 processors and 4 GB of memory, for a total of 192 processors and 384 GB of memory. The nodes are connected by the proprietary SGI NUMALink4 network and run a single system image of SuSE Linux kernel 2.6.16. We used the GCC 4.5.2 compiler as the native compiler for our ROSE-transformed code and the GCC OpenMP run-time for comparison against Qthreads. The version of ICC on the system is not recent enough to include support for OpenMP tasks. Sequential execution times, given in <xref ref-type="table" rid="table10-1094342011434065">Table 10</xref>
, are slower than those of the other machines, because the Itanium2 is an older processor, runs at a lower clock rate, and uses a different instruction set (ia64).</p>
<table-wrap id="table10-1094342011434065" position="float">
<label>Table 10.</label>
<caption>
<p>Sequential execution times on the SGI Altix.</p>
</caption>
<graphic alternate-form-of="table10-1094342011434065" xlink:href="10.1177_1094342011434065-table10.tif"/>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>
<italic>Alignment</italic>
</th>
<th>
<italic>Fib</italic>
</th>
<th>
<italic>Health</italic>
</th>
<th>
<italic>NQueens</italic>
</th>
<th>
<italic>Sort</italic>
</th>
<th>
<italic>SparseLU</italic>
</th>
<th>
<italic>Strassen</italic>
</th>
</tr>
</thead>
<tbody>
<tr>
<td>GCC</td>
<td>53.96</td>
<td>139.2</td>
<td>45.60</td>
<td>63.62</td>
<td>33.59</td>
<td>632.7</td>
<td>551.3</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The best observed performance on any of the benchmarks was on <italic>NQueens</italic>, shown in <xref ref-type="fig" rid="fig15-1094342011434065">Figure 15</xref>
. <italic>WS</italic> achieves 115<inline-formula id="inline-formula42-1094342011434065">
<mml:math id="mml-inline42-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> on 128 threads (90% parallel efficiency) and reaches 148<inline-formula id="inline-formula43-1094342011434065">
<mml:math id="mml-inline43-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> on 192 threads. <italic>MTS</italic> reaches 134<inline-formula id="inline-formula44-1094342011434065">
<mml:math id="mml-inline44-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup. (On this machine, the <italic>MTS</italic> configuration has two threads per shepherd to match the two processors per NUMA node.) <italic>CQ</italic> tops out at 77<inline-formula id="inline-formula45-1094342011434065">
<mml:math id="mml-inline45-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup on 96 threads, beyond which overheads from queue contention become overwhelming. GCC gets up to only 40<inline-formula id="inline-formula46-1094342011434065">
<mml:math id="mml-inline46-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup. Although no run-time achieves linear speedup on the full machine, they all reach 30<inline-formula id="inline-formula47-1094342011434065">
<mml:math id="mml-inline47-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> to 32<inline-formula id="inline-formula48-1094342011434065">
<mml:math id="mml-inline48-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup with 32 threads; this underlines the importance of testing at higher processor counts to evaluate scalability. On the <italic>Fib</italic> benchmark, shown in <xref ref-type="fig" rid="fig16-1094342011434065">Figure 16</xref>
, <italic>MTS</italic> almost doubles the performance of <italic>CQ</italic> and GCC on 192 threads, with a maximum speedup of 97<inline-formula id="inline-formula49-1094342011434065">
<mml:math id="mml-inline49-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. <italic>CQ</italic> peaks at 68<inline-formula id="inline-formula50-1094342011434065">
<mml:math id="mml-inline50-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup on 128 threads and <italic>WS</italic> exhibits its worst performance relative to <italic>MTS</italic>, maxing out at 77<inline-formula id="inline-formula51-1094342011434065">
<mml:math id="mml-inline51-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup on 96 threads.</p>
<fig id="fig15-1094342011434065" position="float">
<label>Figure 15.</label>
<caption>
<p>
<italic>NQueens</italic> on SGI Altix.</p>
</caption>
<graphic alternate-form-of="fig15-1094342011434065" xlink:href="10.1177_1094342011434065-fig15.tif"/>
</fig>
<fig id="fig16-1094342011434065" position="float">
<label>Figure 16.</label>
<caption>
<p>
<italic>Fib</italic> on SGI Altix.</p>
</caption>
<graphic alternate-form-of="fig17-1094342011434065" xlink:href="10.1177_1094342011434065-fig16.tif"/>
</fig>
<p>We see better peak performance on <italic>Alignment</italic>- for (<xref ref-type="fig" rid="fig18-1094342011434065">Figure 18</xref>
) than <italic>Alignment</italic>- <bold>single</bold> (<xref ref-type="fig" rid="fig17-1094342011434065">Figure 17</xref>
). <italic>WS</italic> reaches 116<inline-formula id="inline-formula52-1094342011434065">
<mml:math id="mml-inline52-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup on 192 threads and <italic>MTS</italic> reaches 107<inline-formula id="inline-formula53-1094342011434065">
<mml:math id="mml-inline53-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>, with <italic>CQ</italic> and GCC performing significantly worse. On the other hand, <italic>SparseLU</italic>- <bold>single</bold> (<xref ref-type="fig" rid="fig19-1094342011434065">Figure 19</xref>
) scales better than <italic>SparseLU</italic>- <bold>for</bold> (<xref ref-type="fig" rid="fig20-1094342011434065">Figure 20</xref>
). Peak speedup on <italic>SparseLU</italic>- <bold>single</bold> is 89<inline-formula id="inline-formula54-1094342011434065">
<mml:math id="mml-inline54-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> with <italic>MTS</italic> and 86<inline-formula id="inline-formula55-1094342011434065">
<mml:math id="mml-inline55-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> with <italic>WS</italic>, while <italic>SparseLU</italic>- <bold>for</bold> achieves a peak speedup of 60<inline-formula id="inline-formula56-1094342011434065">
<mml:math id="mml-inline56-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>. As was the case on the four-socket Intel machine, GCC is unable to complete after a timeout equal to the sequential execution time.</p>
<fig id="fig17-1094342011434065" position="float">
<label>Figure 17.</label>
<caption>
<p>
<italic>Alignment</italic>- single on SGI Altix.</p>
</caption>
<graphic alternate-form-of="fig17-1094342011434065" xlink:href="10.1177_1094342011434065-fig17.tif"/>
</fig>
<fig id="fig18-1094342011434065" position="float">
<label>Figure 18.</label>
<caption>
<p>
<italic>Alignment</italic>- for on SGI Altix.</p>
</caption>
<graphic alternate-form-of="fig18-1094342011434065" xlink:href="10.1177_1094342011434065-fig18.tif"/>
</fig>
<fig id="fig19-1094342011434065" position="float">
<label>Figure 19.</label>
<caption>
<p>
<italic>SparseLU</italic>- single on SGI Altix.</p>
</caption>
<graphic alternate-form-of="fig19-1094342011434065" xlink:href="10.1177_1094342011434065-fig19.tif"/>
</fig>
<fig id="fig20-1094342011434065" position="float">
<label>Figure 20.</label>
<caption>
<p>
<italic>SparseLU</italic>- for on SGI Altix.</p>
</caption>
<graphic alternate-form-of="fig20-1094342011434065" xlink:href="10.1177_1094342011434065-fig20.tif"/>
</fig>
<p>For three of the benchmarks, no improvement in speedup was observed beyond 32 threads: <italic>Health</italic>, <italic>Sort</italic>, and <italic>Strassen</italic>. As shown in <xref ref-type="fig" rid="fig21-1094342011434065">Figure 21</xref>
, none exceed 10<inline-formula id="inline-formula57-1094342011434065">
<mml:math id="mml-inline57-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> speedup on the Altix. These were also observed to be the most challenging on the four-socket Intel and two-socket AMD systems. <italic>Health</italic> and <italic>Sort</italic> are the most data-intensive and require new strategies to achieve performance improvement, an important area of research going forward.</p>
<fig id="fig21-1094342011434065" position="float">
<label>Figure 21.</label>
<caption>
<p><italic>Health</italic>, <italic>Sort</italic>, and <italic>Strassen</italic> on SGI Altix: 32 threads.</p>
</caption>
<graphic alternate-form-of="fig21-1094342011434065" xlink:href="10.1177_1094342011434065-fig21.tif"/>
</fig>
</sec>
</sec>
<sec id="section14-1094342011434065">
<title>5 Related work</title>
<p>Many theoretical and practical issues of task-parallel languages and their run-time implementations were explored during the development of earlier task-parallel programming models, both hardware supported, e.g. Tera MTA (<xref ref-type="bibr" rid="bibr1-1094342011434065">Alverson et al., 1992</xref>), and software supported, e.g. Cilk (<xref ref-type="bibr" rid="bibr6-1094342011434065">Blumofe et al., 1995</xref>; <xref ref-type="bibr" rid="bibr18-1094342011434065">Frigo et al., 1998</xref>). Much of our practical reasoning was influenced by experience with the Tera MTA run-time, designed for massive multithreading and low-overhead thread synchronization. Cilk scheduling uses a <italic>work-first</italic> scheduling strategy coupled with a randomized work-stealing load balancing strategy shown to be optimal (<xref ref-type="bibr" rid="bibr7-1094342011434065">Blumofe and Leiserson, 1994</xref>). Our use of shared queues is inspired by Parallel Depth-First Scheduling (PDFS) (<xref ref-type="bibr" rid="bibr5-1094342011434065">Blelloch et al., 1999</xref>), which attempts to maintain a schedule close to serial execution order, and its constructive cache sharing benefits (<xref ref-type="bibr" rid="bibr12-1094342011434065">Chen et al., 2007</xref>).</p>
<p>The first prototype compiler and run-time for OpenMP 3.0 tasks was an extension of Nanos Mercurium (<xref ref-type="bibr" rid="bibr31-1094342011434065">Teruel et al., 2007</xref>). An evaluation of scheduling strategies for tasks using Nanos compared centralized breadth-first and fully-distributed depth-first work-stealing schedulers (<xref ref-type="bibr" rid="bibr14-1094342011434065">Duran et al., 2008b</xref>). Later extensions to Nanos included internal dynamic cut-off methods to limit overhead costs by inlining tasks (<xref ref-type="bibr" rid="bibr13-1094342011434065">Duran et al., 2008a</xref>).</p>
<p>In addition to OpenMP 3.0, there are currently several other task-parallel languages and libraries available to developers: Microsoft Task Parallel Library (<xref ref-type="bibr" rid="bibr23-1094342011434065">Leijen et al., 2009</xref>) for Windows, Intel Thread Building Blocks (TBB) (<xref ref-type="bibr" rid="bibr22-1094342011434065">Kukanov and Voss, 2007</xref>), and Intel Cilk Plus (<xref ref-type="bibr" rid="bibr21-1094342011434065">Intel Corp., 2010</xref>) (formerly Cilk++). The task-parallel model and its run-time support are also key components of the X10 (<xref ref-type="bibr" rid="bibr10-1094342011434065">Charles et al., 2005</xref>) and Chapel (<xref ref-type="bibr" rid="bibr9-1094342011434065">Chamberlain et al., 2007</xref>) languages.</p>
<p>Hierarchical work stealing, i.e. stealing at all levels of a hierarchical scheduler, has been implemented for clusters and grids in Satin (<xref ref-type="bibr" rid="bibr32-1094342011434065">van Nieuwpoort et al., 2000</xref>), ATLAS (<xref ref-type="bibr" rid="bibr4-1094342011434065">Baldeschwieler et al., 1996</xref>), and more recently in Kaapi (<xref ref-type="bibr" rid="bibr19-1094342011434065">Gautier et al., 2007</xref>; <xref ref-type="bibr" rid="bibr28-1094342011434065">Quintin and Wagner, 2010</xref>). Those libraries are not optimized for shared caches in multi-core systems, which is the basis for the shared LIFO queue at the lower level of our hierarchical scheduler. The ForestGOMP run-time system (<xref ref-type="bibr" rid="bibr8-1094342011434065">Broquedis et al., 2010</xref>) also uses work stealing at both levels of its hierarchical scheduler, but like our system targets NUMA shared memory systems. It schedules OpenMP nested data parallelism by clustering related threads (not tasks) into “bubbles,” scheduling them by work stealing among cores on the same chip, and selecting for work stealing between chips those threads with the lowest amount of associated memory. Data is migrated between sockets along with the stolen threads.</p>
</sec>
<sec id="section15-1094342011434065">
<title>6 Conclusions and future work</title>
<p>As multicore systems proliferate, the future of software development for supercomputing relies increasingly on high-level programming models such as OpenMP for on-node parallelism. The recently added OpenMP constructs for task parallelism raise the level of abstraction to improve programmer productivity. However, if the run-time cannot execute applications efficiently on the available multicore systems, the benefits will be lost.</p>
<p>The complexity of multicore architectures grows with each hardware generation. Today, even off-the-shelf server chips have 6–12 cores and a chip-wide shared cache. Tomorrow may bring 30+ cores and multiple caches that service subsets of cores. Existing scheduling approaches were developed based on a flat system model. Our performance study revealed their strengths and limitations on a current generation multi-socket multicore architecture and demonstrated that mirroring the hierarchical nature of the hardware in the run-time scheduler can indeed improve performance. Qthreads (by way of ROSE) accepts a large number of OpenMP 3.0 programs, and, using our <italic>MTS</italic> scheduler, has performance as high or higher than the commonly available OpenMP 3.0 implementations. Its combination of shared LIFO queues and work stealing maintains good load balance while supporting effective cache performance and limiting overhead costs. On the other hand, pure work stealing has been shown to provide the least variability in performance, an important consideration for distributed applications in which barriers cause the application to run at the speed of the slowest worker, e.g. in a Bulk Synchronous Processing (BSP) application with task parallelism used in the computation phase.</p>
<p>The scalability results on the SGI Altix are important because previous BOTS evaluations (<xref ref-type="bibr" rid="bibr16-1094342011434065">Duran et al., 2009</xref>; <xref ref-type="bibr" rid="bibr26-1094342011434065">Olivier et al., 2011</xref>) only presented results on up to 32 cores. It is encouraging that several benchmarks reach speedups of 90<inline-formula id="inline-formula58-1094342011434065">
<mml:math id="mml-inline58-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula>–150<inline-formula id="inline-formula59-1094342011434065">
<mml:math id="mml-inline59-1094342011434065">
<mml:mo stretchy="false">×</mml:mo>
</mml:math>
</inline-formula> on 196 cores. The challenge on those benchmarks is to close the performance gap between observed speedup and ideal speedup through incremental reductions in overhead costs and idle times and better exploitation of locality. Other benchmarks fail to scale well even at 32 threads or fewer. On the data-intensive <italic>Sort</italic> and <italic>Health</italic> benchmarks we have observed a sharp increase in computation time due to increased load latencies compared to sequential execution. To ameliorate that issue, we are investigating programmer annotations to specify task scheduling constraints that identify and maintain data locality.</p>
<p>One challenge posed by our hierarchical scheduling strategy is the need for an efficient queue supporting concurrent access on both ends, since workers within a shepherd share a queue. Most existing lock-free queues for work stealing, such as the Arora, Blumofe, and Plaxton (ABP) queue (<xref ref-type="bibr" rid="bibr2-1094342011434065">Arora et al., 1998</xref>) and resizable variants (<xref ref-type="bibr" rid="bibr11-1094342011434065">Chase and Lev, 2005</xref>; <xref ref-type="bibr" rid="bibr20-1094342011434065">Hendler et al., 2006</xref>), allow only one thread to execute <bold>push( )</bold> and <bold>pop( )</bold> operations. Lock-free double-ended queues (deques) generalize the ABP queue to allow for concurrent insertion and removal on both ends of the queue. Lock-free deques have been implemented with compare-and-swap atomic primitives (<xref ref-type="bibr" rid="bibr25-1094342011434065">Michael, 2003</xref>, <xref ref-type="bibr" rid="bibr30-1094342011434065">Sundell and Tsigas, 2005</xref>), but speed is limited by their use of linked lists. We are currently working to implement an array-based lock-free deque, though even with a lock-based queue we have achieved results competitive with and in many cases better than ICC and GCC.</p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure" id="fn6-1094342011434065">
<p>This work is supported in part by a grant from the United States Department of Defense. Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Department of Energy’s National Nuclear Security Administration under contract DE-AC04-94AL85000.</p>
</fn>
<fn fn-type="other" id="fn7-1094342011434065">
<p>None declared. </p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Alverson</surname>
<given-names>GA</given-names>
</name>
<name>
<surname>Alverson</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Callahan</surname>
<given-names>D</given-names>
</name>
<etal/>
</person-group>. (<year>1992</year>) <article-title>Exploiting heterogeneous parallelism on a multithreaded multiprocessor</article-title>. In: <source>ICS’92: Proceedings of the 6th ACM International Conference on Supercomputing</source>, pp. <fpage>188</fpage>–<lpage>197</lpage>.</citation>
</ref>
<ref id="bibr2-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Arora</surname>
<given-names>NS</given-names>
</name>
<name>
<surname>Blumofe</surname>
<given-names>RD</given-names>
</name>
<name>
<surname>Plaxton</surname>
<given-names>CG</given-names>
</name>
</person-group> (<year>1998</year>) <article-title>Thread scheduling for multiprogrammed multiprocessors</article-title>. In: <source>SPAA’98: Proceedings of the 10th ACM Symposium on Parallel Algorithms and Architectures</source>, pp. <fpage>119</fpage>–<lpage>129</lpage>.</citation>
</ref>
<ref id="bibr3-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ayguadé</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Copty</surname>
<given-names>N</given-names>
</name>
<name>
<surname>Duran</surname>
<given-names>A</given-names>
</name>
<etal/>
</person-group>. (<year>2009</year>) <article-title>The design of OpenMP tasks</article-title>. <source>IEEE Transactions on Parallel and Distributed Systems</source>
<volume>20</volume>: <fpage>404</fpage>–<lpage>418</lpage>.</citation>
</ref>
<ref id="bibr4-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Baldeschwieler</surname>
<given-names>JE</given-names>
</name>
<name>
<surname>Blumofe</surname>
<given-names>RD</given-names>
</name>
<name>
<surname>Brewer</surname>
<given-names>EA</given-names>
</name>
</person-group> (<year>1996</year>) <article-title>Atlas: an infrastructure for global computing</article-title>. In: <source>EW 7: Proceedings of the 7th ACM SIGOPS European Workshop</source>, pp. <fpage>165</fpage>–<lpage>172</lpage>.</citation>
</ref>
<ref id="bibr5-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blelloch</surname>
<given-names>GE</given-names>
</name>
<name>
<surname>Gibbons</surname>
<given-names>PB</given-names>
</name>
<name>
<surname>Matias</surname>
<given-names>Y</given-names>
</name>
</person-group> (<year>1999</year>) <article-title>Provably efficient scheduling for languages with fine-grained parallelism</article-title>. <source>Journal of the ACM</source>
<volume>46</volume>: <fpage>281</fpage>–<lpage>321</lpage>.</citation>
</ref>
<ref id="bibr6-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blumofe</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Joerg</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Kuszmaul</surname>
<given-names>B</given-names>
</name>
<etal/>
</person-group>. (<year>1995</year>) <article-title>Cilk: An efficient multithreaded runtime system</article-title>. In: <source>PPoPP’95: Proceedings of the 5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming</source>, pp. <fpage>207</fpage>–<lpage>216</lpage>.</citation>
</ref>
<ref id="bibr7-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Blumofe</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Leiserson</surname>
<given-names>C</given-names>
</name>
</person-group> (<year>1994</year>) <article-title>Scheduling multithreaded computations by work stealing</article-title>. In: <source>SFCS’94: Proceedings of the 35th Annual Symposium on Foundations of Computer Science</source>, pp. <fpage>356</fpage>–<lpage>368</lpage>.</citation>
</ref>
<ref id="bibr8-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Broquedis</surname>
<given-names>F</given-names>
</name>
<name>
<surname>Aumage</surname>
<given-names>O</given-names>
</name>
<name>
<surname>Goglin</surname>
<given-names>B</given-names>
</name>
<etal/>
</person-group>. (<year>2010</year>) <article-title>Structuring the execution of OpenMP applications for multicore architectures</article-title>. In: <source>IPDPS 2010: Proceedings of the 25th IEEE International Parallel and Distributed Processing Symposium</source>, pp. <fpage>1</fpage>–<lpage>10</lpage>.</citation>
</ref>
<ref id="bibr9-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chamberlain</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Callahan</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Zima</surname>
<given-names>H</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Parallel programmability and the Chapel language</article-title>. <source>International Journal of High Performance Computing Applications</source>
<volume>21</volume>: <fpage>291</fpage>–<lpage>312</lpage>.</citation>
</ref>
<ref id="bibr10-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Charles</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Grothoff</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Saraswat</surname>
<given-names>V</given-names>
</name>
<etal/>
</person-group>. (<year>2005</year>) <article-title>X10: An object-oriented approach to non-uniform cluster computing</article-title>. In: <source>OOPSLA’05: Proceedings of the 20th ACM SIGPLAN Conference on Object Oriented Programming Systems, Languages, and Applications</source>, pp. <fpage>519</fpage>–<lpage>538</lpage>.</citation>
</ref>
<ref id="bibr11-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chase</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Lev</surname>
<given-names>Y</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>Dynamic circular work-stealing deque</article-title>. In: <source>SPAA’05: Proceedings of the 17th ACM Symposium on Parallelism in Algorithms and Architectures</source>, pp. <fpage>21</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr12-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chen</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Gibbons</surname>
<given-names>PB</given-names>
</name>
<name>
<surname>Kozuch</surname>
<given-names>M</given-names>
</name>
<etal/>
</person-group>. (<year>2007</year>) <article-title>Scheduling threads for constructive cache sharing on CMPs</article-title>. In: <source>SPAA’07: Proceedings of the 19th ACM Symposium on Parallel Algorithms and Architectures</source>, pp. <fpage>105</fpage>–<lpage>115</lpage>.</citation>
</ref>
<ref id="bibr13-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Duran</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Corbalán</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Ayguadé</surname>
<given-names>E</given-names>
</name>
</person-group> (<year>2008a</year>) <article-title>An adaptive cut-off for task parallelism</article-title>. In: <source>SC08: ACM/IEEE Supercomputing 2008</source>, pp. <fpage>1</fpage>–<lpage>11</lpage>. <publisher-loc>Piscataway, NJ</publisher-loc>: <publisher-name>IEEE Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-1094342011434065">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Duran</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Corbalán</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Ayguadé</surname>
<given-names>E</given-names>
</name>
</person-group> (<year>2008b</year>) <article-title>Evaluation of OpenMP task scheduling strategies</article-title>. In: <source>IWOMP’08: Proceedings of the International Workshop on OpenMP</source> (eds <person-group person-group-type="editor">
<name>
<surname>Eigenmann</surname>
<given-names>R</given-names>
</name>
<name>
<surname>de Supinski</surname>
<given-names>BR</given-names>
</name>
</person-group>), <comment>LNCS 5004</comment>: <fpage>100</fpage>–<lpage>110</lpage>.</citation>
</ref>
<ref id="bibr15-1094342011434065">
<citation citation-type="web">
<person-group person-group-type="author">
<name>
<surname>Duran</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Teruel</surname>
<given-names>X</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Barcelona</article-title>
<article-title>OpenMP Tasks Suite</article-title>. <ext-link ext-link-type="uri" xlink:href="http://nanos.ac.upc.edu/projects/bots">http://nanos.ac.upc.edu/projects/bots</ext-link>.</citation>
</ref>
<ref id="bibr16-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Duran</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Teruel</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Ferrer</surname>
<given-names>R</given-names>
</name>
<etal/>
</person-group>. (<year>2009</year>) <article-title>Barcelona OpenMP Tasks Suite: A set of benchmarks targeting the exploitation of task parallelism in OpenMP</article-title>. In: <source>ICPP’09: Proceedings of the 38th International Conference on Parallel Processing</source>, pp. <fpage>124</fpage>–<lpage>131</lpage>.</citation>
</ref>
<ref id="bibr17-1094342011434065">
<citation citation-type="web">
<collab collab-type="author">Free Software Foundation Inc</collab> (<year>2010</year>) <article-title>GNU Compiler Collection</article-title>. <ext-link ext-link-type="uri" xlink:href="http://www.gnu.org/software/gcc/">http://www.gnu.org/software/gcc/</ext-link>.</citation>
</ref>
<ref id="bibr18-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Frigo</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Leiserson</surname>
<given-names>CE</given-names>
</name>
<name>
<surname>Randall</surname>
<given-names>KH</given-names>
</name>
</person-group> (<year>1998</year>) <article-title>The implementation of the Cilk-5 multithreaded language</article-title>. In: <source>PLDI’98: Proceedings of the 1998 ACM SIGPLAN Conference on Programming Language Design and Implementation</source>, pp. <fpage>212</fpage>–<lpage>223</lpage>.</citation>
</ref>
<ref id="bibr19-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gautier</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Besseron</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Pigeon</surname>
<given-names>L</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>Kaapi: A thread scheduling runtime system for data flow computations on cluster of multi-processors</article-title>. In: <source>PASCO’07: Proceedings of the 2007 International Workshop on Parallel Symbolic Computation</source>, pp. <fpage>15</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr20-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Hendler</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Lev</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Moir M et</surname>
<given-names>al.</given-names>
</name>
</person-group> (<year>2006</year>) <article-title>A dynamic-sized nonblocking work stealing deque</article-title>. <source>Distributed Computing</source>
<volume>18</volume>: <fpage>189</fpage>–<lpage>207</lpage>.</citation>
</ref>
<ref id="bibr21-1094342011434065">
<citation citation-type="web">
<collab collab-type="author">Intel Corp</collab> (<year>2010</year>) <article-title>Intel Cilk Plus</article-title>. <ext-link ext-link-type="uri" xlink:href="http://software.intel.com/en-us/articles/intel-cilk-plus/">http://software.intel.com/en-us/articles/intel-cilk-plus/</ext-link>.</citation>
</ref>
<ref id="bibr22-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kukanov</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Voss</surname>
<given-names>M</given-names>
</name>
</person-group> (<year>2007</year>) <article-title>The foundations for scalable multi-core software in Intel Threading Building Blocks</article-title>. <source>Intel Technology Journal</source> <volume>11</volume>.</citation>
</ref>
<ref id="bibr23-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Leijen</surname>
<given-names>D</given-names>
</name>
<name>
<surname>Schulte</surname>
<given-names>W</given-names>
</name>
<name>
<surname>Burckhardt</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2009</year>) <article-title>The design of a task parallel library</article-title>. <source>SIGPLAN Notices: OOPSLA’09: 24th ACM SIGPLAN Conference on Object Oriented Programming Systems, Languages, and Applications</source>
<volume>44</volume>: <fpage>227</fpage>–<lpage>242</lpage>.</citation>
</ref>
<ref id="bibr24-1094342011434065">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Liao</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Quinlan</surname>
<given-names>DJ</given-names>
</name>
<name>
<surname>Panas T et</surname>
<given-names>al.</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>A ROSE-based OpenMP 3.0 research compiler supporting multiple runtime libraries</article-title>. In: <source>IWOMP 2010: Proceedings of the 6th International Workshop on OpenMP</source> (eds <person-group person-group-type="editor">
<name>
<surname>Sato</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Hanawa</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Müller</surname>
<given-names>MS</given-names>
</name>
<etal/>
</person-group>.), <comment>
<italic>LNCS</italic> 6132</comment>: <fpage>15</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr25-1094342011434065">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Michael</surname>
<given-names>MM</given-names>
</name>
</person-group> (<year>2003</year>) <article-title>CAS-based lock-free algorithm for shared deques</article-title>. In: <source>Euro-Par 2003: Proceedings of the 9th Euro-Par Conference on Parallel Processing</source> (eds <person-group person-group-type="editor">
<name>
<surname>Kosch</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Böszörményi</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Hellwagner</surname>
<given-names>H</given-names>
</name>
</person-group>), <comment>LNCS 2790</comment>: <fpage>651</fpage>–<lpage>660</lpage>.</citation>
</ref>
<ref id="bibr26-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Olivier</surname>
<given-names>SL</given-names>
</name>
<name>
<surname>Porterfield</surname>
<given-names>AK</given-names>
</name>
<name>
<surname>Wheeler</surname>
<given-names>KB</given-names>
</name>
<etal/>
</person-group>. (<year>2011</year>) <article-title>Scheduling task parallelism on multi-socket multicore systems</article-title>. In: <source>ROSS’11: Proceedings of the International Workshop on Runtime and Operating Systems for Supercomputers (in Conjunction with 2011 ACM International Conference on Supercomputing)</source>, pp. <fpage>49</fpage>–<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr27-1094342011434065">
<citation citation-type="book">
<collab collab-type="author">OpenMP Architecture Review Board</collab> (<year>2008</year>) <article-title>OpenMP API</article-title>, <comment>Version 3.0</comment>.</citation>
</ref>
<ref id="bibr28-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Quintin</surname>
<given-names>JN</given-names>
</name>
<name>
<surname>Wagner</surname>
<given-names>F</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>Hierarchical work-stealing</article-title>. In: <source>EuroPar’10: Proceedings of the 16th International Euro-Par Conference on Parallel Processing: Part I</source>, pp. <fpage>217</fpage>–<lpage>229</lpage>. <publisher-loc>Berlin, Heidelberg</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr29-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Smith</surname>
<given-names>BJ</given-names>
</name>
</person-group> (<year>1981</year>) <article-title>Architecture and applications of the HEP multiprocessor computer system</article-title>. In: <source>4th symposium on Real-Time Signal Processing</source>, pp. <fpage>241</fpage>–<lpage>248</lpage>.</citation>
</ref>
<ref id="bibr30-1094342011434065">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Sundell</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Tsigas</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2005</year>) <article-title>Lock-free and practical doubly linked list-based deques using single-word compare-and-swap</article-title>. In: <source>OPODIS 2004: 8th International Conference on Principles of Distributed Systems</source> (ed <person-group person-group-type="editor">
<name>
<surname>Higashino</surname>
<given-names>T</given-names>
</name>
</person-group>), <comment>LNCS</comment>
<fpage>240</fpage>–<lpage>255</lpage>.</citation>
</ref>
<ref id="bibr31-1094342011434065">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Teruel</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Martorell</surname>
<given-names>X</given-names>
</name>
<name>
<surname>Duran</surname>
<given-names>A</given-names>
</name>
<etal/>
</person-group>. (<year>2007</year>) <article-title>Support for OpenMP tasks in Nanos v4</article-title>. In: <source>CASCON’07: Proceedings of the 2007 Conference of the Center for Advanced Studies on Collaborative Research</source> (eds <person-group person-group-type="editor">
<name>
<surname>Lyons</surname>
<given-names>KA</given-names>
</name>
<name>
<surname>Couturier</surname>
<given-names>C</given-names>
</name>
</person-group>), pp. <fpage>256</fpage>–<lpage>259</lpage>.</citation>
</ref>
<ref id="bibr32-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>van Nieuwpoort</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Kielmann</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Bal</surname>
<given-names>HE</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>Satin: Efficient parallel divide-and-conquer in Java</article-title>. In: <source>Euro-Par’00: Proceedings of the 6th international Euro-Par Conference on Parallel Processing</source>, pp. <fpage>690</fpage>–<lpage>699</lpage>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>Springer</publisher-name>.</citation>
</ref>
<ref id="bibr33-1094342011434065">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wheeler</surname>
<given-names>KB</given-names>
</name>
<name>
<surname>Murphy</surname>
<given-names>RC</given-names>
</name>
<name>
<surname>Thain</surname>
<given-names>D</given-names>
</name>
</person-group> (<year>2008</year>) <article-title>Qthreads: An API for programming with millions of lightweight threads</article-title>. In: <source>IPDPS 2008: Proceedings of the 22nd IEEE International Symposium on Parallel and Distributed Processing</source>, pp. <fpage>1</fpage>–<lpage>8</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>