<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">NVS</journal-id>
<journal-id journal-id-type="hwp">spnvs</journal-id>
<journal-title>Nonprofit and Voluntary Sector Quarterly</journal-title>
<issn pub-type="ppub">0899-7640</issn>
<issn pub-type="epub">1552-7395</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0899764011423840</article-id>
<article-id pub-id-type="publisher-id">10.1177_0899764011423840</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Web and Mail Surveys</article-title>
<subtitle>An Experimental Comparison of Methods for Nonprofit Research</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lin</surname><given-names>Weiwei</given-names></name>
<xref ref-type="aff" rid="aff1-0899764011423840">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Van Ryzin</surname><given-names>Gregg G.</given-names></name>
<xref ref-type="aff" rid="aff2-0899764011423840">2</xref>
</contrib>
</contrib-group>
<aff id="aff1-0899764011423840"><label>1</label>University of Massachusetts, Dartmouth, North Dartmouth, MA, USA</aff>
<aff id="aff2-0899764011423840"><label>2</label>Rutgers University, Newark, NJ, USA</aff>
<author-notes>
<corresp id="corresp1-0899764011423840">Weiwei Lin, University of Massachusetts, Dartmouth, 285 Old Westport Road, North Dartmouth, MA 02747-2300 Email: <email>wlin@umassd.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>12</month>
<year>2012</year>
</pub-date>
<volume>41</volume>
<issue>6</issue>
<fpage>1014</fpage>
<lpage>1028</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">Association For Research On Nonprofit Organizations And Voluntary Action</copyright-holder>
</permissions>
<abstract>
<p>This study aims to compare two widely used methods of original data collection in nonprofit research: web and mail surveys. We employ an experimental design to assign a web-based survey and a mail survey to nonprofit professionals working in human services organizations in New Jersey. We then compare responses generated from the two survey methods in terms of response rates and data quality. Our study finds that the mail survey achieved a significantly higher response rate than the web survey, and data obtained from the mail survey produced higher internal consistency than that obtained from the web survey. There was no difference between methods, however, in respondent characteristics, the completeness of the survey, and the percentage of missing items. Taken together, the findings suggest that a mail survey, although more costly, may have response-rate and data-quality advantages over a web survey as a methodology for gathering data from nonprofit organizations.</p>
</abstract>
<kwd-group>
<kwd>mail and web surveys</kwd>
<kwd>experimental comparison</kwd>
<kwd>response rate</kwd>
<kwd>data quality</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-0899764011423840" sec-type="intro">
<title>Introduction</title>
<p>Survey methods are a widely used approach to collecting data on nonprofit organizations and the people who run them. A review of the most recent (2010 and 2011) issues of <italic>Nonprofit and Voluntary Sector Quarterly</italic> (<italic>NVSQ</italic>) demonstrates the popularity of survey methods in the field: of the 50 empirical studies (as opposed to review essays or theory pieces), more than half (30) used survey data of some kind. Among the various survey modes, web surveys (involving an emailed request to complete an online questionnaire) and mail surveys (involving self-administered paper questionnaires) appear to be the most frequently used modes to study nonprofits.</p>
<p>Because survey mode is an important factor in the success of a study, in terms of response rate and data quality, it is important to better understand the strengths and weaknesses of different modes (<xref ref-type="bibr" rid="bibr13-0899764011423840">Groves et al., 2009</xref>). However, very few empirical studies have examined survey methods and related data quality issues in the context of nonprofit research.</p>
<p>The purpose of this study is therefore to compare two of the most common modes of surveying nonprofits, web and mail surveys, and to explore strategies that can help improve survey quality in the field. Following <xref ref-type="bibr" rid="bibr4-0899764011423840">Cole’s (2005)</xref> study comparing mail and web-based survey methods in tourism research, this study is trying to address the following research questions: Does the web survey mode generate the same response rate as the traditional mail survey mode when gathering data on nonprofits? Do respondents to the web survey have the same organizational or/and personal characteristics as those who respond to the mail survey? Does the web survey have the same data quality as the regular mail survey, particularly in terms of item nonresponse? Do scales in the web survey have the same levels of internal consistency as those in the regular mail survey? And finally, do responses generated in the web survey have the same mean values on study variables as those generated in the mail survey?</p>
</sec>
<sec id="section2-0899764011423840">
<title>Background</title>
<p>Web surveys have a number of advantages over traditional mail surveys. Financially, web surveys are usually associated with relatively lower logistical and personnel costs (<xref ref-type="bibr" rid="bibr5-0899764011423840">Couper, 2000</xref>; <xref ref-type="bibr" rid="bibr8-0899764011423840">Dolnicar, Laesser, &amp; Matus, 2009</xref>; <xref ref-type="bibr" rid="bibr9-0899764011423840">Fricker &amp; Schonlau, 2002</xref>; <xref ref-type="bibr" rid="bibr15-0899764011423840">Jones &amp; Pitt, 1999</xref>; <xref ref-type="bibr" rid="bibr17-0899764011423840">Kwak &amp; Radler, 2002</xref>; <xref ref-type="bibr" rid="bibr20-0899764011423840">Naus, Philipp, &amp; Samsi, 2009</xref>). Previous studies also suggest that web surveys can be administered faster than conventional mail surveys, allowing more rapid analysis of the responses (<xref ref-type="bibr" rid="bibr5-0899764011423840">Couper, 2000</xref>; <xref ref-type="bibr" rid="bibr8-0899764011423840">Dolnicar et al., 2009</xref>; <xref ref-type="bibr" rid="bibr15-0899764011423840">Jones &amp; Pitt, 1999</xref>). A significant portion of the research on various survey methods has mainly focused on comparing web surveys and traditional mail surveys in three broad areas of inquiry: response rate, respondent profile, and data quality (<xref ref-type="bibr" rid="bibr17-0899764011423840">Kwak &amp; Radler, 2002</xref>).</p>
<p>In terms of survey response rate, which is critical to the representativeness and generalizability of survey findings, most studies find that paper-based methods tend to obtain higher response rates than their electronic equivalents (<xref ref-type="bibr" rid="bibr13-0899764011423840">Groves et al., 2009</xref>). <xref ref-type="bibr" rid="bibr9-0899764011423840">Fricker and Schonlau (2002)</xref> examined literature generated from two social sciences databases and found that web surveys generally do not achieve response rates equal to those of mail surveys. <xref ref-type="bibr" rid="bibr18-0899764011423840">Manfreda, Bosnjak, Berzelak, Hass, and Vehovar (2008)</xref> review of 26 research articles comparing web survey and mail survey response rates identified from multiple sources revealed a similar pattern. A few studies have shown that web surveys can achieve comparable response rates to mail surveys (<xref ref-type="bibr" rid="bibr16-0899764011423840">Kaplowitz, Hadlock, &amp; Levine, 2004</xref>; <xref ref-type="bibr" rid="bibr19-0899764011423840">McCabe, Boyd, Young, Crawford, and Pope, 2005</xref>; Weible &amp; Wallace, 1998 in <xref ref-type="bibr" rid="bibr18-0899764011423840">Manfreda et al., 2008</xref>). A review of tourism surveys conducted by <xref ref-type="bibr" rid="bibr8-0899764011423840">Dolnicar and his colleagues (2009)</xref> found that web surveys demonstrated higher return rates than mail surveys. Six of the 26 papers reviewed by <xref ref-type="bibr" rid="bibr18-0899764011423840">Manfreda et al. (2008)</xref> also reported higher response rates for web surveys. <xref ref-type="bibr" rid="bibr14-0899764011423840">Hager, Wilson, Pollak, and Rooney (2003)</xref> surveyed a sample of nonprofit organizational mail survey studies published in an academic journal and found that, like other types of organizational surveys, surveys of nonprofit organizations have low response rates. There have been no published studies to date comparing response rates of different survey modes in nonprofit research.</p>
<p>Previous studies have attempted to examine demographic profiles of survey respondents to see whether web respondents have comparable characteristics to those of mail respondents. <xref ref-type="bibr" rid="bibr10-0899764011423840">Friedman, Clusen, and Hartzell (2004)</xref> compared web and mail respondents to a health care survey and found that web survey respondents tend to be younger, White, male, active duty beneficiaries, and to live outside of the United States. <xref ref-type="bibr" rid="bibr17-0899764011423840">Kwak and Radler’s (2002)</xref> survey to university students revealed that the web survey involved fewer female and younger respondents. <xref ref-type="bibr" rid="bibr2-0899764011423840">Bech and Kristensen’s (2009)</xref> survey of an older population in Denmark revealed that female are less likely to respond to web surveys than males. However, <xref ref-type="bibr" rid="bibr23-0899764011423840">Schillewaert and Meulemeester (2005)</xref> concluded that the difference between web and mail survey modes in terms of sociodemographics was only minor.</p>
<p>Survey researchers also have compared the quality of data generated from different survey modes, with the most frequently employed criteria of data quality being the number of incomplete or partially completed surveys and the percentage of missing items (item nonresponse). Web surveys have been found to have a lower rate of item nonresponse (<xref ref-type="bibr" rid="bibr22-0899764011423840">Schaefer &amp; Dillman, 1998</xref>). <xref ref-type="bibr" rid="bibr4-0899764011423840">Cole (2005)</xref> reviewed some previous survey studies which consistently reported that web survey responses had fewer missing items than paper surveys. Other studies (<xref ref-type="bibr" rid="bibr2-0899764011423840">Bech &amp; Kristensen, 2009</xref>; <xref ref-type="bibr" rid="bibr10-0899764011423840">Friedman et al., 2004</xref>) found web surveys not only have significantly fewer item nonresponses but also fewer “don’t know” answers than mail surveys. Some researchers (<xref ref-type="bibr" rid="bibr19-0899764011423840">McCabe et al., 2005</xref>) reported that their study did not find statistical difference in data quality, as measured by the number of partially completed questionnaires and the overall rate of item nonresponse between the two survey modes.</p>
<p>In addition to the three areas discussed above, some studies have compared the substantive answers to survey questions obtained from web and mail surveys. However, findings from these studies are mixed. Although some have revealed that web surveys produce similar responses to those obtained from mail surveys, others have shown evidence of significant differences in responses generated from the two different survey modes. For example, <xref ref-type="bibr" rid="bibr20-0899764011423840">Naus et al. (2009)</xref> did not find significant differences for variables and scales between web and mail surveys to female university students. Likewise, a survey of third- and fourth-grade students that asked about alcohol and tobacco use (<xref ref-type="bibr" rid="bibr19-0899764011423840">McCabe et al., 2005</xref>) revealed very few statistical differences in the distributions of variables between the two survey modes. Results from a survey to travel retailers (<xref ref-type="bibr" rid="bibr4-0899764011423840">Cole, 2005</xref>) showed that web and mail modes produced different reliability scores for multi-item scales, with web mode having higher reliability on some scales but lower reliability on others.</p>
<p>In sum, although web surveys offer cost and speed advantages over traditional mail surveys, findings from methodological research in various settings seem to suggest that web surveys may suffer from somewhat lower response rates. However, web surveys appear to have an advantage over mail surveys in terms of item nonresponse. And with respect to substantive differences in the distribution of study variables, the evidence comparing the two modes is mixed. But most of this methodological research has focused on surveys of individuals or households, rather than organizations, and none of the studies to date has explicitly looked at mode differences in the context of surveying nonprofit organizations. Thus, given the importance of surveys in empirical nonprofit research, combined with evidence of some important mode differences in other types of surveys, we believe it is important to examine these issues in the nonprofit context.</p>
</sec>
<sec id="section3-0899764011423840" sec-type="methods">
<title>Method</title>
<sec id="section4-0899764011423840">
<title>Sample and Data</title>
<p>To compare web and mail survey modes in the context of nonprofit research, this study uses data from a study of nonprofit executive directors drawn from a sampling frame of New Jersey nonprofits obtained from GuideStar Premium. Human services and community improvement organizations were selected because of their visibility among all types of nonprofits, their traditional and relatively heavy reliance on government as a major funding source, and that “their provision of similar goods and services are primarily consumed by local residents” (<xref ref-type="bibr" rid="bibr12-0899764011423840">Grønbjerg, 1993</xref>). These organizations include eight types: crime and legal related; employment; food, agriculture and nutrition; housing and shelter; public safety, disaster preparedness and relief; youth development; multipurpose human services; and community improvement and capacity building. Having obtained a sampling frame from GuideStar Premium, we verified the address, email address, and other contact information by checking other sources, such as the organization’s official website.</p>
<p>We downloaded from GuideStar Premium information on all organizations that meet the following four criteria: (a) the organization is located in the State of New Jersey; (b) the organization’s NTEE code is one of the eight groups identified as human services and community improvement organizations; (c) IRS subsection of the organization is defined as 501(c)3 Public Charity; and (d) the organization’s annual income (for the most recent record year available in the GuideStar Premium database) was at least US$200,000. In addition, we cleaned the resulting list by deleting organizations whose NTEE codes do not fall into any of the eight groups, removing duplicates, and reassigning organizations that were misclassified. Because the data were being collected primarily for a study looking at revenue diversification, the sample was further trimmed of nonprofits that had mostly fee-driven income, such as labor unions, day care centers, and senior/retirement housing organizations. Finally, we removed organizations that failed to report with the IRS since 2006, because of concerns that they may no long be in existence or that their more recent revenues had dropped below IRS reporting requirements (US$25,000). These various procedures yielded a final list sample of 1,115 organizations.</p>
<p>We were able to obtain the name and email address for 320 executive directors or chief executive officers, out of the total cleaned sample of 1,115 organizations. We randomly split these 320 organizations into two experimental groups: one assigned to the web survey (<italic>n</italic> = 160) and the other to the mail survey (<italic>n</italic> = 160). The remaining 795 organizations, for which no email address was available, were sent the mail survey as part of the larger study. However, for purposes of the methodological experiment reported here, the analysis focuses only on those 320 organizations with complete email contact information that could thus be randomly assigned to the two survey modes (<xref ref-type="bibr" rid="bibr13-0899764011423840">Groves et al., 2009</xref>). <xref ref-type="table" rid="table1-0899764011423840">Table 1</xref> provides the distributions of organizations by NTEE category in the large sample as well as in this study sample (experimental sample).</p>
<table-wrap id="table1-0899764011423840" position="float">
<label>Table 1.</label>
<caption>
<p>Study Sample by NTEE Category</p>
</caption>
<graphic alternate-form-of="table1-0899764011423840" xlink:href="10.1177_0899764011423840-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">NTEE category</th>
<th align="center">Large sample</th>
<th align="center">Study sample</th>
</tr>
</thead>
<tbody>
<tr>
<td>I. Crime &amp; legal-related</td>
<td>46 (4.1%)</td>
<td>14 (4.4%)</td>
</tr>
<tr>
<td>J. Employment</td>
<td>48 (4.3%)</td>
<td>6 (1.9%)</td>
</tr>
<tr>
<td>K. Food, agriculture &amp; nutrition</td>
<td>33 (3%)</td>
<td>10 (3.1%)</td>
</tr>
<tr>
<td>L. Housing &amp; shelter</td>
<td>186 (16.6%)</td>
<td>31 (9.7%)</td>
</tr>
<tr>
<td>M. Public safety, disaster preparedness &amp; relief</td>
<td>23 (2.1%)</td>
<td>4 (1.3%)</td>
</tr>
<tr>
<td>O. Youth development</td>
<td>95 (8.5%)</td>
<td>48 (15%)</td>
</tr>
<tr>
<td>P. Human services</td>
<td>527 (47.3%)</td>
<td>171 (53.3%)</td>
</tr>
<tr>
<td>S. Community improvement &amp; capacity building</td>
<td>157 (14.1%)</td>
<td>36 (11.3%)</td>
</tr>
<tr>
<td>Total</td>
<td>1,115</td>
<td>320</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section5-0899764011423840">
<title>Survey Instrument</title>
<p>The survey instrument addressed nonprofit financial strategies and organizational performance. The questionnaire was finalized after being piloted with a number of nonprofit professionals. The survey instrument comprised 35 mostly closed-ended questions. There are five sections in the questionnaire: Section 1 asked about basic organizational characteristics, including service field, geographic location, organizational size, organizational age, board size, number of board meetings each year, and number of fund-raising professionals. Section 2 asked about the organization’s funding environment, such as annual budget and revenue sources, and also the perceived difficulty in obtaining revenues. Section 3 asked respondents to share their fund-raising experiences and strategies. Section 4 asked respondents to self-report their organizational performance. And Section 5 asked demographic questions, such as gender, age, job title, and tenure with their organization. Pretesting suggested that the questionnaire took approximately 15 minutes to complete. Respondents were allowed to consult with their colleagues for assistance in answering survey questions.</p>
<p>The web and mail versions of the questionnaire were identical in terms of question content. We designed, programmed, and posted the web version of the questionnaire in SurveyMonkey (<ext-link ext-link-type="uri" xlink:href="http://SurveyMonkey.com">SurveyMonkey.com</ext-link>), then downloaded the questionnaire as a PDF for use in the mail survey. Thus, the mail survey and web survey had essentially the same format and were highly comparable visually. We did not make survey questions mandatory. We disabled “require answer to question” feature when programming survey questions, so web survey respondents were able to skip questions just like when they complete paper surveys. We also set up the web survey in a way that allowed respondents to move backward to a previous page. The respondents were therefore able to go back to review or edit their answers any time in the survey or after the survey was complete.</p>
</sec>
<sec id="section6-0899764011423840">
<title>Administration of Mail and Web Surveys</title>
<p><xref ref-type="bibr" rid="bibr3-0899764011423840">Berry, Arons, Bass, Carter, and Portney (2003)</xref> suggested that nonprofit survey researchers make some logistical effort such as adopting multiple mailings to obtain good response rates. We used a modified version of <xref ref-type="bibr" rid="bibr7-0899764011423840">Dillman, Smyth, and Christian (2009)</xref> method that involved two mailings of the complete questionnaire (plus a cover letter), with one reminder post card in between. First, we sent a paper questionnaire, along with a personalized cover letter (including the individual’s name) and a business reply envelope. Two weeks after the first survey mailing, we sent a reminder postcard to all nonrespondents. After another 2 weeks, we sent a second letter, a replacement questionnaire, and another business reply envelope enclosed to all organizations who had not responded by that time.</p>
<p>For the web survey group, we also made three contact attempts. First, we sent an initial email invitation with language very similar to that of the cover letter used in the mail survey, with only minor modifications in wording to fit the web survey situation. As with the cover letter in the mail survey, we personalized these email messages with the name of the individual. Respondents were instructed to click on an embedded URL address link provided in the email text that would direct them to the survey webpage hosted on SurveyMonkey. Six days after the initial invitation email, we sent a follow-up email to all nonrespondents as well as those who only partially completed the survey. After another 6 days, we sent a final follow-up email to all remaining nonrespondents. Again, the wording of these messages was essentially the same as the wording in the follow-up cover letters used in the mail survey.</p>
</sec>
</sec>
<sec id="section7-0899764011423840" sec-type="results">
<title>Results</title>
<p>Using chi square tests and two-sample <italic>t-</italic>tests (for categorical and quantitative outcomes, respectively), we compared the results of the web survey and mail survey in terms of response rates, organization and respondent characteristics, item nonresponse, internal consistency of scales, and mean values for the substantive study variables.</p>
<sec id="section8-0899764011423840">
<title>Response Rates</title>
<p>The response rate is an important factor in assessing the value of organizational survey research findings (<xref ref-type="bibr" rid="bibr1-0899764011423840">Baruch &amp; Holtom, 2008</xref>). The overall response rate for the survey is 53.4% (171/320). A number of reasons might explain why the survey achieved such a respectable return. First, nonprofit organizations in New Jersey might identify strongly with this study which was conducted by researchers affiliated Rutgers, the State University of New Jersey. Second, we personalized survey invitations (in our experimental study). Berry and his colleagues (2003) suggested that letters and emails with a specific name are more likely to be perceived as important which worked for the response rate. Third, the survey was administered at a time when the American economy was experiencing the most severe recession since the Great Depression and nonprofits were financially stressed. Nonprofit executives were highly likely to be attracted to the purpose of the study—examining how certain revenue patterns could hopefully help nonprofits survive the economic downturn. Finally, we designed the survey to make it as respondent friendly as possible so it would be easier for respondents to answer the questions and complete the survey.</p>
<p>Consistent with previous studies in other settings (<xref ref-type="bibr" rid="bibr6-0899764011423840">Couper &amp; Miller, 2008</xref>), this study showed (in <xref ref-type="table" rid="table2-0899764011423840">Table 2</xref>) that the mail survey achieved a much higher response rate (62.5%) than did the web survey (44.4%). This difference is statistically significant (<italic>p</italic> = .001) and substantively important as well, in terms of both the larger number of observations available for analysis and the more respectable overall rate. One of the major concerns of low response rates in surveys is that a lower response rate might harm the representativeness of the data due to nonresponse bias. To assess whether the generalizability of the collected data in the web survey is lower than that in the mail survey, we compared the characteristics of the responding organizations and those of survey respondents.</p>
<table-wrap id="table2-0899764011423840" position="float">
<label>Table 2.</label>
<caption>
<p>Comparison of Response Rates</p>
</caption>
<graphic alternate-form-of="table2-0899764011423840" xlink:href="10.1177_0899764011423840-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center" colspan="2">Web<hr/></th>
<th align="center" colspan="2">Mail<hr/></th>
<th/>
<th/>
</tr>
<tr>
<th/>
<th align="center"><italic>N</italic></th>
<th align="center">%</th>
<th align="center"><italic>n</italic></th>
<th align="center">%</th>
<th align="center">Chi sq</th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td/>
<td/>
<td/>
<td/>
<td/>
<td>10.56</td>
<td>.001</td>
</tr>
<tr>
<td>Responded</td>
<td>71</td>
<td>44.4</td>
<td>100</td>
<td>62.5</td>
<td/>
<td/>
</tr>
<tr>
<td>Not responded</td>
<td>89</td>
<td>55.6</td>
<td>60</td>
<td>37.5</td>
<td/>
<td/>
</tr>
<tr>
<td>Total</td>
<td>160</td>
<td>100</td>
<td>160</td>
<td>100</td>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section9-0899764011423840">
<title>Organization and Respondent Characteristics</title>
<p>It is important and necessary to compare whether different survey modes generate responses from different types of respondents. Thus <xref ref-type="table" rid="table3-0899764011423840">Table 3</xref> shows the comparison of the web and mail survey respondents in terms of both organizational and individual respondent characteristics. The two groups of responding organizations are quite similar in terms of their budgetary size and service field. And the individuals are similar as well, in terms of gender, age, and tenure with their organizations. Of course, the groups were formed by random assignment and thus there should be no initial differences in such characteristics. But these results suggest that, although the response rate to the web survey was lower, there does not appear to be much of a systematic difference in the types of organizations and individuals who responded in each group, at least in terms of observed characteristics.</p>
<table-wrap id="table3-0899764011423840" position="float">
<label>Table 3.</label>
<caption>
<p>Comparison of Responding Organizations and Respondent Demographics</p>
</caption>
<graphic alternate-form-of="table3-0899764011423840" xlink:href="10.1177_0899764011423840-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Web<hr/></th>
<th align="center">Mail<hr/></th>
<th/>
<th/>
</tr>
<tr>
<th align="center">Variables</th>
<th align="center">Mean (<italic>SD</italic>)</th>
<th align="center">Mean (<italic>SD</italic>)</th>
<th align="center"><italic>t</italic></th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Organizational budget (in millions)</td>
<td>3.9 (6.2)</td>
<td>3.9 (6.8)</td>
<td>−0.001</td>
<td>.999</td>
</tr>
<tr>
<td colspan="5"><hr/></td>
</tr>
<tr>
<th/>
<th align="center"><italic>n</italic> (%)</th>
<th align="center"><italic>n</italic> (%)</th>
<th align="center">Chi sq</th>
<th align="center"><italic>p</italic></th>
</tr>
<tr>
<td colspan="5"><hr/></td>
</tr>
<tr>
<td>Organizational service field</td>
<td/>
<td/>
<td>2.13</td>
<td>.<italic>953</italic></td>
</tr>
<tr>
<td> HS</td>
<td>40 (56.3)</td>
<td>47 (47.0)</td>
<td/>
<td/>
</tr>
<tr>
<td> Youth</td>
<td>9 (12.7)</td>
<td>15 (15.0)</td>
<td/>
<td/>
</tr>
<tr>
<td> Housing</td>
<td>7 (9.9)</td>
<td>11 (11.0)</td>
<td/>
<td/>
</tr>
<tr>
<td> Community</td>
<td>7 (9.9)</td>
<td>13 (13.0)</td>
<td/>
<td/>
</tr>
<tr>
<td> All other</td>
<td>8 (11.2)</td>
<td>14 (14.0)</td>
<td/>
<td/>
</tr>
<tr>
<td colspan="5"><hr/></td>
</tr>
<tr>
<th/>
<th align="center">Mean (<italic>SD</italic>)</th>
<th align="center">Mean (<italic>SD</italic>)</th>
<th align="center"><italic>t</italic></th>
<th align="center"><italic>p</italic></th>
</tr>
<tr>
<td colspan="5"><hr/></td>
</tr>
<tr>
<td>Respondent’s tenure with current organization (years)</td>
<td>10.7 (8.8)</td>
<td>11.1 (4.9)</td>
<td>0.283</td>
<td>.777</td>
</tr>
<tr>
<td colspan="5"><hr/></td>
</tr>
<tr>
<th/>
<th align="center"><italic>n</italic> (%)</th>
<th align="center"><italic>n</italic> (%)</th>
<th align="center">Chi sq</th>
<th align="center"><italic>p</italic></th>
</tr>
<tr>
<td colspan="5"><hr/></td>
</tr>
<tr>
<td>Respondent’s gender</td>
<td/>
<td/>
<td>0.004</td>
<td>.948</td>
</tr>
<tr>
<td> Male</td>
<td>35 (50.0)</td>
<td>48 (49.0)</td>
<td/>
<td/>
</tr>
<tr>
<td>Respondent’s age</td>
<td/>
<td/>
<td>2.54</td>
<td>.638</td>
</tr>
<tr>
<td> 25-34</td>
<td>4 (5.6)</td>
<td>4 (4.1)</td>
<td/>
<td/>
</tr>
<tr>
<td> 35-54</td>
<td>34 (47.9)</td>
<td>48 (49.0)</td>
<td/>
<td/>
</tr>
<tr>
<td> 55 and above</td>
<td>33 (46.4)</td>
<td>46 (46.9)</td>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section10-0899764011423840">
<title>Item Nonresponse</title>
<p>Questionnaire completeness is an important indicator of data quality that can be measured as (a) the percentage of fully (as opposed to partially) complete questionnaires, or (b) the average percentage of missing items (<xref ref-type="bibr" rid="bibr8-0899764011423840">Dolnicar et al., 2009</xref>; <xref ref-type="bibr" rid="bibr9-0899764011423840">Fricker &amp; Schonlau, 2002</xref>; <xref ref-type="bibr" rid="bibr19-0899764011423840">McCabe et al., 2005</xref>). The questionnaire in this study had a total of 94 response items, so a fully complete questionnaire has recorded response for all 94 items (including “not applicable (N/A)” as a recorded response). As <xref ref-type="table" rid="table4-0899764011423840">Table 4</xref> shows, only 21% of web survey responses and 20% of mail survey responses had fully complete questionnaires, but this difference between the groups is not statistically significant. Compared to high response rates (63% for mail survey and 44% for web survey); the percentages of fully complete responses are strikingly low. It needs to be noted that we counted as valid responses those that answered the survey to the last question. We however did not count as valid responses those that returned the paper survey but did not answer any questions or clicked on the email links but did not answer any questions. We also did not count as valid responses those that stopped abruptly after answering only the first questions at the beginning of the survey. One possible explanation of the low “fully complete response rate” is that our survey included 32 items that have “not applicable (N/A)” option. Some respondents might have not answered these questions if they considered them to be not applicable, and their nonresponse to these questions significantly reduced the “fully complete response rate.”</p>
<table-wrap id="table4-0899764011423840" position="float">
<label>Table 4.</label>
<caption>
<p>Comparison of Survey Completion</p>
</caption>
<graphic alternate-form-of="table4-0899764011423840" xlink:href="10.1177_0899764011423840-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Web<hr/></th>
<th align="center">Mail<hr/></th>
<th/>
<th/>
</tr>
<tr>
<th/>
<th align="center"><italic>n</italic> (%)</th>
<th align="center"><italic>n</italic> (%)</th>
<th align="center">Chi sq</th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Completion</td>
<td/>
<td/>
<td>0.032</td>
<td>.857</td>
</tr>
<tr>
<td> Fully completed</td>
<td>15 (21)</td>
<td>20 (20)</td>
<td/>
<td/>
</tr>
<tr>
<td> Partially completed</td>
<td>56 (79)</td>
<td>80 (80)</td>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
<p>Similarly, there is no significant difference between the two groups in terms of the percentage of missing items, although the mean percent of missing items for mail survey respondents is smaller than that of web survey respondents (see <xref ref-type="table" rid="table5-0899764011423840">Table 5</xref>). Thus, the two survey modes are quite similar in terms of the level of item nonresponse.</p>
<table-wrap id="table5-0899764011423840" position="float">
<label>Table 5.</label>
<caption>
<p>Comparison of Missing Items</p>
</caption>
<graphic alternate-form-of="table5-0899764011423840" xlink:href="10.1177_0899764011423840-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">% of missing items</th>
<th align="center">Web</th>
<th align="center">Mail</th>
<th align="center"><italic>t</italic></th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Mean</td>
<td>12 (7.4)</td>
<td>11.9 (5.33)</td>
<td>0.51</td>
<td>.613</td>
</tr>
<tr>
<td colspan="5">Quantiles</td>
</tr>
<tr>
<td> Min</td>
<td>0</td>
<td>0</td>
<td/>
<td/>
</tr>
<tr>
<td> 0.25</td>
<td>6</td>
<td>6</td>
<td/>
<td/>
</tr>
<tr>
<td> Mdn</td>
<td>11</td>
<td>11</td>
<td/>
<td/>
</tr>
<tr>
<td> 0.75</td>
<td>16</td>
<td>16</td>
<td/>
<td/>
</tr>
<tr>
<td> Max</td>
<td>33</td>
<td colspan="3">39</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section11-0899764011423840">
<title>Internal Consistency of Scales</title>
<p>Internal consistency is an important measure of reliability that assesses the consistency of multiple items that are designed to measure the same construct (Remler &amp; Van Ryzin, 2010; <xref ref-type="bibr" rid="bibr25-0899764011423840">Trochim, 2001</xref>). A frequently used measure of internal consistency reliability is Cronbach’s alpha. A larger alpha value means the response to the scale items are more closely correlated with each other and thus a more consistent measure of the latent construct. In our case, the main study involved several multi-item scales that were developed to measure nonprofit characteristics that might potentially influence the level of revenue diversification and organizational performance. These scales include the following: managerial success (five items), internal development (three items), and external relationships (two items). Factor analysis was conducted on the entire sample to examine the dimensionality of each scale. <xref ref-type="table" rid="table6-0899764011423840">Table 6</xref> presents the Cronbach’s alpha for each scale separately for the web survey group and the mail survey group. The results show that the internal consistency (reliability) of all three scales was higher in the mail survey group than in the web survey group.</p>
<table-wrap id="table6-0899764011423840" position="float">
<label>Table 6.</label>
<caption>
<p>Reliability Coefficients of Scales in Web and Mail Surveys</p>
</caption>
<graphic alternate-form-of="table6-0899764011423840" xlink:href="10.1177_0899764011423840-table6.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center">Web</th>
<th align="center">Mail</th>
</tr>
</thead>
<tbody>
<tr>
<td>Scale 1 (Managerial success)</td>
<td>.7990</td>
<td>.8769</td>
</tr>
<tr>
<td>Scale 2 (Internal development)</td>
<td>.8703</td>
<td>.9011</td>
</tr>
<tr>
<td>Scale 3 (External relationships)</td>
<td>.4135</td>
<td>.6488</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section12-0899764011423840">
<title>Mean Values of Study Variables</title>
<p>We compared mean values of selected variables across the two different survey modes to see if there were any differences between the mail and web surveys in terms of the substantive levels of the study variables. <xref ref-type="table" rid="table7-0899764011423840">Table 7</xref> presents the mean values of the three scales just discussed, as well as the mean values for five single items that were the primary dependent variables for the original study. The results indicate few differences in these substantive variables across survey modes. Respondents to the web and mail surveys showed no significant difference on mean scores of the three scales. And the two groups of respondents did not differ significantly on most of the five major dependent variables, with the exception on “Funding Success.” Web survey respondents reported a higher level of funding success than mail survey respondents (<italic>p</italic> = .037).</p>
<table-wrap id="table7-0899764011423840" position="float">
<label>Table 7.</label>
<caption>
<p>Comparison of Means for Selected Study Variables</p>
</caption>
<graphic alternate-form-of="table7-0899764011423840" xlink:href="10.1177_0899764011423840-table7.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th/>
<th align="center"><italic>N</italic></th>
<th align="center">Mean</th>
<th align="center"><italic>SD</italic></th>
<th align="center"><italic>t</italic></th>
<th align="center"><italic>p</italic></th>
</tr>
</thead>
<tbody>
<tr>
<td>Scale 1 (managerial success)</td>
<td/>
<td/>
<td/>
<td>1.125</td>
<td>.262</td>
</tr>
<tr>
<td> Web</td>
<td>68</td>
<td>0.151</td>
<td>0.831</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>98</td>
<td>−0.028</td>
<td>1.113</td>
<td/>
<td/>
</tr>
<tr>
<td>Scale 2 (internal development)</td>
<td/>
<td/>
<td/>
<td>−0.082</td>
<td>.935</td>
</tr>
<tr>
<td> Web</td>
<td>71</td>
<td>−0.054</td>
<td>0.983</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>99</td>
<td>−0.042</td>
<td>1.053</td>
<td/>
<td/>
</tr>
<tr>
<td>Scale 3 (external relationships)</td>
<td/>
<td/>
<td/>
<td>0.598</td>
<td>.551</td>
</tr>
<tr>
<td> Web</td>
<td>71</td>
<td>0.363</td>
<td>0.932</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>99</td>
<td>0.277</td>
<td>0.92</td>
<td/>
<td/>
</tr>
<tr>
<td>Funding variety</td>
<td/>
<td/>
<td/>
<td>0.336</td>
<td>.738</td>
</tr>
<tr>
<td> Web</td>
<td>66</td>
<td>4.833</td>
<td>1.697</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>94</td>
<td>4.745</td>
<td>1.606</td>
<td/>
<td/>
</tr>
<tr>
<td>Revenue balance</td>
<td/>
<td/>
<td/>
<td>−0.441</td>
<td>.660</td>
</tr>
<tr>
<td> Web</td>
<td>66</td>
<td>0.467</td>
<td>0.290</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>94</td>
<td>0.488</td>
<td>0.315</td>
<td/>
<td/>
</tr>
<tr>
<td>Financial stress</td>
<td/>
<td/>
<td/>
<td>−0.115</td>
<td>.909</td>
</tr>
<tr>
<td> Web</td>
<td>69</td>
<td>6.406</td>
<td>2.439</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>98</td>
<td>6.449</td>
<td>2.364</td>
<td/>
<td/>
</tr>
<tr>
<td>Maintain programs and services</td>
<td/>
<td/>
<td/>
<td>1.551</td>
<td>.123</td>
</tr>
<tr>
<td> Web</td>
<td>71</td>
<td>6.943</td>
<td>0.286</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>99</td>
<td>6.323</td>
<td>0.269</td>
<td/>
<td/>
</tr>
<tr>
<td>Future funding success</td>
<td/>
<td/>
<td/>
<td>2.105</td>
<td>.037</td>
</tr>
<tr>
<td> Web</td>
<td>71</td>
<td>4.057</td>
<td>1.123</td>
<td/>
<td/>
</tr>
<tr>
<td> Mail</td>
<td>99</td>
<td>3.720</td>
<td>0.956</td>
<td/>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section13-0899764011423840">
<title>Costs</title>
<p>Finally, costs were examined. Each mail survey and accompanying letter required US$1.05 postage, with a total of US$299.05 for 285 surveys (initial and third mailings). Each postcard required US$0.28 postage, a total of US$43.96 for 157 postcards (second mailing). Each business reply required US$0.44, a total of US$44 for all 100 returned surveys. The total postage for the mail survey was US$443.01. Other logistical costs for mail survey included envelops, postcards, business reply envelops, mailing labels, and letter head papers. Postage costs did not apply to web survey participants. The only cost for web surveys was subscription to the online survey tool, which was US$16.99 per month, a total of US$51 for 3 months. Cost per response was US$4.43 for the mail survey (only postage is calculated) and US$0.72 for the web survey (only subscription is calculated).</p>
<p>Other costs pertaining to labor time are more difficult to determine (<xref ref-type="bibr" rid="bibr11-0899764011423840">Greenlaw &amp; Brown-Welty, 2009</xref>). The creation of the web survey sample required time to collect and verify relevant information. It also took a large volume of time to prepare surveys for mailing (i.e., printing questionnaires and postcards, printing mailing labels, and stuffing envelopes). In addition, once the survey was returned, the data were manually entered into a database, which consumed more time than with the web survey that automatically generated a database after the survey was closed. Although labor cost was not calculated, it was clear from our experience that more labor was dedicated to the mail survey administration than that to the web survey administration.</p>
</sec>
</sec>
<sec id="section14-0899764011423840" sec-type="discussion">
<title>Discussion</title>
<p>Findings from this experimental study clearly show that a traditional mail survey of nonprofit professionals achieved a much higher response rate than did a web survey. There are several possible explanations for this finding. It could be that nonprofit directors view a mail survey request as more legitimate, perhaps because they are accustomed to getting important documents by mail or because they receive an overload of unsolicited requests by email. It is also possible that some nonprofit professionals may simply prefer to fill out the survey on paper, rather than at their computers. There is a growing concern about web surveys that rely only on email invitations because of the proliferation of email messages of all kinds and because of the ease with which many people ignore or discard unsolicited email messages (<xref ref-type="bibr" rid="bibr7-0899764011423840">Dillman et al., 2009</xref>). Regardless of the reasons for the experimentally observed difference in response rates, these results clearly indicate that mail surveys are likely to produce higher response rates when surveying nonprofits and, if web survey methods are employed, extra efforts may well be needed to achieve comparable response rates.</p>
<p>Comparison of the profiles of responding organizations and respondents did not demonstrate any significant difference between the two modes. Although previous studies have reported certain demographic variations between mail and web survey respondents (<xref ref-type="bibr" rid="bibr10-0899764011423840">Friedman et al., 2004</xref>; <xref ref-type="bibr" rid="bibr17-0899764011423840">Kwak &amp; Radler, 2002</xref>), these differences did not emerge in the context of nonprofit organizations and their directors in this study. Thus, it appears that there is no obvious, systematic difference or potential bias related to the types of organizations and individuals that are responding, or not responding, to the web survey. On a related note, when we compare our study sample to the larger nonexperimental sample, a number of noticeable differences deserve mention. In particular, the study sample is more likely to have a male respondent (significant at .001), more likely to have a respondent that is 55 or older (significant at .10), and is more likely to be a youth or community improvement organization (significant at .05). These differences suggest certain systematic differences between the study sample and the larger nonexperimental sample, which implies somewhat of a limitation on the generalizability of the experimental findings.</p>
<p>We found that our mail survey and web survey did not differ in terms of survey completeness and percentage of missing items, two important indicators of data quality. And in terms of the levels (means) of various substantive variables in the study, we found few differences between the two survey modes. However, we did find that scales in the web survey did not display the same level of internal consistency as those in the mail survey. This difference in reliability matters in the context of nonprofit research because many measures of organizational behavior involve multi-item scales. Moreover, the lower levels of reliability in the web survey imply that these variables contain more noise (random error) and thus analytical correlations or regressions based on these variables are likely to be attenuated (<xref ref-type="bibr" rid="bibr21-0899764011423840">Remler &amp; Van Ryzin, 2011</xref>).</p>
<p>The above findings should be interpreted with some caution and study limitations in mind. Our study involved a survey of nonprofit organizations in New Jersey only, and only those organizations operating in human services and community improvement areas. The finding of an advantage for mail surveys may not generalize to other states (or other countries) and other categories of nonprofit organizations. Another limitation in our study is coverage error that might arise from the fact that we used a sample of 320 organizations that had available personal email addresses, which is only a proportion of the original sample of 1,115. In other words, the study does not consider those organizations that are less electronically accessible and might have different approaches to responding to web and mail surveys from those included in our frame sample. Finally, response rates and other outcomes, such as internal consistency and means, also depend on the particular design and content of a survey (<xref ref-type="bibr" rid="bibr13-0899764011423840">Groves et al., 2009</xref>). Other types of surveys, covering different substantive topics and involving different types of questions, could result in outcomes that vary from what we found in our study.</p>
<p>Having recognized these limitations, our study suggests some implications and directions for future methodological research involving nonprofit organizations. First, it would be important to replicate the kind of mode experiment reported here with other types of nonprofits, in other locations, and using other survey instruments. Second, future methodological studies could follow-up on the reasons why nonprofit directors seem to prefer mail surveys, if indeed this finding is confirmed. It would be desirable, for example, to conduct qualitative studies to explore nonprofit professionals’ attitudes toward different survey modes. Finally, we believe it would be extremely useful to test the potential of mixed-mode surveys for studying nonprofits, as such an approach allows the respondents to choose their particular mode preference (<xref ref-type="bibr" rid="bibr24-0899764011423840">Shannon &amp; Bradshaw, 2002</xref>). Mixed-mode surveys have been proposed to be as cost efficient and successful in improving survey response rates (<xref ref-type="bibr" rid="bibr7-0899764011423840">Dillman et al., 2009</xref>; <xref ref-type="bibr" rid="bibr11-0899764011423840">Greenlaw &amp; Brown-Welty, 2009</xref>). Due to time constraint and lack of resources, this study was not able to use a multimode approach.</p>
<p>In summary, the results of our mode experiment involving nonprofits lends support for findings from other fields that web surveys generate lower response rates than mail surveys, yet with small differences between these modes in terms of data quality and in the distributions of respondent characteristics and substantive study variables. These results warrant further research to investigate the extent of mode effects on nonprofit survey research. It would be desirable to replicate this study with more general nonprofit populations and by using different study designs that could include mixed modes.</p>
</sec>
<sec id="section15-0899764011423840" sec-type="conclusions">
<title>Conclusion</title>
<p>Comparing mail and web surveys in the nonprofit context is important because surveys are a basic source of data in the study of nonprofits and because mode differences may influence data quality and substantive findings in the field. Our experimental study found that the mail survey achieved a significantly higher response rate than the web survey, suggesting that mail surveys may produce more representative and generalizable results. Survey data obtained from the mail survey also produced measures with greater reliability than those obtained from the web survey. There was no difference between methods, however, in respondent characteristics, the completeness of the survey, and the percentage of missing items. As for some of the logistical concerns, our experience suggests that the mail survey was more expensive to administer and required more labor costs. Taken together, the findings imply that a mail survey, although more costly, may have response-rate and data-quality advantages over a web survey as a methodology for gathering data from nonprofit organizations.</p>
</sec>
</body>
<back>
<ack>
<p>The authors gratefully acknowledge Jeffrey L. Brudney and the anonymous reviewers for their helpful comments.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure">
<label>Funding</label>
<p>The author(s) received no financial support for the research, authorship, and/or publication of this article.</p>
</fn>
</fn-group>
<bio>
<title>Bios</title>
<p><bold>Weiwei Lin</bold>, PhD, is an assistant professor in the Department of Public Policy, University of Massachusetts Dartmouth. Her research focuses on nonprofit management, public and nonprofit performance measurement, and program and policy evaluation.</p>
<p><bold>Gregg G. Van Ryzin</bold>, PhD, is an associate professor in the School of Public Affairs and Administration, Rutgers University. He has published widely on citizen satisfaction with public services, program evaluation, and survey research methods and is coauthor (with Dahlia K. Remler) of <italic>Research Methods in Practice</italic> (SAGE).</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Baruch</surname><given-names>Y.</given-names></name>
<name><surname>Holtom</surname><given-names>B. C.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Survey response rate levels and trends in organizational research</article-title>. <source>Human Relations</source>, <volume>61</volume>, <fpage>1139</fpage>-<lpage>1160</lpage>.</citation>
</ref>
<ref id="bibr2-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bech</surname><given-names>M.</given-names></name>
<name><surname>Kristensen</surname><given-names>M. B.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Differential response rates in postal and web-based surveys among older respondents</article-title>. <source>Survey Research Methods</source>, <volume>3</volume>(<issue>1</issue>), <fpage>1</fpage>-<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr3-0899764011423840">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Berry</surname><given-names>J. M.</given-names></name>
<name><surname>Arons</surname><given-names>D. F.</given-names></name>
<name><surname>Bass</surname><given-names>G. D.</given-names></name>
<name><surname>Carter</surname><given-names>M. F.</given-names></name>
<name><surname>Portney</surname><given-names>K. E.</given-names></name>
</person-group> (<year>2003</year>). <source>Surveying nonprofits: A methods handbook</source>. <publisher-loc>Washington, DC</publisher-loc>: <publisher-name>Aspen Institute</publisher-name>.</citation>
</ref>
<ref id="bibr4-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cole</surname><given-names>S. T.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Comparing mail and web-based survey distribution methods: Results of surveys to leisure travel retailers</article-title>. <source>Journal of Travel Research</source>, <volume>43</volume>, <fpage>422</fpage>-<lpage>430</lpage>.</citation>
</ref>
<ref id="bibr5-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Couper</surname><given-names>M. P.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Web surveys: A review of issues and approaches</article-title>. <source>Public Opinion Quarterly</source>, <volume>64</volume>, <fpage>464</fpage>-<lpage>494</lpage>.</citation>
</ref>
<ref id="bibr6-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Couper</surname><given-names>M. P.</given-names></name>
<name><surname>Miller</surname><given-names>P. V.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Web survey methods: Introduction</article-title>. <source>Public Opinion Quarterly</source>, <volume>72</volume>, <fpage>831</fpage>-<lpage>835</lpage>.</citation>
</ref>
<ref id="bibr7-0899764011423840">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Dillman</surname><given-names>D. A.</given-names></name>
<name><surname>Smyth</surname><given-names>J. D.</given-names></name>
<name><surname>Christian</surname><given-names>L. M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Internet, mail, and mixed method surveys: The tailored design method</article-title>. <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr8-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Dolnicar</surname><given-names>S.</given-names></name>
<name><surname>Laesser</surname><given-names>C.</given-names></name>
<name><surname>Matus</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Online versus paper: Format effects in tourism surveys</article-title>. <source>Journal of Travel Research</source>, <volume>47</volume>, <fpage>259</fpage>-<lpage>316</lpage>.</citation>
</ref>
<ref id="bibr9-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fricker</surname><given-names>R. D.</given-names></name>
<name><surname>Schonlau</surname><given-names>M.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Advantages and disadvantages of internet research surveys: Evidence from the literature</article-title>. <source>Field Methods</source>, <volume>14</volume>, <fpage>347</fpage>-<lpage>367</lpage>.</citation>
</ref>
<ref id="bibr10-0899764011423840">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Friedman</surname><given-names>E. M.</given-names></name>
<name><surname>Clusen</surname><given-names>N. A.</given-names></name>
<name><surname>Hartzell</surname><given-names>M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The net effect: A comparison of internet and mail survey respondents</article-title>. Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.websm.org/uploadi/editor/1179950243Friedman_et_al_2004_Net_Effect.pdf">http://www.websm.org/uploadi/editor/1179950243Friedman_et_al_2004_Net_Effect.pdf</ext-link></citation>
</ref>
<ref id="bibr11-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Greenlaw</surname><given-names>C.</given-names></name>
<name><surname>Brown-Welty</surname><given-names>S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>A comparison of web-based and paper-based survey methods: Testing assumptions of survey mode and response cost</article-title>. <source>Evaluation Review</source>, <volume>33</volume>, <fpage>464</fpage>-<lpage>480</lpage>.</citation>
</ref>
<ref id="bibr12-0899764011423840">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Grønbjerg</surname><given-names>K. A.</given-names></name>
</person-group> (<year>1993</year>). <source>Understanding nonprofit funding: Managing revenues in social service and community development organizations</source>. <publisher-loc>San Francisco, CA</publisher-loc>: <publisher-name>Jossey-Bass</publisher-name>.</citation>
</ref>
<ref id="bibr13-0899764011423840">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Groves</surname><given-names>R. M.</given-names></name>
<name><surname>Fowler</surname><given-names>F. J.</given-names></name>
<name><surname>Couper</surname><given-names>M. P.</given-names></name>
<name><surname>Lepkowski</surname><given-names>J. M.</given-names></name>
<name><surname>Singer</surname><given-names>E.</given-names></name>
<name><surname>Tourangeau</surname><given-names>R.</given-names></name>
</person-group> (<year>2009</year>). <source>Survey methodology</source> (<edition>2nd ed.</edition>). <publisher-loc>Hoboken, NJ</publisher-loc>: <publisher-name>John Wiley</publisher-name>.</citation>
</ref>
<ref id="bibr14-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hager</surname><given-names>M. A.</given-names></name>
<name><surname>Wilson</surname><given-names>S.</given-names></name>
<name><surname>Pollak</surname><given-names>T. H.</given-names></name>
<name><surname>Rooney</surname><given-names>P. M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Response rates for mail surveys of nonprofit organizations: A review and empirical test</article-title>. <source>Nonprofit and Voluntary Sector Quarterly</source>, <volume>32</volume>, <fpage>252</fpage>-<lpage>267</lpage>.</citation>
</ref>
<ref id="bibr15-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Jones</surname><given-names>R.</given-names></name>
<name><surname>Pitt</surname><given-names>N.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Health surveys in the workplace: Comparison of postal, email, and word wide web methods</article-title>. <source>Occupational Medicine</source>, <volume>49</volume>, <fpage>556</fpage>-<lpage>558</lpage>.</citation>
</ref>
<ref id="bibr16-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kaplowitz</surname><given-names>M. D.</given-names></name>
<name><surname>Hadlock</surname><given-names>T. D.</given-names></name>
<name><surname>Levine</surname><given-names>R.</given-names></name>
</person-group> (<year>2004</year>). <article-title>A comparison of web and mail survey response rates</article-title>. <source>Public Opinion Quarterly</source>, <volume>68</volume>(<issue>1</issue>), <fpage>94</fpage>-<lpage>101</lpage>.</citation>
</ref>
<ref id="bibr17-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kwak</surname><given-names>N.</given-names></name>
<name><surname>Radler</surname><given-names>B.</given-names></name>
</person-group> (<year>2002</year>). <article-title>A Comparison between mail and web surveys: Response pattern, respondent profile, and data quality</article-title>. <source>Journal of Official Statistics</source>, <volume>18</volume>, <fpage>257</fpage>-<lpage>273</lpage>.</citation>
</ref>
<ref id="bibr18-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Manfreda</surname><given-names>K. L.</given-names></name>
<name><surname>Bosnjak</surname><given-names>M.</given-names></name>
<name><surname>Berzelak</surname><given-names>J.</given-names></name>
<name><surname>Hass</surname><given-names>I.</given-names></name>
<name><surname>Vehovar</surname><given-names>V.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Web surveys versus other survey modes—A meta-analysis comparing response rates</article-title>. <source>International Journal of Market Research</source>, <volume>50</volume>(<issue>1</issue>), <fpage>79</fpage>-<lpage>104</lpage>.</citation>
</ref>
<ref id="bibr19-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>McCabe</surname><given-names>S. E.</given-names></name>
<name><surname>Boyd</surname><given-names>C. J.</given-names></name>
<name><surname>Young</surname><given-names>A.</given-names></name>
<name><surname>Crawford</surname><given-names>S.</given-names></name>
<name><surname>Pope</surname><given-names>D.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Mode effects for collecting alcohol and tobacco data among 3rd and 4th grade students: A randomized pilot study of web-form versus paper-form surveys</article-title>. <source>Addictive Behaviors</source>, <volume>30</volume>, <fpage>663</fpage>-<lpage>671</lpage>.</citation>
</ref>
<ref id="bibr20-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Naus</surname><given-names>M. J.</given-names></name>
<name><surname>Philipp</surname><given-names>L. M.</given-names></name>
<name><surname>Samsi</surname><given-names>M.</given-names></name>
</person-group> (<year>2009</year>). <article-title>From paper to pixels: A comparison of paper and computer formats in psychological assessment</article-title>. <source>Computers in Human Behavior</source>, <volume>25</volume>, <fpage>1</fpage>-<lpage>7</lpage>.</citation>
</ref>
<ref id="bibr21-0899764011423840">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Remler</surname><given-names>D. K.</given-names></name>
<name><surname>Van Ryzin</surname><given-names>G. G.</given-names></name>
</person-group> (<year>2011</year>). <source>Research methods in practice</source>. <publisher-loc>Los Angeles, CA</publisher-loc>: <publisher-name>Sage</publisher-name>.</citation>
</ref>
<ref id="bibr22-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schaefer</surname><given-names>D. R.</given-names></name>
<name><surname>Dillman</surname><given-names>D. A.</given-names></name>
</person-group> (<year>1998</year>). <article-title>Development of a standard e-mail methodology: Results of an experiment</article-title>. <source>Public Opinion Quarterly</source>, <volume>62</volume>, <fpage>378</fpage>-<lpage>397</lpage>.</citation>
</ref>
<ref id="bibr23-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schillewaert</surname><given-names>N.</given-names></name>
<name><surname>Meulemeester</surname><given-names>P.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Comparing response distributions of offline and online data collection methods</article-title>. <source>International Journal of Market Research</source>, <volume>47</volume>,<fpage>163</fpage>-<lpage>178</lpage>.</citation>
</ref>
<ref id="bibr24-0899764011423840">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shannon</surname><given-names>D. M.</given-names></name>
<name><surname>Bradshaw</surname><given-names>C. C.</given-names></name>
</person-group> (<year>2002</year>). <article-title>A comparison of response rate, response time, and costs of mail and electronic surveys</article-title>. <source>Journal of Experimental Education</source>, <volume>70</volume>, <fpage>179</fpage>-<lpage>192</lpage>.</citation>
</ref>
<ref id="bibr25-0899764011423840">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Trochim</surname><given-names>W. M. K.</given-names></name>
</person-group> (<year>2001</year>). <source>The research methods knowledge base</source>. <publisher-loc>Cincinnati, OH</publisher-loc>: <publisher-name>Atomic Dog</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>