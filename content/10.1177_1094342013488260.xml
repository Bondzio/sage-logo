<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HPC</journal-id>
<journal-id journal-id-type="hwp">sphpc</journal-id>
<journal-title>The International Journal of High Performance Computing Applications</journal-title>
<issn pub-type="ppub">1094-3420</issn>
<issn pub-type="epub">1741-2846</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1094342013488260</article-id>
<article-id pub-id-type="publisher-id">10.1177_1094342013488260</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Special Issue Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Feature-based analysis of large-scale spatio-temporal sensor data on hybrid architectures</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Saltz</surname>
<given-names>Joel H.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488260">1</xref>
<xref ref-type="corresp" rid="corresp1-1094342013488260"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Teodoro</surname>
<given-names>George</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488260">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Pan</surname>
<given-names>Tony</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488260">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Cooper</surname>
<given-names>Lee A.D.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488260">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kong</surname>
<given-names>Jun</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488260">1</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Klasky</surname>
<given-names>Scott</given-names>
</name>
<xref ref-type="aff" rid="aff2-1094342013488260">2</xref>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Kurc</surname>
<given-names>Tahsin M.</given-names>
</name>
<xref ref-type="aff" rid="aff1-1094342013488260">1</xref>
<xref ref-type="aff" rid="aff2-1094342013488260">2</xref>
</contrib>
<bio>
<title>Author biographies</title>
<p>
<italic>Joel H. Saltz</italic> is Director of the Center for Comprehensive Informatics, Professor and Chair of the Department of Biomedical Informatics, Professor of Pathology, Biostatistics and Bioinformatics, Mathematics and Computer Science at Emory University; Adjunct Professor at Georgia Institute of Technology in the School of Computer Science and the Division of Computational Science; Georgia Research Alliance Eminent Scholar; and Georgia Cancer Coalition Distinguished Cancer Scholar. In addition, he serves as Director of the In Silico Center for Brain Tumor Research at Emory. Prior to joining Emory, Dr Saltz was Professor and Chair of the Department of Biomedical Informatics at The Ohio State University (OSU) and Davis Endowed Chair of Cancer at OSU. He served on the faculty of Johns Hopkins Medical School, University of Maryland College Park, and Yale University in the departments of Pathology and Computer Science. He received his MD and PhD (computer science) degrees at Duke University and is a board-certified Clinical Pathologist trained at Johns Hopkins University. Dr Saltz’s research objective is to develop the principles, techniques, and tools that can be used by researchers to assemble a coherent biomedical picture by integrating information from multiple, complementary data sources. Over the past 25 years he has led projects to develop innovative techniques, methodologies, algorithms, and software systems to support data federation, high-performance computing, data management, and data analyses.</p>
<p>
<italic>George Teodoro</italic> holds a PhD Degree in Computer Science from the Universidade Federal de Minas Gerais, Belo Horizonte, Brazil, and a MS Degree from the same university. His primary areas of expertise include high-performance computing runtime systems to support efficient execution of biomedical and data-mining applications on distributed heterogeneous environments. Dr Teodoro has researched innovative software solutions to enable the efficient deployment and execution of biomedical image analysis pipelines for very large multidimensional digitalized image slides.</p>
<p>
<italic>Tony Pan</italic> received a BSc in Biophysics from Brown University and a MS degree in computer science from Rensselaer Polytechnic Institute. He is a Research Software Architect in the Center for Comprehensive Informatics at Emory University. His current research interests include grid-enabled storage, management, processing, and analysis of large-scale microscopy and radiology images using distributed middleware.</p>
<p>
<italic>Lee A.D. Cooper</italic> received the PhD degree in electrical engineering from Ohio State University in 2009. He is currently an Assistant Professor in the Biomedical Informatics Department at Emory University, Atlanta. Dr Cooper's research combines microscopy imaging, genomics, and patient outcome to identify relationships between morphology, genetics, and transcription. He has developed these techniques in studies of glioblastoma tumors to identify morphology-driven disease subtypes and microenvironmental influences on transcriptional regulators.</p>
<p>
<italic>Jun Kong</italic> received the PhD degree from the Department of Electrical and Computer Engineering, Ohio State University, Columbus, in 2008. He is currently a Senior Researcher Scientist in the Center for Comprehensive Informatics, Emory University, Atlanta, GA. His research interests include computer vision, statistical machine learning, and applications to microscopy image analysis and translational oncology research.</p>
<p>
<italic>Scott Klasky</italic> is the group leader for the Scientific Data Group at Oak Ridge National Laboratory, and is an Adjunct Professor at University of Tennessee, Knoxville, TN, in the Department of Electric and Computer Science, and also at North Carolina State University, Raleigh, NC, in the Department of Information Technology. Dr Klasky’s research interest is in addressing large-scale data issues for computational science applications, particularly focused on the creation of new techniques that allow large-scale data-producing applications to process data and gain insight. Towards this goal Dr Klasky, in collaboration with Georgia Institute of Technology and Rutgers University, leads the development of the data management middleware platform, ADIOS. ADIOS is currently used by many of the top leadership applications at the DoE supercomputing center at Oak Ridge.</p>
<p>
<italic>Tahsin M. Kurc</italic> is Chief Software Architect in the Center for Comprehensive Informatics and a Research Associate Professor in the Department of Biomedical Informatics at Emory University. He received a PhD degree in computer science from Bilkent University in Turkey. His research focuses on distributed and parallel computing, grid computing, and systems software for large-scale, data-intensive scientific applications.</p>
</bio>
</contrib-group>
<aff id="aff1-1094342013488260"><label>1</label>Center for Comprehensive Informatics and Biomedical Informatics Department, Emory University, USA</aff>
<aff id="aff2-1094342013488260"><label>2</label>Scientific Data Group, Oak Ridge National Laboratory, USA</aff>
<author-notes>
<corresp id="corresp1-1094342013488260">Joel H. Saltz, Biomedical Informatics Department, 36 Eagle Row, Suite 566, Atlanta, GA, 30322, USA. Email: <email>jhsaltz@emory.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2013</year>
</pub-date>
<volume>27</volume>
<issue>3</issue>
<issue-title>Special Issue section on CCDSC 2012 Workshop</issue-title>
<fpage>263</fpage>
<lpage>272</lpage>
<history>
<date date-type="received">
<day>11</day>
<month>12</month>
<year>2012</year>
</date>
<date date-type="accepted">
<day>11</day>
<month>3</month>
<year>2013</year>
</date>
</history>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The analysis of large sensor datasets for structural and functional features has applications in many domains, including weather and climate modeling, characterization of subsurface reservoirs, and biomedicine. The vast amount of data obtained from state-of-the-art sensors and the computational cost of analysis operations create a barrier to such analyses. In this paper, we describe middleware system support to take advantage of large clusters of hybrid CPU–GPU nodes to address the data and compute-intensive requirements of feature-based analyses of large spatio-temporal datasets.</p>
</abstract>
<kwd-group>
<kwd>Sensor data</kwd>
<kwd>imaging data</kwd>
<kwd>data analysis and management</kwd>
<kwd>cluster computing</kwd>
<kwd>GPGPU</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section1-1094342013488260">
<title>1. Introduction</title>
<p>Advances in sensor technologies make it possible to rapidly collect vast quantities of low-dimensional, spatio-temporal datasets. Spatio-temporal datasets contain data elements that are associated with coordinates in a multi-dimensional space with low-dimensionality, and potentially obtained at multiple time steps. Analysis and characterization of features (e.g. spatial structures, their properties, function of the properties over space and time) in these datasets are important in many scientific domains, including weather prediction and climate modeling, earth systems science, biomedicine, and materials science. Satellite observations of the earth, for instance, provide valuable data regarding spatially distributed phenomena (wild fires, air and water pollution, vegetation index, and weather patterns) at regional to global scales. An earth systems researcher may want to investigate the growth of residential zoning in a given area and the decline of certain vegetation in surrounding areas using satellite imagery. The researcher would use sensor readings from different satellites to detect residential areas and the density and types of the flora. She could then compute correlations between vegetation density and the size and proximity to vegetation of residential areas using a series of readings over time. In biomedical research, advanced microscopy scanners can capture incredibly detailed pictures of tissue in a few minutes and enable healthcare organizations to collect thousands of images daily. Histo-pathological analysis of images from tissue samples enables the examination of disease morphology at the sub-cellular level. In a recent study of glioblastoma, <xref ref-type="bibr" rid="bibr8-1094342013488260">Cooper et al. (2011</xref>) have characterized morphological extracts from serial sections of high-resolution image data and correlated them with clinical data and extracted various subtypes. That work demonstrated that morphological features self-aggregate into distinct clusters with significantly different clinical outcomes.</p>
<p>In order to fully exploit the potential of spatio-temporal sensor datasets in scientific research, high-performance computing capabilities are needed to rapidly extract and classify various features from large volumes of data, ranging from multiple terabytes to petabytes, using data- and computation-intensive analysis pipelines. While datasets in an application domain are used to answer domain-specific questions, analysis processes have common patterns. Analysis of a spatio-temporal dataset involves detecting spatial objects of interest, characterizing their structural and functional features, and monitoring and quantifying changes over space and time. We refer to structural features as the spatial characteristics of objects such as shape and area, whereas functional features represent signals measured within the space occupied by the object such as texture and intensity. These processing patterns are realized by the composition of a suite of core operations (see <xref ref-type="table" rid="table1-1094342013488260">Table 1</xref>) into analysis pipelines. A middleware framework that can support the core operation categories and processing patterns can benefit a wide range of applications.</p>
<table-wrap id="table1-1094342013488260" position="float">
<label>Table 1.</label>
<caption>
<p>Core operation categories in analytics applications for spatio-temporal sensor data.</p>
</caption>
<graphic alternate-form-of="table1-1094342013488260" xlink:href="10.1177_1094342013488260-table1.tif"/>
<table>
<thead>
<tr>
<th>Core operation category</th>
<th>Operations</th>
<th>Data access patterns and computational complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data cleaning and low-level transformations</td>
<td>Transformations to reduce effects of sensor/measurement artifacts. Transform sensor-acquired measurements to domain-specific variables.</td>
<td>Mainly local and regular data access patterns. Moderate computational complexity.</td>
</tr>
<tr>
<td>Data subsetting, filtering, subsampling</td>
<td>Select portions of a dataset corresponding to regions in atlas and/or time intervals. Select portions of a dataset based on value ranges (e.g. regions with temperature greater than <italic>x</italic> degrees). Resample data to reduce resolution and data size or to match resolutions for comparative or integrative analysis downstream.</td>
<td>Local data access patterns as well as indexed access. Low to moderate, mainly data-intensive computations.</td>
</tr>
<tr>
<td>Spatio-temporal mapping and registration</td>
<td>Map datasets to an atlas. Resolve data redundancy at tile boundaries to form mosaics. Create composite dataset from multiple spatially co-incident datasets. Create derived dataset from spatially co-incident datasets obtained at different times.</td>
<td>Irregular local and global data access patterns. Moderate to high computational complexity.</td>
</tr>
<tr>
<td>Object segmentation </td>
<td>Segment <italic>base-level</italic> objects such as nuclei, buildings, lakes. Extract features from base-level objects.</td>
<td>Irregular, but primarily local, data access patterns. High computational complexity.</td>
</tr>
<tr>
<td>Object classification</td>
<td>Classify base-level objects through possibly iterative combination of clustering, machine learning, and human input (active learning).</td>
<td>Irregular and global data access patterns. High computational complexity. </td>
</tr>
<tr>
<td>Spatio-temporal aggregation</td>
<td>Construct <italic>high-level</italic> objects composed of classified base-level object aggregates, e.g. residential areas vs. industrial complexes. Compute time-series aggregates over a given imaged area.</td>
<td>Primarily local with a crucial global component for aggregation. Moderate/high computation complexity.</td>
</tr>
<tr>
<td>Change detection, comparison, and quantification</td>
<td>Quantify changes over time in domain-specific low-level variables, base-level objects, and high-level objects. Construct <italic>change objects</italic> to describe changes in low-level domain-specific variables, base-level objects, and high-level objects. Spatial queries for selecting and comparing segmented objects.</td>
<td>Compute and data-intensive computations. Mixture of local and global data access patterns as well as indexed access.</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>In this paper we examine middleware system support that targets clusters containing large numbers of hybrid CPU–GPU nodes. These systems are becoming widely deployed at national laboratories and supercomputing centers. While they provide significant processing power and memory capacity, they introduce increased complexities of programming and scalability due to more complex architectural characteristics, including higher concurrency, deep memory hierarchies, and new forms of heterogeneity.</p>
</sec>
<sec id="section2-1094342013488260">
<title>2. Data processing patterns and challenges in analysis of spatio-temporal sensor datasets</title>
<p>
<xref ref-type="table" rid="table1-1094342013488260">Table 1</xref> lists core operation categories along with data access and computation patterns in each category. <xref ref-type="table" rid="table2-1094342013488260">Table 2</xref> shows a list of application-specific operations in pathology image analysis (<xref ref-type="bibr" rid="bibr9-1094342013488260">Cooper et al., 2010</xref>), weather prediction studies (<xref ref-type="bibr" rid="bibr16-1094342013488260">Liu et al., 2008</xref>), and change analysis using satellite imagery (<xref ref-type="bibr" rid="bibr6-1094342013488260">Chandola and Vatsavai, 2011</xref>). As is seen from the table, applications from these domains have similar operations, although they use spatio-temporal datasets for different purposes. The core operations can produce different levels of data products that can be consumed by client applications. For example, a client application may request only satellite imagery data covering the east coast of the US; this request can be satisfied by a data subsetting operation applied on global coverage satellite data to retrieve sensor readings corresponding to the US east coast. The operations can also be chained to form analysis workflows to create data products. An example workflow could be a pipeline of: <bold>[</bold>data cleaning → mapping → object segmentation → object classification → change detection<bold>]</bold> operations.</p>
<table-wrap id="table2-1094342013488260" position="float">
<label>Table 2.</label>
<caption>
<p>Operations from example application scenarios mapped to the core operation categories.</p>
</caption>
<graphic alternate-form-of="table2-1094342013488260" xlink:href="10.1177_1094342013488260-table2.tif"/>
<table>
<thead>
<tr>
<th>Operation category</th>
<th>Pathology image analysis</th>
<th>Weather prediction</th>
<th>Monitoring and change analysis</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data cleaning and low-level transformations</td>
<td>Color normalization. Thresholding of pixel and regional gray-scale values.</td>
<td>Remove anomalous measurements and convert spectral intensities to value of interest.</td>
<td>Remove unusual readings. Convert signal intensities to color and other values of interest.</td>
</tr>
<tr>
<td>Data subsetting, filtering, subsampling</td>
<td>Selection of regions within an image. Thresholding of pixel values.</td>
<td>Spatial selection/cross match to find portion of a dataset corresponding to a given geographic region.</td>
<td>Spatial selection/cross match to find portion of a dataset corresponding to a given geographic region.</td>
</tr>
<tr>
<td>Spatio-temporal mapping and registration</td>
<td>Deformable registration of images to anatomical atlas.</td>
<td>Generation of mosaic of tiles to get complete coverage.</td>
<td>Registering low- and high-resolution images corresponding to same regions.</td>
</tr>
<tr>
<td>Object segmentation </td>
<td>Segmentation of nuclei and cells. Compute texture and shape features.</td>
<td>Segmentation of regions with similar land surface temperature. </td>
<td>Segmentation of buildings, trees, plants, etc.</td>
</tr>
<tr>
<td>Object classification</td>
<td>
<italic>K</italic>-means clustering of nuclei into categories.</td>
<td>Classification of segmented regions.</td>
<td>Classification of buildings, trees, plants.</td>
</tr>
<tr>
<td>Spatio-temporal aggregation</td>
<td>Aggregation of object features for per-image features.</td>
<td>Time-series calculations on changing land and air conditions. </td>
<td>Aggregation of labeled buildings, trees, plants into residential, industrial, vegetation areas.</td>
</tr>
<tr>
<td>Change detection, comparison, and quantification</td>
<td>Spatial queries to compare segmented nuclei and features.</td>
<td>Spatial and temporal queries on classified regions, and aggregation to look for changing weather patterns. </td>
<td>Characterize vegetation changes over time and area. </td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The data access patterns and computational complexity of the core operations presented in <xref ref-type="table" rid="table1-1094342013488260">Table 1</xref> are important factors in I/O, communication, memory, and processing overheads in analysis applications. The local data access patterns involve retrieval and processing of a single data element or data elements within a small neighborhood in a spatial and temporal region (e.g. data accesses by data cleaning and low-level transformation operations). The regular access patterns perform sweeps over data elements, while irregular accesses may involve accesses to random collections of data elements (e.g. data accesses by certain types of object classification algorithms, morphological reconstruction operations in object segmentation). Generalized reductions (e.g. aggregation) and indexed accesses (e.g. queries for data subsetting and change quantification) also are common patterns. Analytics applications composed of the core operations encapsulate several application-level data processing patterns as well. First, datasets are often partitioned into tiles or chunks, and some operations in <xref ref-type="table" rid="table1-1094342013488260">Table 1</xref> can be executed on each chunk independently. This leads to a bag-of-tasks processing pattern. Second, processing of a single chunk or a group of chunks can be expressed as a hierarchical coarse-grain dataflow pattern (<xref ref-type="bibr" rid="bibr4-1094342013488260">Beynon et al., 2001</xref>; <xref ref-type="bibr" rid="bibr22-1094342013488260">Plale and Schwan, 2000</xref>). For example, transformation, filtering, mapping, and segmentation operations can be composed into a workflow. The segmentation operation itself may consist of a pipeline of lower-level operations as well. Third, several types of operations such as aggregation and classification can be represented as MapReduce style (<xref ref-type="bibr" rid="bibr10-1094342013488260">Dean and Ghemawat, 2010</xref>) computations.</p>
<p>We describe pathology image analysis in biomedicine as an example to illustrate the core operation categories and the challenges of synthesizing and classifying features in more detail. Biopsied tissues contain morphological information that manifests from various molecular interactions that arise from disease or therapy. Microscopic examination of a tissue reveals this information, enabling the pathologist to render accurate diagnoses and guide therapy. Some crucial disease sub-types can be identified through image analysis of whole-slide tissue images. A typical analysis starts with selecting a subset of images and performing color normalization to reduce image acquisition artifacts. If the dataset consists of stacks of images, the images may be registered to each other to form a 3D volume. If there is an application-specific atlas (e.g. a brain atlas), each image may be mapped to the atlas. A segmentation algorithm is executed on each image to determine the boundaries of micro-anatomic objects, such as nuclei, cells, and small blood vessels. A set of shape and texture features is computed for each segmented object. The objects are classified into groups using a machine-learning algorithm such as clustering. Feature vectors for objects may also be aggregated to compute image-level features. The images are grouped into categories using similar machine-learning algorithms. Feature sets from different image datasets can be compared to find common occurrences and differences in patient populations, or to assess the sensitivity of analysis algorithms to input data.</p>
<p>Analysis of whole slide images is a data- and computation-intensive process. There are about 10<sup>5</sup> to 10<sup>7</sup> cells in a 10<sup>5</sup> × 10<sup>5</sup>-pixel image – the size of such an image is about 40 GB uncompressed. The process of classifying a segmented cell is done using roughly 10 to 100 shape and texture features. A state-of-the-art scanner can capture hundreds of images per day, and a project can collect and reference thousands of slides. An analysis limited to classifying cells in a dataset with a thousand images could require processing of 10<sup>12</sup> features. Computational requirements for segmentation or feature extraction are approximately 10–15 h on a single processor system for a single image at a resolution of 0.5 microns/pixel – this corresponds to processing approximately 10<sup>8</sup> features. The computational requirements increase significantly when a detailed characterization of morphology in a large image dataset is carried out. This process requires the coordinated use of many interrelated analysis pipelines in order to improve analysis accuracy and classification power, and to assess the sensitivity of analysis to input data.</p>
</sec>
<sec id="section3-1094342013488260">
<title>3. Middleware software support</title>
<p>The middleware framework to support the core operations and analysis pipelines should provide solutions in the form of efficient data representations, programming abstractions, and runtime that leverage the heterogeneity and memory hierarchies of available computing platforms. In the following sections, we present our research in these areas.</p>
<sec id="section4-1094342013488260">
<title>3.1. Data representation</title>
<p>We are developing a data representation and library to support data exchange among application-specific instances of the core operations and facilitate runtime functionality in a unified framework with high performance. For this purpose, we define a <italic>region template</italic> abstraction to describe 2D/3D static and temporal regions. The objective with the region template is to provide a generic container template for common data types in spatio-temporal datasets: <italic>points</italic>, <italic>arrays</italic>, <italic>meshes</italic>, and <italic>spatial</italic> <italic>objects</italic> within a spatial and temporal bounding box – an array represents a set of connected points on a regular grid, whereas a mesh is a set of points connected in a graph. A <italic>data region</italic> object is a storage materialization of a region template and contains elements from a spatio-temporal region. Data elements contained in a data region can be stored in dense or sparse structures; an image tile, for example, would be represented with a dense 2D array, while a group of objects would be stored as a set. Application operations would interact with data region objects to read and write/store data elements; the runtime system would manage the data regions and their content across memory hierarchies for high performance. A data region is identified by a (namespace:: key, timestamp, version number) triple. This triple identifier intends to provide temporal relationships among data regions related to the same spatial area. The metadata for a data region contains one or more bounding boxes and the type of the data region: array, mesh, points, objects, and <italic>compound </italic>– a compound data region contains other data regions. Data regions corresponding to the same spatial area may contain different data types and data products. For instance, a region template may define a data region of a 1 × 1 km<sup>2</sup> regular grid over the USA; another one may define a data region for states; and a third may specify a data region storing the precise spatial coordinates of satellite-borne sensor readings. Data regions may be related to one another in a defined manner. For instance, region templates may specify a precise correspondence or mapping between 30 × 30 km<sup>2</sup> grid elements in data region A and 1 × 1 km<sup>2</sup> grid elements in data region B. The spatial and temporal relationship information can be used for supporting decisions regarding the placement and computation of data regions in analyses applications. For instance, in operations applied to objects, objects’ location in the data domain can be used to perform better distribution of the computation in a distributed environment, as compared to statically partitioning the data domain in equal size data chunks without considering object locations.</p>
<p>We are in the process of implementing a region template library. This library provides constructors for defining region templates and instantiating data regions and mechanisms for accessing data elements. Access mechanisms will support associative queries and direct access to data elements. Data regions can be associated with <italic>data source </italic>components, which encapsulate mechanisms for storage. This design allows for the decisions regarding data movement and placement to be delegated to the runtime environment, which may use different layers of a system memory to store the data according to the analysis pipeline used. The current implementation supports data sources for file-based, CPU memory, and GPU memory storage of data regions. The ability of the runtime system to transfer data regions back and forth between different memory layers is especially important to take advantage of CPU–GPU equipped nodes.</p>
</sec>
<sec id="section5-1094342013488260">
<title>3.2. Programming abstractions and runtime services for distributed execution</title>
<p>Applications that make use of spatio-temporal sensor datasets share common processing structures, which include bag-of-tasks execution, generalized reduction and MapReduce (<xref ref-type="bibr" rid="bibr10-1094342013488260">Dean and Ghemawat, 2010</xref>), and coarse-grain dataflow (<xref ref-type="bibr" rid="bibr4-1094342013488260">Beynon et al., 2001</xref>; <xref ref-type="bibr" rid="bibr22-1094342013488260">Plale and Schwan, 2000</xref>), as described in Section 2. Our earlier work has shown that many processing patterns in multi-dimensional scientific data analyses can be implemented as MapReduce-style applications (<xref ref-type="bibr" rid="bibr7-1094342013488260">Chang et al., 2000</xref>). The coarse-grain dataflow pattern can be supported by a filter-stream framework (<xref ref-type="bibr" rid="bibr4-1094342013488260">Beynon et al., 2001</xref>; <xref ref-type="bibr" rid="bibr22-1094342013488260">Plale and Schwan, 2000</xref>), in which an analysis application is expressed as a network of components linked through logical streams to exchange data and control information. These frameworks provide a flexible application development platform and hide the complexities of runtime optimizations from the developer. At the same time, the data access, communication, and processing structure of the application is exposed, which can be leveraged for performance optimization at the middleware level. Nevertheless, optimizing the transport and analysis of large volumes of data on high-performance systems requires characterizing operational states and making appropriate and timely scheduling decisions along multiple application and hardware levels. Managing complex processing and data interactions among operations in an analysis application is a significant challenge on large-scale homogeneous clusters. It becomes more difficult on systems with nodes equipped with multiple GPUs and CPUs due to significant differences in their computing and memory characteristics (<xref ref-type="bibr" rid="bibr3-1094342013488260">Augonnet et al., 2011</xref>; <xref ref-type="bibr" rid="bibr5-1094342013488260">Bosilca et al., 2012</xref>; <xref ref-type="bibr" rid="bibr12-1094342013488260">Hartley et al., 2008</xref>; <xref ref-type="bibr" rid="bibr23-1094342013488260">Teodoro et al., 2010</xref>, <xref ref-type="bibr" rid="bibr24-1094342013488260">2012a</xref>).</p>
<p>We have developed runtime support that couples bag-of-tasks and filter-stream processing with function variants and implements performance-aware scheduling heuristics to exploit the aggregate computing capacity in hybrid systems via coordinated use of CPUs and GPUs (<xref ref-type="bibr" rid="bibr26-1094342013488260">Teodoro et al., 2012b</xref>). A function variant is defined as a group of functions with the same name, arguments, and result type of a high-level, abstract operation, and enables the runtime system to choose the appropriate function or functions during execution, allowing multiple computing devices (e.g. CPUs and GPUs) to be used in a coordinated manner. In our image processing applications, we have observed performance variations in GPU versus CPU efficiency across different input data as well as across different types of operations (<xref ref-type="bibr" rid="bibr24-1094342013488260">Teodoro et al., 2012a</xref>). A scheduling strategy for CPU–GPU machines should, therefore, take into account performance variability due to both different operations and different input datasets in order to achieve more effective assignment of operations to computing devices. We have devised a scheduling strategy, called PRIORITY, which uses a sorted queue of (task, data element) tuples based on the relative GPU/CPU speedup expected for each tuple. As more (task, data element) tuples are created by analysis operations, these tuples are inserted into the queue such that the queue remains sorted. When a CPU core or GPU remains idle, one of the tuples from the queue is assigned to the idle device. If the idle device is a CPU core, the tuple with minimum estimated speedup is assigned to the CPU core. If the idle device is a GPU, the tuple with the maximum estimated speedup is assigned to the GPU. Although PRIORITY relies on speedup estimates, the speedup values are used only to order tasks. Thus, the most accurate speedup values are not needed as long as the order of tuples is not affected.</p>
<p>Analysis of large spatio-temporal datasets may also incur high I/O overheads. In addition to large volumes of input and potentially output data, intermediate results or checkpoints may need to be staged to disk. For example, studying the sensitivity to input parameters and algorithm variations of the segmentation stage output would require us to execute multiple runs, the results of which may not fit in memory due to time and resource constraints. The output from a stage in a single run may need to be stored on disk for inspection or visualization at a later time. The runtime system, hence, should provide capability to stage data to and from disk efficiently by buffering and organizing in-memory data and disk-bound data chunks, distributing data chunks across multiple storage nodes (e.g. on a high-performance file system such as Lustre), and overlapping I/O and computations. Since data can be processed in bag-of-tasks, dataflow, and MapReduce patterns, there can be concurrent access requests to the shared storage resources from tens of thousands of processes. The runtime system needs to coordinate all of these requests to minimize contentions. We are currently investigating the integration of data sources with efficient I/O mechanisms, such as ADIOS (<xref ref-type="bibr" rid="bibr17-1094342013488260">Lofstead et al., 2008</xref>), which would provide I/O scalability on very large machines. These I/O mechanisms are based on a stream-I/O approach, drawing from filter-stream networks (<xref ref-type="bibr" rid="bibr2-1094342013488260">Arpaci-Dusseau et al., 1999</xref>; <xref ref-type="bibr" rid="bibr4-1094342013488260">Beynon et al., 2001</xref>) and data staging (<xref ref-type="bibr" rid="bibr1-1094342013488260">Abbasi et al., 2010</xref>; <xref ref-type="bibr" rid="bibr11-1094342013488260">Docan et al., 2010</xref>). In the stream I/O approach, a set of CPU cores, designated as I/O unit, are connected to the computation units via logical streams, over which data chunks are transmitted. Streams are managed by a scheduler, which assigns data chunks to I/O units to increase I/O performance and reduce resource usage. A group of I/O units can be collocated with a group of computation units or can be mapped to a separate set of machines in the system, such as its dedicated special-purpose I/O nodes.</p>
</sec>
</sec>
<sec id="section6-1094342013488260">
<title>4. Implementation and experimental evaluation</title>
<p>We have implemented and evaluated the PRIORITY scheduler (<xref ref-type="bibr" rid="bibr24-1094342013488260">Teodoro et al., 2012a</xref>) on the Keeneland system (<xref ref-type="bibr" rid="bibr27-1094342013488260">Vetter et al., 2011</xref>) for segmentation of nuclei and computation of object features in pathology image datasets. Keeneland has 120 nodes equipped with a dual socket Intel (Santa Clara, USA) X5660 2.8 Ghz processor, three NVIDIA (Santa Clara, USA) Tesla M2070 (Fermi) GPUs, and 24 GB of DDR3 RAM. The input image dataset used in this evaluation was obtained from <italic>in silico</italic> study of brain tumors (<xref ref-type="bibr" rid="bibr9-1094342013488260">Cooper et al., 2010</xref>). Each image was partitioned into tiles of 4 k × 4 k pixels. The region template prototype for this implementation specifies a 2D array to store an input image tile and another 2D array for the mask array, which is created at the end of the segmentation stage to store segmented objects in the respective image tile. The feature computation stage computes features for segmented nuclei (<italic>per nucleus</italic>) and for the entire input image (<italic>per image</italic>). We implemented CPU and GPU versions of four types of features: Haralick, Sobel, intensity, and gradient. We refer the reader to <xref ref-type="bibr" rid="bibr24-1094342013488260">Teodoro et al. (2012a</xref>) for implementation details. The GPU-versus-CPU speedup values for each operation using different tile types (<xref ref-type="fig" rid="fig1-1094342013488260">Figure 1</xref>) are presented in <xref ref-type="fig" rid="fig2-1094342013488260">Figure 2</xref>. The speedup values are relative to a single CPU core implementation. </p>
<fig id="fig1-1094342013488260" position="float">
<label>Figure 1.</label>
<caption>
<p>Tiles with different tissue coverage.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488260-fig1.tif"/>
</fig>
<fig id="fig2-1094342013488260" position="float">
<label>Figure 2.</label>
<caption>
<p>Feature computation scalability.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488260-fig2.tif"/>
</fig>
<p>The results highlight the fact that there are significant variations in GPU-vs-CPU speedups for these operations. The performance of per nucleus feature computations is both operation and data dependent – operations on type 4 tile have the highest speedup, since type 4 tile has more nuclei and presents more parallelization opportunities. There is also variation in performance across different types of operations in per image computations, but the variation across tile types is small since the operations are applied on the entire tile irrespective of the tile’s content. We observe small speedup values for the intensity and gradient per image computations compared to per nucleus computations. This is because a histogram is computed for each nucleus in a tile in <italic>per nuclei</italic> computations, while only one histogram is computed for a tile in <italic>per tile</italic> computations, reducing opportunity for parallelism. These results justify the use of a dynamic, performance-aware scheduling strategy.</p>
<p>We performed a scaling experiment in which the size of the input data and number of nodes used were increased proportionally for the feature computation stage. The input dataset for the baseline single node execution contains an image with 100 (4 k × 4 k) mixed type tiles, and the subsequent executions in multiple nodes scale this baseline dataset. <xref ref-type="fig" rid="fig3-1094342013488260">Figure 3</xref> presents the throughput (tiles processed per s) as the configuration of the application and the number of nodes is varied. The <italic>linear</italic> curve refers to the linear throughput improvement based on the best single node execution. The results show that PRIORITY results in additional speedups of 1.88 and 1.80, with respect to the three-GPU only cases and a first come, first serve (FCFS) strategy, respectively, in which task tuples are assigned to idle devices in the order they are inserted into the queue.</p>
<fig id="fig3-1094342013488260" position="float">
<label>Figure 3.</label>
<caption>
<p>Performance evaluation of GPU-based feature computation operations.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488260-fig3.tif"/>
</fig>
<p>Our initial investigation of middleware optimization for concurrent I/O demonstrated the need and benefits of special I/O handling on large-scale systems. <xref ref-type="fig" rid="fig4-1094342013488260">Figure 4</xref> presents the weak scaling performance of the baseline implementation, where each compute unit performed its own I/O (baseline) vs. an implementation where the disk writes were isolated in 300 dedicated I/O processes (dedicated I/O). This experiment, conducted on the Jaguar system at ORNL, showed that runtime scaled non-linearly with process count due to I/O concurrency. Using dedicated I/O processes improved the overall running time significantly, but network delays and read operations remained the majority of the running time. Complex interactions discovered during the initial investigation, such as reduction in read times when optimizing write operations, are subjects of our on-going studies.</p>
<fig id="fig4-1094342013488260" position="float">
<label>Figure 4.</label>
<caption>
<p>I/O weak scalability with naive and dedicated I/O implementations.</p>
</caption>
<graphic xlink:href="10.1177_1094342013488260-fig4.tif"/>
</fig>
</sec>
<sec id="section7-1094342013488260">
<title>5. Related work</title>
<p>Several libraries for scientific analysis have been proposed with the intention of increasing scientists’ ability to process and share data (<xref ref-type="bibr" rid="bibr19-1094342013488260">Miller et al., 2001</xref>; <xref ref-type="bibr" rid="bibr21-1094342013488260">Moran, 2001</xref>). These libraries provide high-level abstractions on spatial discretization and vector spaces, but do not have implementations on systems with CPU–GPU nodes. A recent work, EAVL (<xref ref-type="bibr" rid="bibr18-1094342013488260">Meredith et al., 2012</xref>), is designed to take advantage of CPUs and GPUs for visualization of mesh-based simulations. Region templates leverage some of the data description concepts proposed by <xref ref-type="bibr" rid="bibr13-1094342013488260">Kohn and Baden (1995</xref>), as we allow for a hierarchical representation of a low-dimensional data domain, but region templates differ from the previous work in several ways. Region templates enable association of data from multiple sources to the same spatial region. This is a common scenario in sensor data analysis, where multiple data measurements may be taken for the same region; for instance, measurements of the humidity of a certain region over time in monitoring and change detection analysis (<xref ref-type="bibr" rid="bibr6-1094342013488260">Chandola and Vatsavai, 2011</xref>). In pathology image analysis, it is common to have images stained with different markers associated with the same region. Region templates also support objects within regions. This adds flexibility and expressivity to the analysis framework that may employ work partitioning policies, which can more accurately estimate the computation costs of regions based on the density of objects, as is seen in the GPU–GPU scheduling results. There are several active projects targeting runtime support for efficient execution on CPU–GPU equipped machines, such as DAGuE (<xref ref-type="bibr" rid="bibr5-1094342013488260">Bosilca et al., 2012</xref>) and StarPU (<xref ref-type="bibr" rid="bibr3-1094342013488260">Augonnet et al., 2011</xref>). These two projects focus on execution of regular linear algebra applications on CPU–GPU machines. They offer different scheduling policies, including those that prioritize computation of critical paths in the dependency graph to maximize parallelism. DAGuE assumes that the application Directed Acyclic Graph (DAG) is static and known before execution. In our applications, however, the dependency graph representing the application must be dynamically built during the execution, as the computation of the next stage of the analysis pipeline may depend on the results of the current stage. Classification of features is an important step in image analysis. A commonly used methodology is to run <italic>k</italic>-means (<xref ref-type="bibr" rid="bibr15-1094342013488260">Lee and Macqueen, 1980</xref>) or consensus clustering (<xref ref-type="bibr" rid="bibr20-1094342013488260">Monti et al., 2003</xref>) on features (or labels) computed for each object or image. Recent work by <xref ref-type="bibr" rid="bibr14-1094342013488260">Le et al. (2012</xref>) investigate unsupervised learning techniques to create high-level features for categorizing images. They present an implementation of their technique on a computation cluster with 16 k cores. </p>
</sec>
<sec id="section8-1094342013488260">
<title>6. Conclusions</title>
<p>Spatio-temporal datasets have applications in numerous scientific and engineering domains. While specific analyses are application- and domain-dependent, they share common processing patterns. Middleware systems designed for efficient realization of these patterns on emerging hybrid CPU–GPU systems can benefit a wide range of applications. Hybrid systems offer significant processing power for researchers to make use of very large sensor datasets. Efficient use of these systems requires new smart data structures, programming models, and runtime support because of the new forms of heterogeneity and deep memory hierarchies. In this work we have presented a framework that combines the concept of region templates, function variants, and dynamic performance-aware scheduling to address some of these challenges. A prototype of this framework on a hybrid system shows significant performance improvements in a pathology image analysis application, indicating its potential for enabling analysis pipelines for large-scale spatio-temporal datasets. Using the framework, we were able to execute the segmentation and feature computation steps on a dataset with 36,848 4 k × 4k-pixel image tiles (about 1.8 TB uncompressed) in less than four minutes on 100 nodes with the PRIORITY strategy (<xref ref-type="bibr" rid="bibr26-1094342013488260">Teodoro et al., 2012b</xref>; <xref ref-type="bibr" rid="bibr25-1094342013488260">2013</xref>). </p>
</sec>
</body>
<back>
<fn-group>
<fn fn-type="financial-disclosure" id="fn1-1094342013488260">
<label>Funding</label>
<p>This work was funded, in part, by contract HHSN261200800001E by the NCI; and grants 5R01LM009239-04 and 1R01LM011119-01 from the NLM, R24HL085343 from the NHLBI, NIH P20EB000591, RC4MD005964 from NIH, and PHS grant UL1TR000454 from the CTSA Program, NIH, NCATS. This research used resources of the Keeneland Computing Facility at the Georgia Institute of Technology, which is supported by the NSF under contract OCI-0910735.</p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Abbasi</surname>
<given-names>H</given-names>
</name>
<name>
<surname>Wolf</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Eisenhauer</surname>
<given-names>G</given-names>
</name>
<etal/>
</person-group> (<year>2010</year>) <article-title>DataStager: scalable data staging services for petascale applications</article-title>. <source>Cluster Computing</source> <volume>13</volume>: <fpage>277</fpage>–<lpage>290</lpage>.</citation>
</ref>
<ref id="bibr2-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Arpaci-Dusseau</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Anderson</surname>
<given-names>E</given-names>
</name>
<name>
<surname>Treuhaft</surname>
<given-names>N</given-names>
</name>
<etal/>
</person-group> (<year>1999</year>) <article-title>Cluster I/O with River: making the fast case common</article-title>. In: <source>IOPADS '99: Input/output for parallel and distributed systems</source>, <comment>Atlanta, GA, 5 May 1999</comment>, pp. <fpage>10</fpage>–<lpage>22</lpage>.</citation>
</ref>
<ref id="bibr3-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Augonnet</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Thibault</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Namyst</surname>
<given-names>R</given-names>
</name>
<etal/>
</person-group> (<year>2011</year>) <article-title>StarPU: a unified platform for task scheduling on heterogeneous multicore architectures</article-title>. <source>Concurrency and Computation: Practice and Experience</source> <volume>23</volume>: <fpage>187</fpage>–<lpage>198</lpage>.</citation>
</ref>
<ref id="bibr4-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Beynon</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Kurc</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Catalyurek</surname>
<given-names>U</given-names>
</name>
<etal/>
</person-group> (<year>2001</year>) <article-title>Distributed processing of very large datasets with DataCutter</article-title>. <source>Parallel Computing</source> <volume>27</volume>: <fpage>1457</fpage>–<lpage>2478</lpage>.</citation>
</ref>
<ref id="bibr5-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bosilca</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Bouteiller</surname>
<given-names>A</given-names>
</name>
<name>
<surname>Danalis</surname>
<given-names>A</given-names>
</name>
<etal/>
</person-group>. (<year>2012</year>) <article-title>DAGuE: A generic distributed DAG engine for high performance computing</article-title>. <source>Parallel Computing</source> <volume>38</volume>(<issue>1–2</issue>): <fpage>37</fpage>–<lpage>51</lpage>.</citation>
</ref>
<ref id="bibr6-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chandola</surname>
<given-names>V</given-names>
</name>
<name>
<surname>Vatsavai</surname>
<given-names>RR</given-names>
</name>
</person-group> (<year>2011</year>) <article-title>A scalable Gaussian process analysis algorithm for biomass monitoring</article-title>. <source>Statistical Analysis and Data Mining</source> <volume>4</volume>: <fpage>430</fpage>–<lpage>445</lpage>.</citation>
</ref>
<ref id="bibr7-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Chang</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Kurc</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Sussman</surname>
<given-names>A</given-names>
</name>
<etal/>
</person-group> (<year>2000</year>) <article-title>Optimizing retrievel and processing of multi-dimensional scientific datasets</article-title>. In: <source>Proceedings of the international parallel and distributed processing symposium (IPDPS 2000)</source>, <publisher-name>Cancun</publisher-name>, <publisher-loc>Mexico, 1–5 May 2000, pp. 405–410</publisher-loc>.</citation>
</ref>
<ref id="bibr8-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Cooper</surname>
<given-names>L</given-names>
</name>
<name>
<surname>Kong</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Wang</surname>
<given-names>F</given-names>
</name>
<etal/>
</person-group> (<year>2011</year>) <article-title>Morphological signatures and genomic correlates in glioblastoma</article-title>. In: <source>IEEE international symposium on biomedical imaging: from nano to macro</source>, <publisher-loc>Chicago, Illinois, USA</publisher-loc>, <comment>30 March–2 April 2011</comment>, pp. <fpage>1624</fpage>–<lpage>1627</lpage>.</citation>
</ref>
<ref id="bibr9-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Cooper</surname>
<given-names>LA</given-names>
</name>
<name>
<surname>Kong</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Gutman</surname>
<given-names>DA</given-names>
</name>
<etal/>
</person-group> (<year>2010</year>) <article-title>An integrative approach for <italic>in silico</italic> glioma research</article-title>. <source>IEEE Transactions on Biomedical Engineering</source> <volume>57</volume>: <fpage>2617</fpage>–<lpage>2621</lpage>.</citation>
</ref>
<ref id="bibr10-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dean</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Ghemawat</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>MapReduce: a flexible data processing tool</article-title>. <source>Communications of the ACM</source> <volume>53</volume>: <fpage>72</fpage>–<lpage>77</lpage>.</citation>
</ref>
<ref id="bibr11-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Docan</surname>
<given-names>C</given-names>
</name>
<name>
<surname>Parashar</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Klasky</surname>
<given-names>S</given-names>
</name>
</person-group> (<year>2010</year>) <article-title>DataSpaces: an interaction and coordination framework for coupled simulations workflows</article-title>. In: <source>Proceedings of the 19th international symposium on high performance distributed computing (HPDC’10)</source> <publisher-loc>Chicago, Illinois, USA</publisher-loc>, <fpage>20</fpage>–<lpage>25</lpage>, <issue>June</issue> 2010, pp. <fpage>25</fpage>–<lpage>36</lpage>.</citation>
</ref>
<ref id="bibr12-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Hartley</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Catalyurek</surname>
<given-names>U</given-names>
</name>
<name>
<surname>Ruiz</surname>
<given-names>A</given-names>
</name>
<etal/>
</person-group> (<year>2008</year>) <article-title>Biomedical image analysis on a cooperative cluster of GPUs and multicores</article-title>. In: <source>ICS'08: Proceedings of the 2008 ACM international conference on supercomputing</source>, Island of Kos, Greece, 7–12 June 2008, pp. <fpage>15</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr13-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Kohn</surname>
<given-names>SR</given-names>
</name>
<name>
<surname>Baden</surname>
<given-names>SB</given-names>
</name>
</person-group> (<year>1995</year>) <article-title>A parallel software infrastructure for structured adaptive mesh methods</article-title>. In: <source>ACM/IEEE conference on supercomputing</source>, <publisher-loc>San Diego, California, USA</publisher-loc>, <fpage>4</fpage>–<lpage>8</lpage> <comment>December 1995, pp. Article no. 36</comment>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Le</surname>
<given-names>Q</given-names>
</name>
<name>
<surname>Ranzato</surname>
<given-names>M</given-names>
</name>
<name>
<surname>Monga</surname>
<given-names>R</given-names>
</name>
<etal/>
</person-group> (<year>2012</year>) <article-title>Building high-level features using large scale unsupervised learning</article-title>. In: <source>Proceedings of the 29th international conference on machine learning</source>, <publisher-name>Edinburgh</publisher-name>, <publisher-loc>Scotland, UK, 26 June– 1 July 2012</publisher-loc>.</citation>
</ref>
<ref id="bibr15-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lee</surname>
<given-names>HB</given-names>
</name>
<name>
<surname>Macqueen</surname>
<given-names>JB</given-names>
</name>
</person-group> (<year>1980</year>) <article-title>A K-means cluster-analysis computer-program with cross-tabulations and next-nearest-neighbor analysis</article-title>. <source>Educational and Psychological Measurement</source> <volume>40</volume>: <fpage>133</fpage>–<lpage>138</lpage>.</citation>
</ref>
<ref id="bibr16-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Liu</surname>
<given-names>Y</given-names>
</name>
<name>
<surname>Hill</surname>
<given-names>DJ</given-names>
</name>
<name>
<surname>Abdelzaher</surname>
<given-names>T</given-names>
</name>
<etal/>
</person-group> (<year>2008</year>) <article-title>Virtual sensor–powered spatiotemporal aggregation and transformation: a case study analyzing near-real-time NEXRAD and precipitation gauge data in a digital watershed</article-title>. In: <source>Proceedings of the environmental information management conference 2008 (EIM-2008)</source>, <publisher-loc>Albuquerque, NM, 10–11 September 2008</publisher-loc>.</citation>
</ref>
<ref id="bibr17-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Lofstead</surname>
<given-names>J</given-names>
</name>
<name>
<surname>Klasky</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Schwan</surname>
<given-names>K</given-names>
</name>
<etal/>
</person-group> (<year>2008</year>) <article-title>Flexible IO and integration for scientific codes through the adaptable IO system (ADIOS)</article-title>. In: <source>CLADE 2008 at high performance distributed computing conference</source>, <publisher-name>Boston</publisher-name>, <publisher-loc>Massachusetts, 23 June 2008, pp. 15–24</publisher-loc>.</citation>
</ref>
<ref id="bibr18-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Meredith</surname>
<given-names>JS</given-names>
</name>
<etal/>
</person-group> (<year>2012</year>) <article-title>EAVL: The extreme-scale analysis and visualization library</article-title>. In: <source>Eurographics symposium on parallel graphics and visualization (EGPGV)</source>, <publisher-loc>Cagliari, Italy</publisher-loc>, <fpage>13</fpage>–<lpage>14</lpage> <comment>May 2012, pp. 21–30</comment>. <publisher-loc>Cabliari</publisher-loc>: <publisher-name>Eurographics Association</publisher-name>.</citation>
</ref>
<ref id="bibr19-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Miller</surname>
<given-names>MC</given-names>
</name>
<etal/>
</person-group> (<year>2001</year>) <article-title>Enabling interoperation of high performance, scientific computing applications: modeling scientific data with the sets and fields (SAF) modeling system</article-title>. In: <source>International conference on computational science</source>, <publisher-loc>San Francisco, CA, USA, 28–30 May 2001</publisher-loc>, pp. <fpage>158</fpage>–<lpage>170</lpage>. <publisher-loc>London</publisher-loc>: <publisher-name>Springer-Verlag</publisher-name>.</citation>
</ref>
<ref id="bibr20-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Monti</surname>
<given-names>S</given-names>
</name>
<name>
<surname>Tamayo</surname>
<given-names>P</given-names>
</name>
<name>
<surname>Mesirov</surname>
<given-names>J</given-names>
</name>
<etal/>
</person-group> (<year>2003</year>) <article-title>Consensus clustering: a resampling-based method for class discovery and visualization of gene expression microarray data</article-title>. <source>Machine Learning</source> <volume>52</volume>: <fpage>91</fpage>–<lpage>118</lpage>.</citation>
</ref>
<ref id="bibr21-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Moran</surname>
<given-names>P</given-names>
</name>
</person-group> (<year>2001</year>) <source>Field model: an object-oriented data model for fields</source>. <comment>Technical Report No. 20010066513</comment>, <publisher-name>NASA Ames Research Center</publisher-name>. </citation>
</ref>
<ref id="bibr22-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Plale</surname>
<given-names>B</given-names>
</name>
<name>
<surname>Schwan</surname>
<given-names>K</given-names>
</name>
</person-group> (<year>2000</year>) <article-title>dQUOB: managing large data flows using dynamic embedded queries</article-title>. In: <source>IEEE international high performance distributed computing (HPDC)</source>, <publisher-loc>Pittsburgh, Pennsylvania, USA</publisher-loc>, <fpage>1</fpage>–<lpage>4</lpage> <comment>August 2000, pp. 263–270</comment>.</citation>
</ref>
<ref id="bibr23-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Teodoro</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Hartley</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Catalyurek</surname>
<given-names>U</given-names>
</name>
<etal/>
</person-group> (<year>2010</year>) <article-title>Run-time optimizations for replicated dataflows on heterogeneous environments</article-title>. In: <source>Proceedings of the 19th ACM international symposium on high performance distributed computing (HPDC)</source>, <publisher-loc>Chicago, Illinois, USA, 20–25 June 2010</publisher-loc>, pp. <fpage>13</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr24-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Teodoro</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Kurc</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Pan</surname>
<given-names>T</given-names>
</name>
<etal/>
</person-group> (<year>2012a</year>) <article-title>Accelerating large scale image analyses on parallel, CPU–GPU equipped systems</article-title>. In: <source>26th IEEE international parallel and distributed processing symposium (IPDPS)</source>, <publisher-loc>Shanghai, China, 21–25 May 2012</publisher-loc>, pp. <fpage>1093</fpage>–<lpage>1104</lpage>. </citation>
</ref>
<ref id="bibr25-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Teodoro</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Pan</surname>
<given-names>T</given-names>
</name>
<etal/>
</person-group> (<year>2013</year>) <article-title>High-throughput analysis of large microscopy image datasets on CPU–GPU cluster platforms</article-title>. In: <source>The 27th IEEE international parallel and distributed processing symposium (IPDPS' 13)</source>
<comment>, Boston, Massachusetts, USA, 20–24 May 2013. USA: IEEE Society</comment>. </citation>
</ref>
<ref id="bibr26-1094342013488260">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Teodoro</surname>
<given-names>G</given-names>
</name>
<name>
<surname>Pan</surname>
<given-names>T</given-names>
</name>
<name>
<surname>Kurc</surname>
<given-names>T</given-names>
</name>
<etal/>
</person-group> (<year>2012b</year>) <source>High-throughput execution of hierarchical analysis pipelines on hybrid cluster platforms</source>. <comment>Technical report no. CCI-TR-2012-4</comment>. <publisher-name>Center for Comprehensive Informatics</publisher-name>, <publisher-loc>Emory University, USA</publisher-loc>.</citation>
</ref>
<ref id="bibr27-1094342013488260">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Vetter</surname>
<given-names>JS</given-names>
</name>
<name>
<surname>Glassbrook</surname>
<given-names>R</given-names>
</name>
<name>
<surname>Dongarra</surname>
<given-names>J</given-names>
</name>
<etal/>
</person-group> (<year>2011</year>) <article-title>Keeneland: bringing heterogeneous GPU computing to the computational science community</article-title>. <source>Computing in Science &amp; Engineering</source> <volume>13</volume>: <fpage>90</fpage>–<lpage>95</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>