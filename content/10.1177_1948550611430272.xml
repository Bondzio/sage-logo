<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">SPP</journal-id>
<journal-id journal-id-type="hwp">spspp</journal-id>
<journal-title>Social Psychological and Personality Science</journal-title>
<issn pub-type="ppub">1948-5506</issn>
<issn pub-type="epub">1948-5514</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1948550611430272</article-id>
<article-id pub-id-type="publisher-id">10.1177_1948550611430272</article-id>
<title-group>
<article-title>Reverse Correlating Social Face Perception</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name>
<surname>Dotsch</surname>
<given-names>Ron</given-names>
</name>
<xref ref-type="aff" rid="aff1-1948550611430272">1</xref>
<xref ref-type="corresp" rid="corresp1-1948550611430272"/>
</contrib>
<contrib contrib-type="author">
<name>
<surname>Todorov</surname>
<given-names>Alexander</given-names>
</name>
<xref ref-type="aff" rid="aff1-1948550611430272">1</xref>
</contrib>
<aff id="aff1-1948550611430272"><label>1</label>Department of Psychology, Princeton University, NJ, USA</aff>
<bio>
<title>Bios</title>
<p>
<bold>Ron Dotsch</bold> is a postdoctoral researcher at the department of psychology at Princeton University. He is currently working with Alexander Todorov to study social face perception. He received his PhD in Psychology from Radboud University Nijmegen, where he worked with Daniel Wigboldus and Ad van Knippenberg. Ron's research interests include face perception, social categorization, prejudice, stereotypes, and the use of advanced methods, such as virtual reality, reverse correlation, and face space models.</p>
<p>
<bold>Alexander Todorov</bold> is an associate professor of psychology and public affairs at Princeton University with a joint appointment in the Department of Psychology and the Woodrow Wilson School of Public and International Affairs. He is also an affiliated faculty of the Princeton Neuroscience Institute and a visiting professor at Radboud University Nijmegen in the Netherlands. His research focuses on the cognitive and neural basis of social cognition. His main line of research is on the cognitive and neural mechanisms of person perception with a particular emphasis on the social dimensions of face perception. His research approach is multidisciplinary, using a variety of methods from behavioral and functional Magnetic Resonance Imaging experiments to computer and statistical modeling. Alexander received his PhD in psychology from New York University in 2002.</p>
</bio>
</contrib-group>
<author-notes>
<corresp id="corresp1-1948550611430272">Ron Dotsch, Department of Psychology, Princeton University, Princeton, NJ 08540, USA Email: <email>rdotsch@princeton.edu</email>
</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>9</month>
<year>2012</year>
</pub-date>
<volume>3</volume>
<issue>5</issue>
<fpage>562</fpage>
<lpage>571</lpage>
<permissions>
<copyright-statement>© The Author(s) 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">Social and Personality Psychology Consortium</copyright-holder>
</permissions>
<abstract>
<p>Reverse correlation (RC) techniques provide a data-driven approach to model internal representations in an unconstrained way. Here, we used this approach to model social perception of faces. In the RC task, participants repeatedly selected from two face images—created by superimposing randomly generated noise masks on the same face—the face that looked most trustworthy (or, in other conditions: untrustworthy, dominant, or submissive). We calculated classification images (CIs) by averaging all selected images. Trait judgments of independent participants, as well as objective metrics, showed that the CIs visualized the intended traits well. Furthermore, tests of pixel clusters showed that diagnostic information resided mostly in mouth, eye, eyebrow, and hair regions. The current work shows that RC provides an excellent tool to extract psychologically meaningful images that map onto social perception.</p>
</abstract>
<kwd-group>
<kwd>facial expressions</kwd>
<kwd>measurement</kwd>
<kwd>person perception</kwd>
<kwd>social cognition</kwd>
<kwd>social judgment</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>It is easy for people to perceive and extract social information from faces. People infer social traits from faces after minimal time exposure (<xref ref-type="bibr" rid="bibr4-1948550611430272">Bar, Neta, &amp; Linz, 2006</xref>; <xref ref-type="bibr" rid="bibr25-1948550611430272">Todorov, Pakrashi, &amp; Oosterhof, 2009</xref>; <xref ref-type="bibr" rid="bibr28-1948550611430272">Willis &amp; Todorov, 2006</xref>). On the other hand, it is hard for people to verbalize what kind of information (i.e., facial feature configurations) they use to make social judgments. There are many reasons for this, including that some diagnostic features may not have verbal labels or that people may not be aware of the cues they use. The problem of finding the facial information that people use in social judgments is further compounded by the fact that the space of possible hypotheses—what features drive specific social perceptions—is infinitely large (Todorov, Dotsch, Wigboldus, &amp; Said, <xref ref-type="bibr" rid="bibr8-1948550611430272">2011</xref>). For example, 15 binary features result in 32,768 combinations, and 20 binary features result in 1,048,576 combinations. Here we show that a data-driven reverse correlation (RC) approach, designed to overcome the aforementioned problems, can be successfully applied to modeling social perception of faces.</p>
<p>In the RC approach, features of stimuli are randomly varied, after which researchers identify which random variations predict judgments. The RC approach originated in the domain of auditory perception during the 70s (<xref ref-type="bibr" rid="bibr3-1948550611430272">Ahumada &amp; Lovell, 1971</xref>) and was later adapted for research on vision (<xref ref-type="bibr" rid="bibr1-1948550611430272">Ahumada, 1996</xref>, <xref ref-type="bibr" rid="bibr2-1948550611430272">2002</xref>; <xref ref-type="bibr" rid="bibr5-1948550611430272">Beard &amp; Ahumada, 1998</xref>; <xref ref-type="bibr" rid="bibr22-1948550611430272">Solomon, 2002</xref>) and neurophysiology (<xref ref-type="bibr" rid="bibr21-1948550611430272">Ringach &amp; Shapley, 2004</xref>; <xref ref-type="bibr" rid="bibr27-1948550611430272">Victor, 2005</xref>). Only recently have researchers begun to apply RC techniques to the study of social perception.</p>
<p>
<xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008</xref>) used face space–based RC to identify those features that drive social perception. In their approach, a previously constructed face space, able to represent faces as unique points in a multidimensional space, was used to randomly generate faces, which were rated on two primary social dimensions: trustworthiness and dominance (<xref ref-type="bibr" rid="bibr26-1948550611430272">Todorov, Said, Engell, &amp; Oosterhof, 2008</xref>). Based on these ratings, Oosterhof and Todorov constructed models of facial trustworthiness and dominance, consisting of linear combinations of face space dimensions. Moving any face in the direction of, for example, the trustworthiness dimension makes that face look more trustworthy.</p>
<p>The work by <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008)</xref> demonstrates the applicability of RC methods to social perception. There are some major advantages of using a face space–based RC approach (see Todorov et al., <xref ref-type="bibr" rid="bibr23-1948550611430272">2011</xref>), such as the limited set of variables needed to represent a face, thereby minimizing potential combinations when modeling social dimensions. However, there are some disadvantages. First, the specific face space model used may have affected the Oosterhof and Todorov results. Specifically, the face space model was based on principal component analysis of 271 3D laser scanned faces (FaceGen, Singular Inversions, Toronto, Canada), limiting the representation of faces to linear combinations of the original faces. Sampling error could have constrained face representation in ways that affect the final models. Second, stimulus faces generated using the face space did not have hair, an important feature affecting person perception (e.g., <xref ref-type="bibr" rid="bibr18-1948550611430272">Macrae &amp; Martin, 2007</xref>). Third, only shape information was used to construct the models, although reflectance information (pigmentation and texture) might play an important role in social perception (<xref ref-type="bibr" rid="bibr24-1948550611430272">Todorov &amp; Oosterhof, 2011</xref>). Last, although Oosterhof and Todorov included visualizations of the constructed social dimensions, they did not report quantitative analyses with respect to diagnostic face regions.</p>
<p>Here, we use a different class of RC techniques aimed at extending the findings of <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008</xref>) by assessing whether their findings are method invariant and by quantitatively identifying which specific facial regions are involved in social perception. This class of RC techniques was developed in parallel by <xref ref-type="bibr" rid="bibr16-1948550611430272">Kontsevich and Tyler (2004</xref>) and <xref ref-type="bibr" rid="bibr19-1948550611430272">Mangini and Biederman (2004</xref>, also see <xref ref-type="bibr" rid="bibr11-1948550611430272">Gosselin &amp; Schyns, 2003</xref>). This technique enables researchers to generate images that reflect participants’ internal representations of faces, without making any assumption about what those representations might look like. Recently, this RC variant has become increasingly popular in social cognitive research (see, e.g., Dotsch, Wigboldus, Langner, &amp; van Knippenberg, <xref ref-type="bibr" rid="bibr8-1948550611430272">2008</xref>; Dotsch, Wigboldus, &amp; van Knippenberg, 2011; Imhoff, Dotsch, Bianchi, Banse, &amp; Wigboldus, <xref ref-type="bibr" rid="bibr12-1948550611430272">in press</xref>; <xref ref-type="bibr" rid="bibr13-1948550611430272">Jack, Caldara, &amp; Schyns, 2011</xref>; Karremans, Dotsch, &amp; Corneille, <xref ref-type="bibr" rid="bibr15-1948550611430272">in press</xref>).</p>
<p>A typical RC image classification task (in this example we discuss a two images forced choice, or 2IFC, variant used by <xref ref-type="bibr" rid="bibr7-1948550611430272">Dotsch et al., 2008</xref>) employs random variations of facial images created with a constant base face (<xref ref-type="fig" rid="fig1-1948550611430272">Figure 1A</xref>) and randomly generated noise patterns (<xref ref-type="fig" rid="fig1-1948550611430272">Figure 1B</xref>) superimposed on the face. Because the noise distorts the base face image, the face looks different with each different random noise pattern. For each superimposed random noise pattern, a negative pattern (the mathematical opposite) is generated. Each pixel that is dark in the original noise pattern is bright in the negative noise pattern, much like photo negatives. In a single trial, the base image with the original noise and the base image with the negative noise superimposed are presented side by side (<xref ref-type="fig" rid="fig1-1948550611430272">Figure 1C</xref>). Participants are then asked to select the face that best resembles the target category. The average of all selected noise patterns constitutes the classification image (CI), whereas the average of all unselected noise patterns is the anti-CI. For instance, Dotsch, Wigboldus, Langner, and van Knippenberg (2008) used a neutral male face as base face, and superimposed random noise consisting of multiple truncated sinusoids. These were then used as stimuli in a Moroccan classification task: participants chose from two stimuli (<xref ref-type="fig" rid="fig1-1948550611430272">Figure 1C</xref>) the stimulus that best resembled a Moroccan face. The average of all noise patterns that participants classified as Moroccan constituted the Moroccan CI. Superimposing this CI on top of the original base image resulted in approximations of what participants thought typical Moroccan faces looked like.</p>
<fig id="fig1-1948550611430272" position="float">
<label>Figure 1.</label>
<caption>
<p>Base face (A), random noise example (B), and example stimuli of noise superimposed on a single base image (C). The left stimulus shows the base image with original noise superimposed and the right stimulus shows the base image with the negative noise superimposed.</p>
</caption>
<graphic xlink:href="10.1177_1948550611430272-fig1.tif"/>
</fig>
<p>The 2IFC task described above is based on the psychophysical RC methodology described by <xref ref-type="bibr" rid="bibr19-1948550611430272">Mangini and Biederman (2004</xref>), where one image was presented in each trial and participants classified the image into one of two categories. In this case, CIs for each category are calculated by averaging all images classified as the respective category. Mangini and Biederman demonstrated that this technique can be used to model identities (John Travolta vs. Tom Cruise), gender categories (male vs. female), and emotional expressions (happy vs. sad). However, the extent to which this task can be applied to modeling social perception is somewhat limited. First, the task used as base face a morph between two images that accurately represented the two target categories (e.g., John Travolta’s and Tom Cruise’s face). When modeling social dimensions, researchers do not possess images that accurately represent the two target categories. Instead, those images are exactly what researchers set out to discover. Second, participants discriminated between two specific categories, making it impossible to tap into the internal representation of just one category without contrasting it with another category. The 2IFC variant does not suffer from these problems, because participants select the image that best fits one target category and the images can be created using a base face unrelated to the target category.</p>
<p>Here, we use the 2IFC RC image classification task to model perception of face trustworthiness and dominance. Importantly, the 2IFC task enables us to go beyond the results of <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008</xref>) in two ways. First, the base image of stimulus faces can include hair and the superimposed noise affects shape as well as reflectance information. Second, with the 2IFC task we can identify facial regions diagnostic for social perception. Arguably, this might be achieved using yet another RC technique, bubbles (<xref ref-type="bibr" rid="bibr10-1948550611430272">Gosselin &amp; Schyns, 2001</xref>), in which good exemplars of the target categories are partially presented (masked by Gaussian “bubbles”). RC analysis then reveals which facial areas predict the correct response. However, the known good exemplars of trustworthiness and dominance have been visualized by Oosterhof and Todorov but might not be optimal because of the reasons mentioned above. The current method, on the other hand, does not need good exemplars as stimuli, but nevertheless can be used to identify diagnostic facial regions.</p>
<p>On the methodological side, the current study provides the opportunity to investigate the interpretation of anti-CIs. Because as yet their meaning is unclear, we included additional RC tasks to validate the anti-CIs. It is likely that an anti-CI will resemble the other end of a bipolar target dimension. For example, in a trustworthiness RC task, the anti-CI may look untrustworthy. We therefore included both trustworthiness and untrustworthiness RC tasks to quantify the extent to which the untrustworthy CI is similar to the antitrustworthy CI and vice versa. For the same reason, we included both dominance and submissiveness RC tasks.</p>
<p>In sum, participants completed a trustworthiness, untrustworthiness, dominance, or submissiveness 2IFC RC task. For example, in the trustworthiness task, on each trial, participants decided which image looked more trustworthy. We calculated the resulting CIs and anti-CIs and used a combination of objective similarity metrics and subjective ratings of independent judges to assess their validity. Because the noise patterns in this task consisted of variations in different regions of the face, we explored which facial regions contained information diagnostic for social judgments using clusters of pixels tests (analogous to tests used with functional magnetic resonance imaging research to identify activated brain regions; see <xref ref-type="bibr" rid="bibr6-1948550611430272">Chauvin, Worsley, Schyns, Arguin, &amp; Gosselin, 2005</xref>).</p>
<sec id="section1-1948550611430272" sec-type="methods">
<title>Method</title>
<sec id="section2-1948550611430272">
<title>Participants and Design</title>
<p>Eighty students from Princeton University participated. Each performed a single RC task for a trait that was either indicative (e.g., trustworthy and dominant) or counterindicative (e.g., untrustworthy and submissive) for one dimension (respectively, trustworthiness or dominance). We generated two stimulus sets for the RC tasks. Participants were either presented with one or the other stimulus set. Because both sets resulted in highly similar results, we collapsed across this variable. The RC study thus consisted of a 2 (Dimension: trustworthiness vs. dominance) × 2 (Trait: indicative vs. counterindicative) between-subject design, with 20 participants per cell.</p>
</sec>
<sec id="section3-1948550611430272">
<title>Materials and Procedure</title>
<p>The stimuli in the RC task all consisted of the same base face with different superimposed random noise on each trial. The base face was a gray scale average of all male faces in the Karolinska Face Database (Lundqvist, Flykt, &amp; Öhman, <xref ref-type="bibr" rid="bibr32-1948550611430272">1998</xref>, see <xref ref-type="fig" rid="fig1-1948550611430272">Figure 1A</xref>). The noise consisted of superimposed truncated sinusoid patches of 2 Cycles in 6 Orientations (0°, 30°, 60°, 90°, 120°, and 150°) × 5 Spatial scales (2, 4, 8, 16, and 32 cycles per image) × 2 Phases (0, π/2), with random contrasts (see <xref ref-type="fig" rid="fig2-1948550611430272">Figure 2</xref>). In sum, the random noise was a function of 4,092 parameters, each defining the contrast value of one truncated sinusoid spanning two cycles. Stimulus size was 512 × 512 pixels.</p>
<fig id="fig2-1948550611430272" position="float">
<label>Figure 2.</label>
<caption>
<p>This figure outlines the process of generating the noise pattern that was superimposed on the base image. For each spatial frequency (2, 4, 8, 16, and 32 cycles per image), 12 sinusoids per cycle were superimposed (6 Orientations × 2 Phases). Each sinusoid had its own random parameter indicating its contrast. The first 2 lines indicate how 12 sinusoids form the combined noise for 1 spatial frequency (2 cycles per image). The last line indicates how sinusoids across all spatial frequencies are combined to create the resulting noise. <italic>Note</italic>. *Contrasts are random example values to illustrate how contrast values relate to the resulting sinusoid patches.</p>
</caption>
<graphic xlink:href="10.1177_1948550611430272-fig2.tif"/>
</fig>
<p>In a single trial two stimuli were presented side by side. One stimulus was the base face with a random noise pattern superimposed and the other the base face with the negative of the random noise pattern superimposed (see <xref ref-type="fig" rid="fig1-1948550611430272">Figure 1C</xref>). We chose to use the negative of the random noise pattern as opposed to just another random noise pattern to maximize the differences between the two presented images, to minimize the number of possible stimulus pairs to be presented, and to simplify data analysis. This procedure has been successfully employed by <xref ref-type="bibr" rid="bibr7-1948550611430272">Dotsch et al. (2008</xref>), Dotsch, Wigboldus, and van Knippenberg (2011), Imhoff, Dotsch, Bianchi, Banse, and Wigboldus (<xref ref-type="bibr" rid="bibr12-1948550611430272">in press</xref>), and Karremans, Dotsch, and Corneille (<xref ref-type="bibr" rid="bibr15-1948550611430272">in press</xref>). Participants were instructed to select the stimulus that most resembled a trustworthy (untrustworthy, dominant, and submissive) face.</p>
<p>Participants completed 300 trials. The presented stimuli were drawn without replacement from one of two sets of 300 original (and 300 matching negative) noise patterns. These two sets were generated to ensure that the obtained results were not an artifact of the specific sets of noise patterns used. The stimulus pairs were presented in random order. The placement of the facial images with original and negative noise on the screen (negative noise on the left vs. on the right) was counterbalanced across trials. A 1,000-ms centered fixation cross preceded each trial. After participants completed the task, they were debriefed.</p>
</sec>
<sec id="section4-1948550611430272">
<title>Data Processing</title>
<p>To generate the CIs, we calculated the mean of all noise patterns a participant selected as most trustworthy (untrustworthy, dominant, and submissive), by averaging the parameters on which those noise patterns were based. This resulted in 4,092 mean parameters per participant. We then averaged the mean parameters across participants for each cell of the design and generated the classification patterns based on cell average parameters. Finally, we superimposed the classification patterns on the original base image to generate the CIs. The faces participants <italic>did not </italic>select as most trustworthy (untrustworthy, dominant, and submissive) underwent the same treatment and yielded the anti-CIs.</p>
</sec>
</sec>
<sec id="section5-1948550611430272">
<title>Results</title>
<p>The resulting CIs and anti-CIs per trait are depicted in <xref ref-type="fig" rid="fig3-1948550611430272">Figure 3</xref>. Visual inspection of the CIs show that a trustworthy face involves a smooth, small face, a smiling mouth, and open eyes, whereas an untrustworthy face seems to involve a downturned mouth with thick lips, angry-looking eyes, sagging cheeks, and a bald spot on top of the head. The dominant face has clear eyebrows, dark eyes, and a slightly downturned mouth. The face also emerges more from the background. The submissive face is frowning, has thin lips and sad-looking eyes. Moreover, the face delineates itself less from the background.</p>
<fig id="fig3-1948550611430272" position="float">
<label>Figure 3.</label>
<caption>
<p>Resulting classification images ([CIs] top row, the average of all noise patterns selected as best resembling the target trait, superimposed on the base image) and anti-CIs (bottom row, the average of all noise patterns <italic>not</italic> selected as best resembling the target trait, superimposed on the base image).</p>
</caption>
<graphic xlink:href="10.1177_1948550611430272-fig3.tif"/>
</fig>
<sec id="section6-1948550611430272">
<title>Participant Agreement</title>
<p>To assess participant agreement, we calculated Cronbach’s α on the pixel luminance values of each participants’ CI noise pattern (masked by an oval shape to only include pixels of the face), for each social judgment. α, in this analysis, reflects the extent to which the pixels of individual participants’ CIs within a cell covary with the pixels of all other participants’ CIs in that cell. It was not possible to assess agreement based on the decision data, because two different sets of stimuli were used. As can be seen in <xref ref-type="table" rid="table1-1948550611430272">Table 1</xref>, there is fair agreement with α ranging between .56 and .76.</p>
<table-wrap id="table1-1948550611430272" position="float">
<label>Table 1.</label>
<caption>
<p>Participants’ Agreement Measures for Each Social Judgment Quantified as Cronbach’s α Computed Over Subjects’ CI Noise Pattern Pixel Luminance Values</p>
</caption>
<graphic alternate-form-of="table1-1948550611430272" xlink:href="10.1177_1948550611430272-table1.tif"/>
<table>
<thead>
<tr>
<th>CI</th>
<th>Cronbach’s α</th>
</tr>
</thead>
<tbody>
<tr>
<td>Trustworthy</td>
<td>.56</td>
</tr>
<tr>
<td>Untrustworthy</td>
<td>.65</td>
</tr>
<tr>
<td>Dominant</td>
<td>.76</td>
</tr>
<tr>
<td>Submissive</td>
<td>.58</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section7-1948550611430272">
<title>Objective Metrics</title>
<p>Because the CIs represent psychologically meaningful constructs that have well-defined mutual relations (e.g., trustworthiness is the opposite of untrustworthiness, and relatively unrelated to dominance or submissiveness), the resulting CIs should have the same mutual relations (e.g., the trustworthy CI should be similar to the opposite of the untrustworthy CI, and not too similar to the dominance or submissive CIs). These relations can be quantified by calculating the correlation between the pixel luminance values of one classification pattern and the pixel luminance values of another classification pattern. These correlations can be best understood as measures of similarity. A high-positive correlation means that the CIs are physically similar, a high-negative correlation means that the CIs are physically opposite, and a zero correlation means that the CIs have little in common. These correlations (based on CIs masked with an oval shape to only include pixels of the face) are summarized in <xref ref-type="table" rid="table1-1948550611430272">Table 2</xref>.<sup>
<xref ref-type="fn" rid="fn1-1948550611430272">1</xref>
</sup>
</p>
<p>The correlations in <xref ref-type="table" rid="table2-1948550611430272">Table 2</xref> show that (1) CIs for a trait on one side of the dimension (e.g., trustworthy) are physically <italic>different</italic> from (i.e., correlate negatively with) CIs for a trait on the other side of the dimension (e.g., untrustworthy); (2) CIs for a trait on one side of the dimension (e.g., trustworthy) are physically <italic>similar</italic> to (i.e., correlate positively with) CIs of the antiface of a trait on the other side of the dimension (e.g., anti-untrustworthy); and (3) CIs for a trait on one dimension (e.g., trustworthiness) are <italic>unrelated</italic> (weakly correlated) to CIs pertaining to another dimension (e.g., dominance). Note however that the correlations with unrelated dimensions are nonzero, indicating that trustworthiness and dominance, although unique dimensions, are not completely orthogonal.</p>
<table-wrap id="table2-1948550611430272" position="float">
<label>Table 2.</label>
<caption>
<p>Objective Metric of Similarity Between the Various Visualized Traits Quantified as Correlations Between the Aggregated CI Noise Patterns Pixel Values by Trait</p>
</caption>
<graphic alternate-form-of="table2-1948550611430272" xlink:href="10.1177_1948550611430272-table2.tif"/>
<table>
<thead>
<tr>
<th>
</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. Trustworthy</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>2. Untrustworthy</td>
<td>−.65</td>
<td>
</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>3. Dominant</td>
<td>−.27</td>
<td>.50</td>
<td>
</td>
<td>
</td>
</tr>
<tr>
<td>4. Submissive</td>
<td>.23</td>
<td>−.41</td>
<td>−.70</td>
<td>
</td>
</tr>
<tr>
<td>5. Antiface<sup><xref ref-type="table-fn" rid="table-fn1-1948550611430272">a</xref></sup>
</td>
<td>.66</td>
<td>.67</td>
<td>.71</td>
<td>.71</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-1948550611430272">
<p>
<sup>a</sup>The antiface is the CI based on the unselected faces in the reverse correlation task for the trait on the other end of the same dimension (e.g., the trustworthy face is correlated with the anti-untrustworthy face).</p>
</fn>
</table-wrap-foot>
</table-wrap>
</sec>
<sec id="section8-1948550611430272">
<title>Subjective Metrics</title>
<p>As subjective metric, an independent sample of 38 Princeton University undergraduate students were asked to rate the trustworthy, untrustworthy, dominant, and submissive CIs (one aggregate CI per trait) on trustworthiness, dominance, and threat, on a scale from 1 (<italic>not trustworthy </italic>[<italic>dominant, threatening</italic>]) to 9 (<italic>very trustworthy </italic>[<italic>dominant, threatening</italic>]). Participants rated the stimuli in random order within blocks (one type of judgment per block). The blocks had a fixed order, respectively, trustworthiness, dominance, and threat. We added threat because previous work by <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008</xref>) showed that threat is a combination of untrustworthiness and dominance. Because both dimensions contribute to threat, we expected the untrustworthy and dominant CIs to be rated equally high on threat, and the trustworthy and submissive CIs to be rated equally low on threat. Beforehand, to prevent that participants would be unfamiliar with the faces when judging trustworthiness and familiar when judging dominance and threat, participants rated the stimuli on attractiveness to become familiar with the stimulus set.</p>
<p>he average ratings are summarized in <xref ref-type="fig" rid="fig4-1948550611430272">Figure 4</xref> and indicate that all CIs visualized the intended traits. That is, the trustworthy CI was judged most trustworthy, whereas the untrustworthy CI was judged least trustworthy. Likewise, the dominant CI was judged most dominant, whereas the submissive CIs were judged least dominant. We subjected the ratings to a 2 (Dimension: trustworthiness vs. submissiveness<sup>
<xref ref-type="fn" rid="fn2-1948550611430272">2</xref>
</sup>) × 2 (Trait: indicative vs. counterindicative) within-subject analysis of variance (ANOVA) for each separate trait. The ANOVA on the trustworthiness ratings revealed both a main effect for trait, <italic>F</italic>(1, 37) = 122.94, <italic>p </italic>&lt; .001, η<sub>G</sub>
<sup>2</sup> = .54, indicating that the trustworthy and submissive CIs were rated more trustworthy than the untrustworthy and dominant CIs, and a Dimension × Trait interaction effect, <italic>F</italic>(1, 37) = 50.06, <italic>p </italic>&lt; .001, η<sub>G</sub>
<sup>2</sup> = .15, indicating that the trustworthiness difference between trustworthy and untrustworthy was greater than the difference between submissive and dominance. The ANOVA on the dominance ratings again revealed both a main effect for trait, <italic>F</italic>(1, 37) = 51.29, <italic>p </italic>&lt; .001, η<sub>G</sub>
<sup>2</sup> = .44, indicating that the trustworthy and submissive CIs were rated less dominant than the untrustworthy and dominant CIs, and a Dimension × Trait interaction effect, <italic>F</italic>(1, 37) = 11.54, <italic>p </italic>&lt; .001, η<sub>G</sub>
<sup>2</sup> = .04, indicating that the dominance difference between trustworthy and untrustworthy was smaller than the dominance difference between submissive and dominance. Last, as expected, the ANOVA on the threat ratings revealed only a main effect for trait, <italic>F</italic>(1, 37) = 67.07, <italic>p </italic>&lt; .001, η<sub>G</sub>
<sup>2</sup> = .56, indicating that the trustworthy and submissive CIs were rated less threatening than the untrustworthy and dominant CIs, and no other differences were significant, n.s.<sup>
<xref ref-type="fn" rid="fn3-1948550611430272">3</xref>
</sup>
</p>
<fig id="fig4-1948550611430272" position="float">
<label>Figure 4.</label>
<caption>
<p>Mean trustworthiness, dominance, threat, and attractiveness ratings of classification images ([CIs] *<italic>p</italic> &lt; .05).</p>
</caption>
<graphic xlink:href="10.1177_1948550611430272-fig4.tif"/>
</fig>
<p>To test whether judgments of both trustworthiness and dominance contributed to judgments of threat, we compared goodness of fit (<italic>R</italic>
<sup>2</sup>) of simple regression models (in which trustworthiness or dominance ratings predicted threat ratings in separate models) with an additive model (in which both trustworthiness and dominance were included as predictors of threat ratings). The additive model predicted threat ratings better (<italic>R</italic>
<sup>2</sup> = .73, β<sub>trustworthiness</sub> = −.35, <italic>p </italic>&lt; .001; β<sub>dominance</sub> = .64, <italic>p </italic>&lt; .001) than trustworthiness (<italic>R</italic>
<sup>2</sup> = .41, β<sub>trustworthiness</sub> = −.64, <italic>p </italic>&lt; .001) or dominance ratings alone (<italic>R</italic>
<sup>2</sup> = .64; β<sub>dominance</sub> = .80, <italic>p </italic>&lt; .001), with significant improvements of fit, respectively, <italic>F</italic>(1, 149) = 177.75, <italic>p </italic>&lt; .001, and <italic>F</italic>(1, 149) = 52.42, <italic>p </italic>&lt; .001.</p>
</sec>
<sec id="section9-1948550611430272">
<title>Diagnostic Facial Regions</title>
<p>We performed a cluster test (<xref ref-type="bibr" rid="bibr6-1948550611430272">Chauvin et al., 2005</xref>) on the pixel data of each CI’s noise pattern to identify facial regions diagnostic for making the various social judgments. We first smoothed the CI noise pattern using a Gaussian filter (σ<italic>
<sub>b</sub>
</italic> = 4 pixels). The smoothed image was masked by an oval shape (only revealing pixels located in the face, including hair) and <italic>Z</italic> transformed. We then performed two-tailed cluster tests (using the stat4CI toolbox, <xref ref-type="bibr" rid="bibr6-1948550611430272">Chauvin et al., 2005</xref>; <italic>Z</italic>
<sub>crit</sub>
<italic> </italic>≥ |2.3|, <italic>p </italic>&lt; .05) for each CI to identify clusters containing pixels for which luminance variation predicted social judgments. The resulting clusters, representing facial regions diagnostic for the respective social judgment, are depicted in <xref ref-type="fig" rid="fig5-1948550611430272">Figure 5</xref>. The clusters in green indicate that pixel luminance in those clusters correlated positively with that respective classification (when these pixels were lighter, participants were more likely to select the image), whereas clusters in red indicate that pixel luminance in those clusters correlated negatively with that respective classification (when these pixels were darker, participants were more likely to select the image).</p>
<fig id="fig5-1948550611430272" position="float">
<label>Figure 5.</label>
<caption>
<p>Significant clusters in classification images. Green clusters show where pixel luminance positively predicted classifications, whereas red clusters show where pixels luminance negatively predicted classifications.</p>
</caption>
<graphic xlink:href="10.1177_1948550611430272-fig5.tif"/>
</fig>
<p>The clusters in <xref ref-type="fig" rid="fig5-1948550611430272">Figure 5</xref> show that for trustworthiness and untrustworthiness judgments mouth, eye, and hair regions were most important. Whereas trustworthiness included bright areas in the mouth and eyebrow regions and dark areas in the eye and hair regions, the opposite held for untrustworthiness. Likewise, the clusters show that for dominance and submissiveness judgments areas around the eyes, hair regions, and delineation of the face (most visible in the right chin of the dominant CI) were diagnostic.</p>
</sec>
</sec>
<sec id="section10-1948550611430272">
<title>Discussion</title>
<p>Psychophysical RC provides a data-driven approach to model social perception. Here, we used a 2IFC RC image classification task (<xref ref-type="bibr" rid="bibr7-1948550611430272">Dotsch et al., 2008</xref>) to model perception of two primary dimensions of face evaluation: trustworthiness and dominance. To validate the resulting visualizations, we used correlations between pixels of the CIs as objective metrics of similarity. The negative correlation between the resulting CIs for trustworthy (dominant) and untrustworthy (submissive) faces indicated that features that increase face trustworthiness (dominance) decrease face untrustworthiness (submissiveness). The weak positive correlation between the CIs involving the trustworthiness dimension and the CIs involving the dominance dimension established discriminant validity in the sense that features that changed face trustworthiness did not change face dominance much. Moreover, the high correlations between CIs on one side of the dimension (trustworthy/dominant) and the anti-CIs on the other side of the dimension (untrustworthy/submissive) provided evidence for the interpretation of the anti-CIs as visualizing the negative side of the intended dimension. Furthermore, using subjective ratings of the CIs by independent participants, we showed that the CIs did represent the intended specific social traits. In short, the 2IFC RC image classification task is capable of visualizing social dimensions, other than gender, identity, and emotional expression.</p>
<p>We explored the regions of the face that were diagnostic for the various social judgments (see <xref ref-type="fig" rid="fig5-1948550611430272">Figure 5</xref>), which highlights the rich body of data that can be gained from RC tasks. Variation in mouth, eye, eyebrow, and hair regions significantly predicted social judgment, which converges with models of trustworthiness and dominance developed by <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008</xref>), although they used different methods. On the other hand, our analysis goes beyond the findings of Oosterhof and Todorov by identifying diagnostic regions that have been previously ignored. These included hair regions which suggest that hairstyle might be an important cue for making trustworthiness and dominance judgments. Furthermore, the RC analysis revealed that dominant faces might be perceived as more protruding from the background, whereas submissive faces seemed to be more part of the background.</p>
<p>The subjective judgments of the CIs showed a very clear pattern: When judged on trustworthiness, the difference between the trustworthy and untrustworthy CIs was larger than the difference between the dominant and submissive CIs, whereas the opposite was the case when judged on dominance. Importantly, when judged on threat (a trait with both an untrustworthy and a dominant component, see <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof &amp; Todorov, 2008</xref>), the difference between trustworthy and untrustworthy CIs was equal to the difference between dominant and submissive CIs. Moreover, judgments of both trustworthiness and dominance contributed to judgment of threat. Although these data provide clear support for the validity of the RC approach to model social perception, two minor artifacts in the data should be addressed. First, the difference between trustworthy and untrustworthy (dominant and submissive) CIs was not absent in dominance (trustworthiness) judgments, indicating that the trustworthiness and dominance dimensions were not completely orthogonal. This is consistent with <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008</xref>) who also found that the two dimensions were not completely orthogonal. Second, it seems that trustworthiness judgments differentiated better between the two dimensions than dominance judgments. This might be due to fixed ordering; CIs were always rated on trustworthiness before they were rated on dominance, potentially causing participants to cluster some CIs together. Alternatively, both artifacts may be interpreted in light of work on the compensation hypothesis (<xref ref-type="bibr" rid="bibr14-1948550611430272">Judd, James-Hawkins, Yzerbyt, &amp; Kashima, 2005</xref>; Yzerbyt, Kervin, &amp; Judd, 2008; <xref ref-type="bibr" rid="bibr31-1948550611430272">Yzerbyt, Provost, &amp; Corneille, 2005</xref>) which holds that people perceived as high on one of the two primary social dimensions are perceived as low on the other dimension (also see <xref ref-type="bibr" rid="bibr9-1948550611430272">Fiske, Cuddy, &amp; Glick, 2007</xref>). This effect is evident in <xref ref-type="fig" rid="fig4-1948550611430272">Figure 4</xref>: the trustworthy CI was judged less dominant than the untrustworthy CI, and the dominant CI was judged less trustworthy than the submissive CI. Moreover, the finding that trustworthiness judgments differentiated between dimensions better than dominance is in line with work showing that people are primarily oriented toward warmth or valence information (<xref ref-type="bibr" rid="bibr29-1948550611430272">Wojciszke, 2005</xref>) and as a result show stronger compensation effects on the second social dimension (in this case, dominance; <xref ref-type="bibr" rid="bibr30-1948550611430272">Yzerbyt et al., 2008</xref>). It should be noted, however, that most work on compensation focused on competence instead of dominance as a second dimension. Whether dominance and competence can be considered different labels for the same social dimension is unclear.</p>
<p>It is important to note that the CIs should not be interpreted as actual mental representations. They are approximations influenced by task-specific factors such as base image or used noise patterns. Nonetheless, the current work clearly shows that RC methods can extract psychologically meaningful images that map onto social perception in a completely unconstrained fashion. The psychological constructs are not necessarily limited to the social dimensions visualized here, nor are they limited to gender, identity, and emotional expressions (<xref ref-type="bibr" rid="bibr19-1948550611430272">Mangini &amp; Biederman, 2004</xref>). The possibilities for laying bare internal representations are endless. <xref ref-type="bibr" rid="bibr13-1948550611430272">Jack, Caldara, and Schyns (2011</xref>) used the method to identify cultural differences in the perception of emotional expressions. Dotsch et al. (2011) have used the method to demonstrate biases in the representation of social categories. Karremans et al. (in press) used it to uncover memory biases in the representation of attractive potential mates, and Imhoff et al. (in press) to reveal spontaneous in-group projection in the facial domain. In the future, the method might even prove to have applied value for domains such as eyewitness testimony (as means to create composites of perpetrators, with the possibility of combining data from multiple eyewitnesses). It is our hope that this research paves the way for more psychophysical RC projects to emerge in the social domain.</p>
</sec>
</body>
<back>
<ack>
<title>Acknowledgments</title>
<p>We thank Jenny Porter for her help with the reverse correlation task and Hillel Aviezer for his help with the rating task.</p>
</ack>
<fn-group>
<fn fn-type="conflict" id="fn4-1948550611430272">
<label>Declaration of Conflicting Interests</label>
<p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p>
</fn>
<fn fn-type="financial-disclosure" id="fn5-1948550611430272">
<label>Funding</label>
<p>The author(s) disclosed receipt of the following financial support for the research, authorship, and/or publication of this article: National Science Foundation grant 0823749, the Russell Sage Foundation, and Netherlands Organisation for Scientific Research (NWO) and Marie Curie Cofund Action Rubicon grant, 446-10-014.</p>
</fn>
</fn-group>
<notes>
<fn-group>
<fn fn-type="other" id="fn1-1948550611430272">
<label>1.</label>
<p>It is also possible to correlate parameters instead of pixels. However, since some parameters (i.e., low spatial frequency parameters) affect many pixels while being few in number, and other parameters (i.e., high spatial frequency parameters) affect just a few pixels in the image while being many in number, the resulting correlations are biased toward high spatial frequency similarity. Moreover, using pixels allowed us to mask those pixels that are not positioned on the face, making the statistics more precise. Nonetheless, we also calculated parameter correlations and the resulting correlation matrix showed a similar pattern.</p>
</fn>
<fn fn-type="other" id="fn2-1948550611430272">
<label>2.</label>
<p>The reason for recoding dominance to submissiveness in these analyses is ease of interpretation, as given in <xref ref-type="fig" rid="fig5-1948550611430272">Figure 5</xref>: a significant interaction effect now indicates that the difference between trustworthiness and untrustworthiness is larger (or smaller) than the difference between dominant and submissive.</p>
</fn>
<fn fn-type="other" id="fn3-1948550611430272">
<label>3.</label>
<p>Although not pertaining to our current purposes, <xref ref-type="fig" rid="fig4-1948550611430272">Figure 4</xref> depicts the attractiveness ratings too. These ratings preceded all other ratings and were included to familiarize participants with the stimuli. We had no prior expectations about the attractiveness ratings, but as can be seen in <xref ref-type="fig" rid="fig4-1948550611430272">Figure 4</xref>, they closely mimic the trustworthiness ratings. This is in line with the findings of <xref ref-type="bibr" rid="bibr20-1948550611430272">Oosterhof and Todorov (2008</xref>) that trustworthiness is highly correlated with several other positive dimensions such as attractiveness and might be described as approximating general valence dimension.</p>
</fn>
</fn-group>
</notes>
<ref-list>
<title>References</title>
<ref id="bibr1-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ahumada</surname>
<given-names>A. J.</given-names>
</name>
</person-group> (<year>1996</year>). <article-title>Perceptual classification images from Vernier acuity masked noise</article-title>. <source>Perception</source>, <volume>26</volume>, <fpage>1831</fpage>–<lpage>1840</lpage>.</citation>
</ref>
<ref id="bibr2-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ahumada</surname>
<given-names>A. J.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Classification image weights and internal noise level estimation</article-title>. <source>Journal of Vision</source>, <volume>2</volume>, <fpage>121</fpage>–<lpage>131</lpage>.</citation>
</ref>
<ref id="bibr3-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ahumada</surname>
<given-names>A. J.</given-names>
</name>
<name>
<surname>Lovell</surname>
<given-names>J.</given-names>
</name>
</person-group> (<year>1971</year>). <article-title>Stimulus features in signal detection</article-title>. <source>Journal of the Acoustical Society of America</source>, <volume>49</volume>, <fpage>1751</fpage>–<lpage>1756</lpage>.</citation>
</ref>
<ref id="bibr4-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Bar</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Neta</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Linz</surname>
<given-names>H.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>Very first impressions</article-title>. <source>Emotion</source>, <volume>6</volume>, <fpage>269</fpage>–<lpage>278</lpage>.</citation>
</ref>
<ref id="bibr5-1948550611430272">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Beard</surname>
<given-names>B. L.</given-names>
</name>
<name>
<surname>Ahumada</surname>
<given-names>A. J.</given-names>
</name>
</person-group> (<year>1998</year>). <article-title>A technique to extract relevant image features for visual tasks</article-title>. In <source>Proceedings of SPIE Human Vision and Electronic Imaging III</source> (Vol. <volume>3299</volume>, pp. <fpage>79</fpage>–<lpage>85</lpage>).</citation>
</ref>
<ref id="bibr6-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Chauvin</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Worsley</surname>
<given-names>K. J.</given-names>
</name>
<name>
<surname>Schyns</surname>
<given-names>P. G.</given-names>
</name>
<name>
<surname>Arguin</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Gosselin</surname>
<given-names>F</given-names>
</name>
</person-group>. (<year>2005</year>). <article-title>Accurate statistical tests for smooth classification images</article-title>. <source>Journal of Vision</source>, <volume>5</volume>, <fpage>1</fpage>–<lpage>1</lpage>. <comment>doi: 10.1167/5.9.1</comment>
</citation>
</ref>
<ref id="bibr7-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dotsch</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Wigboldus</surname>
<given-names>D. H. J.</given-names>
</name>
<name>
<surname>Langner</surname>
<given-names>O.</given-names>
</name>
<name>
<surname>Van Knippenberg</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Ethnic out-group faces are biased in the prejudiced mind</article-title>. <source>Psychological Science</source>, <volume>19</volume>, <fpage>978</fpage>–<lpage>980</lpage>.</citation>
</ref>
<ref id="bibr8-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Dotsch</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Wigboldus</surname>
<given-names>D. H. J.</given-names>
</name>
<name>
<surname>van Knippenberg</surname>
<given-names>A</given-names>
</name>
</person-group>. (<year>2011, March 28</year>). <article-title>Biased allocation of faces to social categories</article-title>. <source>Journal of Personality and Social Psychology</source>. <comment>Advance online publication. doi: 10.1037/a0023026</comment>
</citation>
</ref>
<ref id="bibr9-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Fiske</surname>
<given-names>S. T.</given-names>
</name>
<name>
<surname>Cuddy</surname>
<given-names>A. J. C.</given-names>
</name>
<name>
<surname>Glick</surname>
<given-names>P.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>Universal dimensions of social cognition: Warmth and competence</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>11</volume>, <fpage>77</fpage>–<lpage>83</lpage>.</citation>
</ref>
<ref id="bibr10-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gosselin</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Schyns</surname>
<given-names>P. G.</given-names>
</name>
</person-group> (<year>2001</year>). <article-title>Bubbles: A technique to reveal the use of information in recognition tasks</article-title>. <source>Vision Research</source>, <volume>41</volume>, <fpage>2261</fpage>–<lpage>2271</lpage>.</citation>
</ref>
<ref id="bibr11-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Gosselin</surname>
<given-names>F.</given-names>
</name>
<name>
<surname>Schyns</surname>
<given-names>P. G.</given-names>
</name>
</person-group> (<year>2003</year>). <article-title>Superstitious perceptions reveal properties of internal representations</article-title>. <source>Psychological Science</source>, <volume>14</volume>, <fpage>505</fpage>.</citation>
</ref>
<ref id="bibr12-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Imhoff</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Dotsch</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Bianchi</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Banse</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Wigboldus</surname>
<given-names>D. H. J</given-names>
</name>
</person-group>. (<year>in press</year>). <article-title>Facing Europe: Visualizing spontaneous ingroup projection</article-title>. <source>Psychological Science</source>.</citation>
</ref>
<ref id="bibr13-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Jack</surname>
<given-names>R. E.</given-names>
</name>
<name>
<surname>Caldara</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Schyns</surname>
<given-names>P. G.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Internal representations reveal cultural diversity in expectations of facial expressions of emotion</article-title>. <source>Journal of Experimental Psychology: General</source>. <comment>doi: 10.1037/a0023463</comment></citation>
</ref>
<ref id="bibr14-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Judd</surname>
<given-names>C. M.</given-names>
</name>
<name>
<surname>James-Hawkins</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Yzerbyt</surname>
<given-names>V. Y.</given-names>
</name>
<name>
<surname>Kashima</surname>
<given-names>Y.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Fundamental dimensions of social judgment: Understanding the relations between judgments of competence and warmth</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>89</volume>, <fpage>899</fpage>–<lpage>913</lpage>.</citation>
</ref>
<ref id="bibr15-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Karremans</surname>
<given-names>J. C.</given-names>
</name>
<name>
<surname>Dotsch</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Corneille</surname>
<given-names>O</given-names>
</name>
</person-group>. (<year>in press</year>). <article-title>Romantic relationship status biases mental representations of attractive opposite-sex others: Evidence from a reverse-correlation paradigm</article-title>. <source>Cognition</source>.</citation>
</ref>
<ref id="bibr16-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Kontsevich</surname>
<given-names>L.</given-names>
</name>
<name>
<surname>Tyler</surname>
<given-names>C. W</given-names>
</name>
</person-group>. (<year>2004</year>). <article-title>What makes Mona Lisa smile</article-title>. <source>Vision research</source>, <volume>44</volume>, <fpage>1493</fpage>–<lpage>1498</lpage>. <comment>doi: 10.1016/j.visres.2003.11.027</comment>
</citation>
</ref>
<ref id="bibr32-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Lundqvist</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Flykt</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Öhman</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>1998</year>). <source>The Karolinska Directed Emotional Faces</source>. <publisher-loc>Stockholm, Sweden</publisher-loc>: <publisher-name>Psychology Section, Department of Clinical Neuroscience, Karolinska Institute</publisher-name>.</citation>
</ref>
<ref id="bibr17-1948550611430272">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Lundqvist</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Litton</surname>
<given-names>J. E</given-names>
</name>
</person-group>. (<year>1998</year>). <article-title>The Averaged Karolinska Directed Emotional Faces-AKDEF, CD ROM from department of clinical neuroscience, psychology section, Karolinska Institutet (Tech. Rep.)</article-title>. <comment>ISBN 91-630-7164-9</comment>.</citation>
</ref>
<ref id="bibr18-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Macrae</surname>
<given-names>C. N.</given-names>
</name>
<name>
<surname>Martin</surname>
<given-names>D.</given-names>
</name>
</person-group> (<year>2007</year>). <article-title>A boy primed sue: Feature-based processing and person construal</article-title>. <source>European Journal of Social Psychology</source>, <volume>37</volume>, <fpage>793</fpage>–<lpage>805</lpage>.</citation>
</ref>
<ref id="bibr19-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Mangini</surname>
<given-names>M. C.</given-names>
</name>
<name>
<surname>Biederman</surname>
<given-names>I.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>Making the ineffable explicit: Estimating the information employed for face classifications</article-title>. <source>Cognitive Science</source>, <volume>28</volume>, <fpage>209</fpage>–<lpage>226</lpage>.</citation>
</ref>
<ref id="bibr20-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Oosterhof</surname>
<given-names>N. N.</given-names>
</name>
<name>
<surname>Todorov</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>The functional basis of face evaluation</article-title>. <source>Proceedings of the National Academy of Sciences of the USA</source>, <volume>105</volume>, <fpage>11087</fpage>–<lpage>11092</lpage>.</citation>
</ref>
<ref id="bibr21-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Ringach</surname>
<given-names>D.</given-names>
</name>
<name>
<surname>Shapley</surname>
<given-names>R.</given-names>
</name>
</person-group> (<year>2004</year>). <article-title>RC in neurophysiology</article-title>. <source>Cognitive Science</source>, <volume>28</volume>, <fpage>147</fpage>–<lpage>166</lpage>.</citation>
</ref>
<ref id="bibr22-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Solomon</surname>
<given-names>J. A.</given-names>
</name>
</person-group> (<year>2002</year>). <article-title>Noise reveals visual mechanisms of detection and discrimination</article-title>. <source>Journal of Vision</source>, <volume>2</volume>, <fpage>105</fpage>–<lpage>120</lpage>.</citation>
</ref>
<ref id="bibr23-1948550611430272">
<citation citation-type="book">
<person-group person-group-type="author">
<name>
<surname>Todorov</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Dotsch</surname>
<given-names>R.</given-names>
</name>
<name>
<surname>Wigboldus</surname>
<given-names>D. H. J.</given-names>
</name>
<name>
<surname>Said</surname>
<given-names>C. P.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Data-driven methods for modeling social perception</article-title>. <source>Social and Personality Psychology Compass</source>, <volume>5</volume> (<issue>10</issue>), <fpage>775</fpage>–<lpage>791</lpage>. <comment>doi: 10.1111/j.1751-9004.2011.00389.x</comment></citation>
</ref>
<ref id="bibr24-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Todorov</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Oosterhof</surname>
<given-names>N. N.</given-names>
</name>
</person-group> (<year>2011</year>). <article-title>Modeling social perception of faces</article-title>. <source>Signal Processing Magazine, IEEE</source>, <volume>28</volume>, <fpage>117</fpage>–<lpage>122</lpage>.</citation>
</ref>
<ref id="bibr25-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Todorov</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Pakrashi</surname>
<given-names>M.</given-names>
</name>
<name>
<surname>Oosterhof</surname>
<given-names>N. N.</given-names>
</name>
</person-group> (<year>2009</year>). <article-title>Evaluating faces on trustworthiness after minimal time exposure</article-title>. <source>Social Cognition</source>, <volume>27</volume>, <fpage>813</fpage>–<lpage>833</lpage>.</citation>
</ref>
<ref id="bibr26-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Todorov</surname>
<given-names>A.</given-names>
</name>
<name>
<surname>Said</surname>
<given-names>C. P.</given-names>
</name>
<name>
<surname>Engell</surname>
<given-names>A. D.</given-names>
</name>
<name>
<surname>Oosterhof</surname>
<given-names>N. N.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Understanding evaluation of faces on social dimensions</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>12</volume>, <fpage>455</fpage>–<lpage>460</lpage>.</citation>
</ref>
<ref id="bibr27-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Victor</surname>
<given-names>J. D.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Analyzing receptive fields, classification images and functional images: Challenges with opportunities for synergy</article-title>. <source>Nature Neuroscience</source>, <volume>8</volume>, <fpage>1651</fpage>–<lpage>1656</lpage>.</citation>
</ref>
<ref id="bibr28-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Willis</surname>
<given-names>J.</given-names>
</name>
<name>
<surname>Todorov</surname>
<given-names>A.</given-names>
</name>
</person-group> (<year>2006</year>). <article-title>First impressions: Making up your mind after 100 ms exposure to a face</article-title>. <source>Psychological Science</source>, <volume>17</volume>, <fpage>592</fpage>–<lpage>598</lpage>.</citation>
</ref>
<ref id="bibr29-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Wojciszke</surname>
<given-names>B.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Morality and competence in person and self perception</article-title>. <source>European Review of Social Psychology</source>, <volume>16</volume>, <fpage>155</fpage>–<lpage>188</lpage>.</citation>
</ref>
<ref id="bibr30-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yzerbyt</surname>
<given-names>V. Y.</given-names>
</name>
<name>
<surname>Kervyn</surname>
<given-names>N.</given-names>
</name>
<name>
<surname>Judd</surname>
<given-names>C. M.</given-names>
</name>
</person-group> (<year>2008</year>). <article-title>Compensation versus halo: The unique relations between the fundamental dimensions of social judgment</article-title>. <source>Personality and Social Psychology Bulletin</source>, <volume>34</volume>, <fpage>1110</fpage>–<lpage>1123</lpage>.</citation>
</ref>
<ref id="bibr31-1948550611430272">
<citation citation-type="journal">
<person-group person-group-type="author">
<name>
<surname>Yzerbyt</surname>
<given-names>V. Y.</given-names>
</name>
<name>
<surname>Provost</surname>
<given-names>V.</given-names>
</name>
<name>
<surname>Corneille</surname>
<given-names>O.</given-names>
</name>
</person-group> (<year>2005</year>). <article-title>Not competent but warm. Really? Compensatory stereotypes in the French-speaking world</article-title>. <source>Group Processes and Intergroup Relations</source>, <volume>8</volume>, <fpage>291</fpage>–<lpage>308</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>