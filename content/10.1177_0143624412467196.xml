<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">BSE</journal-id>
<journal-id journal-id-type="hwp">spbse</journal-id>
<journal-title>Building Services Engineering Research &amp; Technology</journal-title>
<issn pub-type="ppub">0143-6244</issn>
<issn pub-type="epub">1477-0849</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0143624412467196</article-id>
<article-id pub-id-type="publisher-id">10.1177_0143624412467196</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Articles</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Data Centre Energy Efficiency Analysis to minimize total cost of ownership</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Flucker</surname><given-names>Sophia</given-names></name>
<xref ref-type="aff" rid="aff1-0143624412467196">1</xref>
<xref ref-type="corresp" rid="corresp1-0143624412467196"/>
</contrib>
<contrib contrib-type="author">
<name><surname>Tozer</surname><given-names>Robert</given-names></name>
<xref ref-type="aff" rid="aff2-0143624412467196">2</xref>
</contrib>
<aff id="aff1-0143624412467196"><label>1</label>MIMechE, Operational Intelligence Ltd.</aff>
<aff id="aff2-0143624412467196"><label>2</label>MCIBSE MASHRAE MIoR, Operational Intelligence Ltd.</aff>
</contrib-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Dwyer</surname><given-names>T</given-names></name>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-0143624412467196">Sophia Flucker, Operational Intelligence Ltd. Email: <email>sophiaflucker@dc-oi.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>2</month>
<year>2013</year>
</pub-date>
<volume>34</volume>
<issue>1</issue>
<issue-title>Special Issue: Adaptation and resilience to a changing climate: Supporting adaptation decision making</issue-title>
<fpage>103</fpage>
<lpage>117</lpage>
<permissions>
<copyright-statement>© The Chartered Institution of Building Services Engineers 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="sage">SAGE Publications</copyright-holder>
</permissions>
<abstract>
<p>The data centre industry grows with our increasing demand for IT services; energy forms a large part of data centre operating costs. Operators face increasing pressure to reduce their environmental impact and deliver competitively priced services to support users. However, many existing data centres were designed without a ‘green’ design brief; priorities were focussed around infrastructure redundancy. There are significant opportunities to implement modifications to design and operation and reduce operating costs, particularly to cooling systems, including managing data hall air, operating at higher temperatures, using free cooling and optimising for part load operation. Energy analysis can identify areas for improvement, often with short payback periods. This enables operators to realise massive reductions in energy (and sometimes capital) costs, whilst still delivering a reliable service.</p>
<p><italic><bold>Practical application:</bold></italic> The paper describes the challenges of reducing energy consumption and operating cost faced by those working in the data centre industry, along with tools and solutions. Details on where and how to achieve energy savings are presented, whilst maintaining the required availability for both new build and legacy facilities. Practical examples of how to put theory into practice are explained, including case studies where real clients have implemented improvements.</p>
</abstract>
<kwd-group>
<kwd>Data centre</kwd>
<kwd>energy efficiency</kwd>
<kwd>total cost of ownership</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="sec1-0143624412467196" sec-type="intro"><title>Introduction</title>
<p>The aim of this research is to examine how analysis of energy usage in data centres can inform design and operational decisions and thus help operators achieve cost savings. This is particularly relevant in an industry where demand and spend are increasing.</p>
<p>Increasing use of IT services results in the growth of data centres, which house the IT infrastructure to deliver the required computer processing. Data centres may be dedicated facilities or occupy space in a mixed use building and range from tens to thousands of kW of IT load. IT hardware such as servers, switches etc. is kept in the data hall (or server room) and may be rack mounted or stand alone. The mechanical and electrical infrastructure which supports the IT equipment comprises power and cooling plant such as back-up generators, uninterruptable power supply (UPS) units, chillers etc. housed in plant rooms, and the corresponding distribution systems. The area required to accommodate the plant is usually at least the same area as the data hall itself and is load density and redundancy dependent.</p>
<p>The total cost of ownership for a data centre may be described as having three elements, as illustrated in <xref ref-type="fig" rid="fig1-0143624412467196">Figure 1</xref>;
<list id="list1-0143624412467196" list-type="order">
<list-item><p>the capital cost or investment cost of the construction and installation of the building, infrastructure and hardware,</p></list-item>
<list-item><p>the reliability costs, relating to the cost of failure/cost to avoid failure,</p></list-item>
<list-item><p>the operating costs, which include staff costs and consumables, of which energy is a major component.</p></list-item>
</list>
<fig id="fig1-0143624412467196" position="float"><label>Figure 1.</label><caption><p>Data centre total cost of ownership</p></caption><graphic xlink:href="10.1177_0143624412467196-fig1.tif"/>
</fig></p>
<p>These costs are significantly higher compared to those for an office building.<sup><xref ref-type="bibr" rid="bibr1-0143624412467196">1</xref></sup> The fit-out cost of the mechanical and electrical infrastructure also exceeds the building fabric costs due to:
<list id="list2-0143624412467196" list-type="bullet">
<list-item><p>the load density (2 kW/m<sup>2</sup>+)</p></list-item>
<list-item><p>use of redundant systems and plant designed to provide reliability and allow for concurrent maintainability (without downtime)/fault tolerant infrastructure</p></list-item>
<list-item><p>use of specialist systems/equipment.</p></list-item>
</list></p>
<p>Data centres are described as mission critical facilities; they are essential for the business to carry out its mission and hence any interruption in service, downtime or unavailability usually has a significant cost impact. The cost of downtime varies according to the business type, resilience architecture and specific activities occurring in each particular data centre.<sup><xref ref-type="bibr" rid="bibr2-0143624412467196">2</xref></sup> The mechanical, electrical and IT infrastructure design and disaster recovery planning can mitigate the failure impact. For example, an investment bank may see a significant loss of profits in the event that one of their data centres is not functioning during the trading day. This is not just due to the inability to trade but also the impact on their share price and reputational damage. The reliability costs are intangible and do not appear in the company financial accounts. They are often forgotten and investment cut due to ignorance or complacency if the organisation has not experienced a failure for a long time; however, the cost of failure comes off the bottom line.</p>
<p>Redundant power and cooling infrastructure contribute towards delivering uptime, but resilience can also be addressed at the application layer (through IT services/software hosted at different facilities) and the human part of operating data centres is also very important. In fact, the vast majority of failures are due to human error, reported at around 70% in the data centre industry.<sup><xref ref-type="bibr" rid="bibr3-0143624412467196">3</xref></sup> This is similar to analysis of failures in other industries (80% attributed to human error).<sup><xref ref-type="bibr" rid="bibr4-0143624412467196">4</xref></sup></p>
<p>Data centre operating costs are high due to the skilled staffing levels required to ensure continuous operation (critical sites are manned 24/7) and the requirement to perform maintenance out of hours to reduce the risk of impact to the business. Highly trained staff also help to make the facility resilient; demonstrated by the speed at which the service is restored to normal following a failure event. Energy costs are high due to the load densities. Many facilities have opportunities to reduce the energy requirements of their mechanical and electrical infrastructure, which may be the same load as the IT or even more. This may allow the available IT capacity of the facility to increase.</p>
<p>When designing and operating a data centre, decisions should be made considering the total cost of ownership, for example, investing more in the capital cost of the build in order to benefit from a reduced operating cost during the facility lifetime; or investing in redundant infrastructure to reduce the downtime of the facility. A business case can be presented for different options, which outline their return on investment.</p>
<p>Operators are under pressure to minimise their costs, whether this is internally within the organisation for an enterprise data centre or in a colocation data centre, where operators compete for customers and there is a stronger link between the data centre operating cost and company profitability. Cost is particularly high profile in a recession where budgets are tight. Spending restrictions may force data centre operators to delay or avoid building new facilities and continue operating legacy facilities. Often these will require upgrades to prolong their life in order to supplement available capacity or replace ageing plant.</p>
<p>In recent years, the industry has started to focus on how to operate more efficient data centres. The main drivers for this are
<list id="list3-0143624412467196" list-type="bullet">
<list-item><p>Increasing energy costs<sup><xref ref-type="bibr" rid="bibr5-0143624412467196">5</xref></sup></p></list-item>
<list-item><p>Regulatory requirements</p></list-item>
<list-item><p>Market pressures around Corporate Social Responsibility (CSR)</p></list-item>
</list></p>
<p>A 1 MW IT load facility, with total load of 2 MW and utility rate of £0.10/kWh, has an annual energy bill of £1 752,000. DECC reported that between 2005 and 2010, the average price of electricity in the industrial sector rose by 44% between 2005 and 2010.<sup><xref ref-type="bibr" rid="bibr6-0143624412467196">6</xref></sup> So, if this trend continues and the rate increases to £0.14/kWh, the resulting additional annual cost equates to £700 800.</p>
<p>Government policy is increasingly driving data centre operators towards improving their energy efficiency,<sup><xref ref-type="bibr" rid="bibr7-0143624412467196">7</xref></sup> for example the Carbon Reduction Commitment in the UK which incentivises energy reduction by requiring qualifying companies to pay for their carbon emissions.<sup><xref ref-type="bibr" rid="bibr8-0143624412467196">8</xref></sup></p>
<p>‘Green’ issues now have a higher profile in the media and companies often publicise their sustainability achievements in their annual report and marketing material. It can provide a competitive advantage where customers make environmental performance part of their decision-making process. Many operators market themselves using their power usage effectiveness (PUE) performance and participation in the EU Code of Conduct for data centre energy efficiency. Their customers may also expect to receive some of the associated cost benefits.</p>
<p>New data centre designs are now in operation where energy efficient operation was a requirement of the design brief; these have a significantly reduced energy cost and often also a reduced capital cost. In some cases, the operator experience falls short of the design expectation of energy efficiency. This also applies outside the data centre industry; surveys of recently completed buildings regularly reveal massive gaps between client and design expectations and delivered performance, especially energy performance.<sup><xref ref-type="bibr" rid="bibr9-0143624412467196">9</xref></sup> It is important that a successful knowledge transfer and handover strategy is in place to tackle this.</p>
<p>It is also possible to implement improvements in legacy facilities despite the restrictions associated with making changes to a live critical environment. Often operators are concerned that energy efficiency could compromise reliability which makes them reluctant to address this issue. This need not be the case, however, it is important that changes are managed in a way which minimises risk, usually involving specialist vendors to optimise plant and systems.</p>
</sec>
<sec id="sec2-0143624412467196"><title>Analysis</title>
<p>Analysis of the different components of a data centre’s energy consumption is made easier where a metering system is in place, with sub-meters monitoring the power/energy usage of the various loads. The UK building regulations state a target figure of at least 90% of the estimated annual energy consumption to be assigned to different categories of load.<sup><xref ref-type="bibr" rid="bibr10-0143624412467196">10</xref></sup> Ideally, the metering system is configured for automatic reporting of live consumption and trends.</p>
<p>The energy consumption of the different components of a data centre’s total load can be broken down as follows, listed in decreasing order for a typical facility:
<list id="list4-0143624412467196" list-type="bullet">
<list-item><p>IT equipment</p></list-item>
<list-item><p>Cooling systems supporting IT equipment and other technical spaces e.g. UPS plant rooms:
<list id="list5-0143624412467196" list-type="simple">
<list-item><p>○ Refrigeration systems (including compressors, pumps, heat rejection fans)</p></list-item>
<list-item><p>○ Air movement (cooling unit fans, ventilation)</p></list-item>
<list-item><p>○ Humidification</p></list-item>
</list></p></list-item>
<list-item><p>Power systems supporting IT equipment:
<list id="list6-0143624412467196" list-type="simple">
<list-item><p>○ UPS system losses</p></list-item>
<list-item><p>○ Generator heaters</p></list-item>
<list-item><p>○ Lighting</p></list-item>
<list-item><p>○ Electrical distribution losses</p></list-item>
</list></p></list-item>
</list></p>
<p>The recommended strategy when addressing data centre energy consumption is to start with the root cause, i.e. the IT equipment, as any reduction of load made here will in turn reduce the cooling and power requirements. The IT equipment load can be reduced through consolidation and virtualisation (see <xref ref-type="fig" rid="fig2-0143624412467196">Figure 2</xref>).
<fig id="fig2-0143624412467196" position="float"><label>Figure 2.</label><caption><p>Data centre energy strategy</p></caption><graphic xlink:href="10.1177_0143624412467196-fig2.tif"/>
</fig></p>
<p>A popular metric for reporting a data centre energy efficiency is PUE, defined as the ratio of total data centre energy consumed (including for cooling and power distribution losses) divided by IT energy consumed. Data Centre Infrastructure Effectiveness or DCiE is the inverse.<sup><xref ref-type="bibr" rid="bibr11-0143624412467196">11</xref></sup> These metrics indicate how efficient a data centre’s mechanical and electrical infrastructure is but do not address the IT efficiency. Other metrics have been proposed to describe IT efficiency, but it has proven challenging to find something simple, easy to understand, measurable and universally relevant to cover the range of computing output occurring in data centres.<sup><xref ref-type="bibr" rid="bibr12-0143624412467196">12</xref></sup></p>
<p>Typical legacy data centres that do not use free cooling have measured PUE values of 1.8, 2 or higher (DCiE = 0.56, 0.5 or lower) and new designs can achieve PUE values of 1.2 or lower (DCiE = 0.83 or higher), as shown in <xref ref-type="fig" rid="fig3-0143624412467196">Figure 3</xref>.
<fig id="fig3-0143624412467196" position="float"><label>Figure 3.</label><caption><p>Data centre power usage effectiveness (PUE) breakdown comparison</p></caption><graphic xlink:href="10.1177_0143624412467196-fig3.tif"/>
</fig></p>
<p>This is accomplished by minimising the energy consumption of the cooling systems, through:
<list id="list7-0143624412467196" list-type="bullet">
<list-item><p>Managing air in the data hall; minimising bypass and recirculation by physically separating hot and cold air streams (through containment) and controlling on cooling unit supply air temperature rather than return air to establish narrow range of temperatures at IT equipment inlet</p></list-item>
<list-item><p>Reducing cooling unit fan speeds/air volumes through use of variable frequency drives</p></list-item>
<list-item><p>Increasing operating temperatures to deliver air at the server inlet in line with IT hardware vendor specifications, 18–27°C recommended and higher allowable,<sup><xref ref-type="bibr" rid="bibr13-0143624412467196">13</xref></sup> in order to improve refrigeration efficiency and increase free cooling opportunity.</p></list-item>
</list></p>
<p>Most legacy data centres suffer from poor air management, which means that cooling is not delivered to where it is required. Energy is wasted in creating very low temperature air and moving around excessive air volumes but hot spots are still present, where some servers receive air above the recommended/allowable temperatures, which impacts reliability.<sup><xref ref-type="bibr" rid="bibr13-0143624412467196">13</xref></sup></p>
<p>The diagram below shows a summary of non-managed legacy data hall air flows in section with temperatures indicated by the colour scale and relative air volume by the arrow width. Bypass (BP), recirculation (R) and negative pressure (NP) are present. Temperature is controlled at the cooling unit return (top left). A range of air temperatures are supplied under the raised floor. A significant proportion of the supply air from the cooling unit is bypassing the IT equipment and mixing with the IT equipment return air. The IT equipment receives air with a range of temperatures (location dependent) mostly due to mixing of recirculated warm air. <xref ref-type="fig" rid="fig4-0143624412467196">Figure 4</xref> shows an example with bypass at 50%; the 1200 kW installed capacity of the cooling units cannot be realised as only 600 kW is available for the IT equipment, which is less than the 800 kW cooling demand. The cooling system is working as designed but it is not effective; hot spots are present and energy is wasted. These metrics can be defined by the data hall weighted average server and cooling unit supply and return temperatures (Tii, Tio, Tco, Tci); calculated using measured data.<sup><xref ref-type="bibr" rid="bibr14-0143624412467196">14</xref></sup>
<fig id="fig4-0143624412467196" position="float"><label>Figure 4.</label><caption><p>Air flow diagram: delivered cooling</p></caption><graphic xlink:href="10.1177_0143624412467196-fig4.tif"/>
</fig></p>
<p>Where refrigeration is used, energy consumption of cooling plant and therefore PUE varies with ambient conditions and is increased at higher temperatures. The outdoor ambient temperature affects the condensing temperature and the evaporating temperature is impacted by the cooling unit set points. Reducing the delta T between the condensing and evaporating temperatures, through decreasing the condensing temperature/increasing the evaporating temperature, means less work is required in the cooling cycle and hence improved efficiency is achieved, as shown in <xref ref-type="fig" rid="fig5-0143624412467196">Figure 5</xref>.
<fig id="fig5-0143624412467196" position="float"><label>Figure 5.</label><caption><p>Areas on <italic>T</italic>-s diagram representing cooling effect and work for ideal refrigeration cycle. <italic>T<sub>c</sub></italic>: condensing temperature; <italic>T<sub>e</sub></italic>: evaporating temperature; <italic>W</italic>: Work (compressor); <italic>Q<sub>e</sub></italic>: cooling (evaporator) and COP<sub>ideal</sub>: ideal coefficient of performance</p></caption><graphic xlink:href="10.1177_0143624412467196-fig5.tif"/>
</fig></p>
<p>However, with the increased server temperature range requirements, free cooling (or economized cooling) is possible throughout the world. The recommended and allowable ranges A1-A4 for class 1-4 equipment during operation are indicated on the psychrometric chart in <xref ref-type="fig" rid="fig6-0143624412467196">Figure 6</xref> and summarised in <xref ref-type="table" rid="table1-0143624412467196">Table 1</xref> below, as agreed by hardware vendors and published by ASHRAE Technical Committee 9.9.<sup><xref ref-type="bibr" rid="bibr13-0143624412467196">13</xref></sup>
<table-wrap id="table1-0143624412467196" position="float"><label>Table 1.</label><caption><p>Summary table of environmental conditions at IT equipment inlet published by ASHRAE</p></caption>
<graphic alternate-form-of="table1-0143624412467196" xlink:href="10.1177_0143624412467196-table1.tif"/>
<table frame="hsides"><thead align="left">
<tr><th>Class</th>
<th>Dry bulb temperature (°C)</th>
<th>Humidity range, noncondensing</th>
<th>Maximum dew point (°C)</th>
</tr></thead>
<tbody align="left">
<tr>
<td colspan="4">Recommended</td>
</tr>
<tr>
<td>A1 to A4</td>
<td>18–27</td>
<td>5.5°C dew point (DP) to 60% relative humidity (RH) and 15°C DP</td>
<td/>
</tr>
<tr>
<td colspan="4">Allowable</td>
</tr>
<tr>
<td>A1</td>
<td>15–32</td>
<td>20%–80% RH</td>
<td>17</td>
</tr>
<tr>
<td>A2</td>
<td>10–35</td>
<td>20%–80% RH</td>
<td>21</td>
</tr>
<tr>
<td>A3</td>
<td>5–40</td>
<td>–12°C DP and 8%–85% RH</td>
<td>24</td>
</tr>
<tr>
<td>A4</td>
<td>5–45</td>
<td>–12°C DP and 8%–90% RH</td>
<td>24</td>
</tr>
</tbody>
</table>
</table-wrap>
<fig id="fig6-0143624412467196" position="float"><label>Figure 6.</label><caption><p>Psychrometric chart indicating ASHRAE environmental ranges classes A1–A4 and London Gatwick weather data</p></caption><graphic xlink:href="10.1177_0143624412467196-fig6.tif"/>
</fig></p>
<p>The London climate envelope is also shown shaded on the chart, with the inner zones indicating the occurrence of more than 90% of annual hours. This data suggests that ambient air at this location does not exceed the allowable maximum temperature for class A2 and rarely for class A1 and hence free cooling should be exploited.</p>
<p>There are various methods for implementing such a solution; direct air, indirect air, water side, etc. Different solutions are appropriate depending on the facility requirements and location (climate, local environment). Air side free cooling solutions can achieve lower PUEs; indirect air free cooling designs are effective in a wider range of climates and allow reduced refrigeration requirements (direct air side free cooling designs often have 100% refrigeration capacity installed for back-up in case of outdoor pollution incidents). Air side free cooling solutions have increased plant footprint requirements and hence are usually only possible for new build facilities, thus water side free cooling may be the only option where there are space restrictions, such as when retrofitting to an existing installation.<sup><xref ref-type="bibr" rid="bibr14-0143624412467196">14</xref></sup></p>
<p>Adiabatic or evaporative cooling is often employed as part of these solutions to reduce refrigeration requirements particularly to treat hot dry ambient air. It requires significantly less energy than refrigeration. <xref ref-type="fig" rid="fig7-0143624412467196">Figure 7</xref> below indicates that for a cooling system where free cooling is available below 10°C, approximately 1000 additional hours of free cooling are available when adiabatic cooling is introduced, which is governed by wet bulb rather than dry bulb ambient temperature, assuming same approach temperature (dry and wet bulb) between outdoor air and supply air.
<fig id="fig7-0143624412467196" position="float"><label>Figure 7.</label><caption><p>Cumulative distribution of dry and wet bulb temperatures at London Gatwick</p></caption><graphic xlink:href="10.1177_0143624412467196-fig7.tif"/>
</fig></p>
<p>In many climates, 100% free cooling or zero refrigeration is possible.<sup><xref ref-type="bibr" rid="bibr15-0143624412467196">15</xref></sup> This reduces the energy cost but also the operating cost associated with maintenance and the capital cost by approximately 20–30%, as removing refrigeration plant has the knock-on effect of reducing the maximum load and hence the electrical infrastructure can also be reduced, as shown in <xref ref-type="fig" rid="fig8-0143624412467196">Figure 8</xref>. The infrastructure is less complex, which can help reduce the risk of human error, thus improving reliability cost.
<fig id="fig8-0143624412467196" position="float"><label>Figure 8.</label><caption><p>Additional plant and capacity requirements with refrigeration</p></caption><graphic xlink:href="10.1177_0143624412467196-fig8.tif"/>
</fig></p>
<p>An interesting analysis is to model the energy consumption of the refrigeration system components across the range of design ambient temperatures and consider the distribution of temperatures across this range i.e., what is the weighted average consumption, in which zone does the system operate most of the time? Example shown in <xref ref-type="fig" rid="fig9-0143624412467196">Figure 9</xref>.
<fig id="fig9-0143624412467196" position="float"><label>Figure 9.</label><caption><p>Example cooling system power and energy consumption analysis showing variation with ambient temperature</p></caption><graphic xlink:href="10.1177_0143624412467196-fig9.tif"/>
</fig></p>
<p>In many cases, analysis of data hall air flows reveals that the volume of air moved by the cooling units fans is far in excess of the volume required by the IT equipment. Ideally, for contained systems, the volume supplied should be slightly more than the demand to maintain a positive pressure in the cold aisle. A cube law relationship exists between the volumetric flow rate and the power draw of the fan and hence minimising the flow rate results in a dramatic reduction in energy consumption, see <xref ref-type="fig" rid="fig10-0143624412467196">Figure 10</xref> and example in <xref ref-type="table" rid="table2-0143624412467196">Table 2</xref>.
<table-wrap id="table2-0143624412467196" position="float"><label>Table 2.</label><caption><p>Comparative power draw example with five cooling units</p></caption>
<graphic alternate-form-of="table2-0143624412467196" xlink:href="10.1177_0143624412467196-table2.tif"/>
<table frame="hsides"><thead align="left">
<tr><th>Fixed speed all run (4 + 1 redundant)</th>
<th>Fixed speed (standby unit off)</th>
<th>Cooling units with VFDs (@4/5 speed)</th>
</tr></thead>
<tbody align="left">
<tr>
<td>8 kW</td>
<td>8 kW</td>
<td><inline-formula id="ilm1-0143624412467196"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math1-0143624412467196"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo/><mml:mo>(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo/><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mtext>kW</mml:mtext></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>8 kW</td>
<td>8 kW</td>
<td><inline-formula id="ilm2-0143624412467196"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math2-0143624412467196"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo/><mml:mo>(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo/><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mtext>kW</mml:mtext></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>8 kW</td>
<td>8 kW</td>
<td><inline-formula id="ilm3-0143624412467196"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math3-0143624412467196"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo/><mml:mo>(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo/><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mtext>kW</mml:mtext></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>8 kW</td>
<td>8 kW</td>
<td><inline-formula id="ilm4-0143624412467196"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math4-0143624412467196"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo/><mml:mo>(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo/><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mtext>kW</mml:mtext></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>8 kW</td>
<td>Off</td>
<td><inline-formula id="ilm5-0143624412467196"><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline" id="mml-math5-0143624412467196"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:mrow><mml:mo/><mml:mo>(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo/><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mtext>kW</mml:mtext></mml:mrow></mml:math></inline-formula></td>
</tr>
<tr>
<td>40 kW</td>
<td>32 kW</td>
<td>20 kW</td>
</tr>
</tbody>
</table>
<table-wrap-foot>
<fn id="table-fn1-0143624412467196"><p>VFD: variable frequency drives.</p></fn></table-wrap-foot>
</table-wrap>
<fig id="fig10-0143624412467196" position="float"><label>Figure 10.</label><caption><p>Relationship between fan speed, power, flow rate and pressure.<sup><xref ref-type="bibr" rid="bibr16-0143624412467196">16</xref></sup> Graph reproduced with permission of CIBSE</p></caption><graphic xlink:href="10.1177_0143624412467196-fig10.tif"/>
</fig></p>
<p>Legacy facilities often have fixed speed fans, sometimes with redundant units on standby. Often these can be retrofitted with variable speed drives (VSD also known as variable frequency drives, VFD) and electronically commutated (EC) fans that use significantly less power; the payback time for the former can be less than 2 years<sup><xref ref-type="bibr" rid="bibr17-0143624412467196">17</xref></sup>; periods of less than 1 year are also reported.</p>
<p>In addition, this solution is more reliable than with one unit on standby, as upon failure of a unit, the remaining units are already running and will increase their speed; they do not need to start-up. Also there will be less wear on components running at lower speed.</p>
<p>There may be some energy reduction opportunities in the electrical distribution system, such as provisioning UPS modules to match load plus redundancy to allow operation at higher efficiencies, operating UPS in offline mode (particularly for UPS supporting cooling fans and pumps where used), adjusting generator heater setpoints, reducing lighting operating hours. However, options to make changes made to critical power distribution may be limited where reliability cannot be compromised.</p>
<p>PUE also varies with part load, with a minimum at full IT load and maximum at near zero IT load. Although mechanical and electrical energy overheads are reduced with a low IT load, this reduction may not be proportional, as shown in <xref ref-type="fig" rid="fig11-0143624412467196">Figure 11</xref>. These overheads have constant and variable components; best efficiency is achieved when the constant component is minimised.
<fig id="fig11-0143624412467196" position="float"><label>Figure 11.</label><caption><p>Facility total load vs. IT load</p></caption><graphic xlink:href="10.1177_0143624412467196-fig11.tif"/>
</fig></p>
<p>Part load efficiency can be improved by ensuring part load scenarios are analysed during the design phase and optimising how the mechanical and electrical systems scale with load. The effectiveness of how efficiency scales with load is defined as scalability<sup><xref ref-type="bibr" rid="bibr18-0143624412467196">18</xref></sup>; it is equal to the ratio of the operating line compared to that of the full load PUE. A modular design approach helps reduce energy consumption while the facility operates below maximum load; the facility may operate in this state for several years and may never become fully loaded. Designers should report on the expected subsystem energy breakdown (from which PUE or DCiE can be derived) at different part loads to allow operators to review whether the systems perform as expected and investigate any anomalies.</p>
<p>It is important to note the limitations of PUE; it does not show the complete picture of facility energy performance. It improves as IT load increases, as the mechanical and electrical system overheads now become a smaller ratio of the total load. IT load can increase without additional servers being installed, for example, if the server fan speeds increase, through operating at higher temperatures. This should be offset by the savings to the cooling systems and the total energy consumed should be reviewed in order to understand the impact.</p>
<p>A list of best practices is available in the EU Code of Conduct for Data Centres Best Practice Guidelines,<sup><xref ref-type="bibr" rid="bibr19-0143624412467196">19</xref></sup> which include the recommendations set out in this paper. The practices relating to power and cooling are described as ‘expected’, ‘expected for new build or retrofit’ and ‘optional’, to assist operators in assessing which are appropriate for legacy facilities/fit outs.</p>
</sec>
<sec id="sec3-0143624412467196" sec-type="cases"><title>Case studies</title>
<p>The following case studies describe the results achieved by a series of risk and energy workshops held with data centre operational teams in two western European locations in 2011. The objectives of these workshops were to produce recommendations for reducing facility risk and energy consumption. The energy efficiency improvements identified had to be achieved without impacting reliability and their implementation managed so as to manage risk. Through knowledge sharing and exploring opportunities in an open forum with different stakeholders, the operational team had the confidence to propose changes and take ownership of their implementation. The best achievable PUE for the installed system in each location for a new build was identified in order to benchmark performance.<sup><xref ref-type="bibr" rid="bibr14-0143624412467196">14</xref></sup> These were 1.5 and 1.23, respectively, but the actual target figures identified were short of these due to site specific restrictions and longer payback periods required to reach these minima.</p>
<sec id="sec4-0143624412467196"><title>Case study 1</title>
<p>The facility is a tier 3 legacy data centre with 3.2 MW IT load at the time of the project in an 8000 m<sup>2</sup> data hall area. The PUE at the start of the programme was 1.85 and some energy saving best practices had already been implemented. The cooling system is a DX/glycol condenser system, with free cooling available at low ambient conditions. A detailed study of data hall air performance and analysis of the cooling system control strategies was undertaken with the operational team and opportunities for improvement identified. These included completion of air management improvements to minimise bypass and recirculation and changing to supply air control in order to allow cooling system set points increases of 3°C. No recommendations were made in relation to reducing the energy losses in the electrical systems due to the high investment cost and risk involved. The target PUE after implementation of these steps was 1.67, resulting in an annual energy saving of $519,000/year with an investment of $270,000 and therefore a simple payback of around 6 months.</p>
</sec>
<sec id="sec5-0143624412467196"><title>Case study 2</title>
<p>The facility is a tier 3 data centre operating since 2008 with a 3 MW IT load at the time of the project in a 4000 m<sup>2</sup> data hall area. The facility uses direct air side free cooling technology. A detailed study of the data hall air performance and analysis of fan control strategy and subsystem energy consumption was undertaken with the operational team and opportunities for improvement identified. These included optimisation of fan control strategy and differential pressure set points and change of UPS system operating mode. The starting PUE was 1.54 and the recommendations identified steps to reduce this to 1.30, resulting in a €665,000 annual saving with no CAPEX investment. At the time of writing, many of the recommendations had been implemented and the improved PUE was close to the target at 1.35.</p>
</sec>
</sec>
<sec id="sec6-0143624412467196" sec-type="conclusions"><title>Conclusions</title>
<p>Through analysis of data centre energy efficiency, opportunities for system optimisation, implementation of best practice and energy reduction can be identified. Practically every facility can implement changes to reduce the energy consumption of the cooling systems, regardless of age. Many improvements can be put in place with short payback periods of less than 1 year. Examining options through engagement and collaboration with stakeholders allows a realistic target reduction to be established along with the series of steps needed to reach it. Through the sharing of experience, the process also helps to improve the knowledge of participants and deliver cost reductions while managing risk. There is a strong business case for applying this methodology to minimise data centre total cost of ownership.</p>
</sec>
</body>
<back>
<sec id="sec7-0143624412467196"><title>Funding</title>
<p>This research received no specific grant from any funding agency in the public, commercial, or not-for-profit sectors.</p>
</sec>
<ref-list>
<title>References</title>
<ref id="bibr1-0143624412467196"><label>1</label><citation citation-type="book"><collab>The Uptime Institute</collab>. <source>Cost model: dollars per kW plus dollars per square foot of computer floor</source>, <comment>Available at: <ext-link ext-link-type="uri" xlink:href="http://ticketing.lamdahellix.com/%5CUserFiles%5CFile%5Cdownloads%5CDollars_KW_plus_Dollars_Square.pdf">http://ticketing.lamdahellix.com/%5CUserFiles%5CFile%5Cdownloads%5CDollars_KW_plus_Dollars_Square.pdf</ext-link></comment>, <year>2008</year>.</citation></ref>
<ref id="bibr2-0143624412467196"><label>2</label><citation citation-type="book"><collab>Liebert Corporation</collab>. <source>Understanding the cost of data center downtime: an analysis of the financial impact on infrastructure vulnerability</source>, <comment>Available at: <ext-link ext-link-type="uri" xlink:href="http://emersonnetworkpower.com/en-US/Brands/Liebert/Documents/White%20Papers/data-center-uptime_24661-R05-11.pdf">http://ticketing.lamdahellix.com/%5CUserFiles%5CFile%5Cdownloads%5CDollars_KW_plus_Dollars_Square.pdf</ext-link></comment>, <year>2011</year>.</citation></ref>
<ref id="bibr3-0143624412467196"><label>3</label><citation citation-type="book"><collab>The Uptime Institute</collab>. <source>Data center site infrastructure tier standard: operational sustainability</source>, <comment>Available at: <ext-link ext-link-type="uri" xlink:href="http://uptimeinstitute.com/publications">http://uptimeinstitute.com/publications</ext-link></comment>, <year>2010</year>.</citation></ref>
<ref id="bibr4-0143624412467196"><label>4</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Duffey</surname><given-names>R</given-names></name><name><surname>Saull</surname><given-names>J</given-names></name></person-group>. <source>Managing risk: the human element</source>, <publisher-name>UK: Wiley</publisher-name>, <year>2008</year>.</citation></ref>
<ref id="bibr5-0143624412467196"><label>5</label><citation citation-type="other"><comment>Singh H. Utility rate breakdown – drivers, influencers and outlook, <ext-link ext-link-type="uri" xlink:href="http://www.thearcticexplorer.com/Links_&amp;_Downloads_files/Utility%20Rate%20Structure%20v2.pdf">http://www.thearcticexplorer.com/Links_&amp;_Downloads_files/Utility%20Rate%20Structure20v2.pdf</ext-link> (January 2011)</comment>.</citation></ref>
<ref id="bibr6-0143624412467196"><label>6</label><citation citation-type="other"><comment>Department of Energy &amp; Climate Change. <italic>Quarterly energy prices</italic>. National Statistics, 2011. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.statistics.gov.uk/hub/release-calendar/index.html?newquery=*&amp;uday=0&amp;umonth=0&amp;uyear=0&amp;title=Quarterly+Energy+Prices&amp;pagetype=calendar-entry&amp;lday=&amp;lmonth=&amp;lyear=">http://www.statistics.gov.uk/hub/release-calendar/index.html?newquery=*&amp;uday=0&amp;umonth=0&amp;uyear=0&amp;title=Quarterly+Energy+Prices&amp;pagetype=calendar-entry&amp;lday=&amp;lmonth=&amp;lyear=</ext-link></comment>.</citation></ref>
<ref id="bibr7-0143624412467196"><label>7</label><citation citation-type="other"><comment>The Green Grid. The green grid energy policy research for data centres. White Paper #25. The Green Grid, 2009</comment>.</citation></ref>
<ref id="bibr8-0143624412467196"><label>8</label><citation citation-type="other"><comment>CRC Energy Efficiency Scheme, <ext-link ext-link-type="uri" xlink:href="http://www.environment-agency.gov.uk/business/topics/pollution/126698.aspx">http://www.environment-agency.gov.uk/business/topics/pollution/126698.aspx</ext-link> (November 2011)</comment>.</citation></ref>
<ref id="bibr9-0143624412467196"><label>9</label><citation citation-type="other"><comment>BSRIA. <italic>The soft landings framework BSRIA BG 4/2009</italic>. UK: BSRIA, 2009</comment>.</citation></ref>
<ref id="bibr10-0143624412467196"><label>10</label><citation citation-type="other"><comment>HM Government. <italic>The building regulations 2000, conservation of fuel and power</italic>, approved document L2A, 2010 edition</comment>.</citation></ref>
<ref id="bibr11-0143624412467196"><label>11</label><citation citation-type="other"><comment>The Green Grid. Green grid data center power efficiency metrics: PUE and DCiE. White Paper #6. The Green Grid, 2007</comment>.</citation></ref>
<ref id="bibr12-0143624412467196"><label>12</label><citation citation-type="other"><comment>The Green Grid. Productivity proxy proposals feedback: interim results. White Paper #24. The Green Grid, 2009</comment>.</citation></ref>
<ref id="bibr13-0143624412467196"><label>13</label><citation citation-type="other"><comment>ASHRAE. <italic>Thermal guidelines for data processing environments – expanded data center classes and usage guidance</italic>. USA: ASHRAE, 2011</comment>.</citation></ref>
<ref id="bibr14-0143624412467196"><label>14</label><citation citation-type="confproc"><person-group person-group-type="author"><name><surname>Flucker</surname><given-names>S</given-names></name><name><surname>Tozer</surname><given-names>R</given-names></name></person-group>. <source>Data centre cooling air performance metrics</source>. <conf-name>CIBSE Technical Symposium</conf-name>. <conf-loc>Leicester</conf-loc>. <comment>CIBSE, 2011</comment>.</citation></ref>
<ref id="bibr15-0143624412467196"><label>15</label><citation citation-type="book"><person-group person-group-type="author"><name><surname>Tozer</surname><given-names>R</given-names></name></person-group>. <source>Global data centre energy strategy</source>, <publisher-name>Data Center Dynamics note</publisher-name>, <year>2009</year>. <comment>. Available at: <ext-link ext-link-type="uri" xlink:href="http://www.datacenterdynamics.com/fr/node/22811">http://www.datacenterdynamics.com/fr/node/22811</ext-link></comment>.</citation></ref>
<ref id="bibr16-0143624412467196"><label>16</label><citation citation-type="other"><comment>Fan Application Guide CIBSE TM42, 2006. Available at: <ext-link ext-link-type="uri" xlink:href="https://www.cibseknowledgeportal.co.uk/component/dynamicdatabase/?layout=publication&amp;revision_id=115">https://www.cibseknowledgeportal.co.uk/component/dynamicdatabase/?layout=publication&amp;revision_id=115</ext-link></comment>.</citation></ref>
<ref id="bibr17-0143624412467196"><label>17</label><citation citation-type="other"><comment>The Green Grid. Case study: the ROI of cooling system energy efficiency upgrades. White Paper #39. The Green Grid, 2011</comment>.</citation></ref>
<ref id="bibr18-0143624412467196"><label>18</label><citation citation-type="other"><comment>The Green Grid. PUE scalability metric and statistical analyses. White Paper #20. The Green Grid, 2009</comment>.</citation></ref>
<ref id="bibr19-0143624412467196"><label>19</label><citation citation-type="other"><comment>European Commission. <italic>Best practices for the EU code of conduct on data centres</italic>, version 3.0.8. European Commission, 2011. Available at: <ext-link ext-link-type="uri" xlink:href="http://iet.jrc.ec.europa.eu/energyefficiency/ict-codes-conduct/data-centres-energy-efficiency">http://iet.jrc.ec.europa.eu/energyefficiency/ict-codes-conduct/data-centres-energy-efficiency</ext-link></comment>.</citation></ref>
</ref-list>
</back>
</article>