<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="editorial">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MDM</journal-id>
<journal-id journal-id-type="hwp">spmdm</journal-id>
<journal-id journal-id-type="nlm-ta">Med Decis Making</journal-id>
<journal-title>Medical Decision Making</journal-title>
<issn pub-type="ppub">0272-989X</issn>
<issn pub-type="epub">1552-681X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0272989X12441398</article-id>
<article-id pub-id-type="publisher-id">10.1177_0272989X12441398</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Editorial</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Accounting for Biases When Linking Empirical Studies and Simulation Models</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Goldhaber-Fiebert</surname><given-names>Jeremy D.</given-names></name>
<degrees>PhD</degrees>
</contrib>
<aff id="aff1-0272989X12441398">Stanford University, Stanford, CA</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0272989X12441398">Jeremy D. Goldhaber-Fiebert, PhD, Stanford Health Policy, Centers for Health Policy and Primary Care and Outcomes Research, Stanford University, Stanford, CA 94305-6019; e-mail: <email>jeremygf@standord.edu</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>5</month>
<year>2012</year>
</pub-date>
<volume>32</volume>
<issue>3</issue>
<fpage>397</fpage>
<lpage>399</lpage>
<custom-meta-wrap>
<custom-meta>
<meta-name>cover-date</meta-name>
<meta-value>May–June 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>Comparative effectiveness and cost-effectiveness analyses have become increasingly common and important.<sup><xref ref-type="bibr" rid="bibr1-0272989X12441398">1</xref><xref ref-type="bibr" rid="bibr2-0272989X12441398"/>–<xref ref-type="bibr" rid="bibr3-0272989X12441398">3</xref></sup> Such analyses aim to assess multiple interventions by comparing their long-term outcomes and costs for real-world patient populations and care delivery settings. To inform current decisions, empirical studies of sufficient size and duration to assess long-term outcomes are often infeasible. Computer-based mathematical models can play a valuable role by providing the needed estimates. Such models generate outcomes based on inputs derived from shorter-term empirical studies. Therefore, appropriate methods for linking empirical studies to mathematical models underpin rigorous analyses. An important concern is that estimates from empirical studies contain a variety of potential biases that, if not corrected, can influence modeled outcomes and resulting policy recommendations.</p>
<p>With recent developments such as the establishment of the Patient Centered Outcomes Research Institute,<sup><xref ref-type="bibr" rid="bibr1-0272989X12441398">1</xref>,<xref ref-type="bibr" rid="bibr2-0272989X12441398">2</xref></sup> the methods literature on combining the strengths of empirical studies and mathematical models to support comparative effectiveness and cost-effectiveness analyses has grown rapidly. The article by Campos and others in this issue of <italic>Medical Decision Making</italic> contributes to this area.<sup><xref ref-type="bibr" rid="bibr4-0272989X12441398">4</xref></sup> The authors describe an approach for assessing and correcting potential biases in estimates from a randomized controlled trial (RCT) of cervical cancer screening technologies. They then provide an illustrative example of the importance of this approach by showing how such bias corrections might alter cervical cancer screening policy conclusions reached with biased estimates. Their approach is designed for situations where individual-level data from a single RCT are available. It represents a valuable tool for other researchers facing similar situations.</p>
<p>Their approach contributes to a consistent set of best practices for selecting, adjusting for bias, and incorporating empirical evidence into decision-analytic mathematical models, a need that has been previously acknowledged.<sup><xref ref-type="bibr" rid="bibr5-0272989X12441398">5</xref>,<xref ref-type="bibr" rid="bibr6-0272989X12441398">6</xref></sup> Such best practices can aid researchers who confront a number of typical situations when selecting empirical study estimates to use in their model-based analyses. These situations can be characterized by 1) the number of available studies (one v. more than one), 2) study types (RCTs v. observational studies), and 3) available information (summary estimates from published articles v. individual-level study data).</p>
<p>Ideally, when multiple studies are available, decision-analytic mathematical models should incorporate their combined information. Systematic review and meta-analytic methods, including Bayesian synthesis and meta-regression techniques, are well developed.<sup><xref ref-type="bibr" rid="bibr7-0272989X12441398">7</xref>,<xref ref-type="bibr" rid="bibr8-0272989X12441398">8</xref></sup> One important source of potential bias when combining study estimates is the failure to account for study differences that can influence outcomes, an area where meta-regression and other similar techniques can be particularly useful. Another common challenge in combining all available high-quality studies without introducing bias is accounting for the fact that studies report different and difficult-to-compare measures. For example, it may be important to assess the impact of diabetes control on health outcomes, but separate studies have measured diabetes control in terms of fasting plasma glucose, oral glucose tolerance tests, or HbA1c. Mapping the effect of a unit change in one measure to that of another requires careful analysis.<sup><xref ref-type="bibr" rid="bibr9-0272989X12441398">9</xref></sup></p>
<p>In attempting to avoid bias, analysts often prefer randomized trials to observational studies, although this choice is not always obvious. The rationale is that randomization balances unobserved covariates, producing an unbiased estimate of the effect of the intervention. However, RCT estimates are only unbiased for the often highly selected patient population in which they are conducted. Incorporating them into a model to consider outcomes for a patient population that differs from the trial population risks introducing serious bias. Observational studies often are conducted in larger, less selected patient populations over longer time periods and may provide estimates that are more applicable and generalizable, potentially more appropriate for inputs into a model. However, observational studies may suffer from nonrandom selection issues that introduce confounding biases. Attempts to correct such biases in observational studies often employ propensity score methods.<sup><xref ref-type="bibr" rid="bibr10-0272989X12441398">10</xref></sup> Propensity score methods estimate effects after balancing observed covariates, but they only provide unbiased causal inference and effect estimates if the strong and nonverifiable assumption of also balancing unobserved covariates is met.<sup><xref ref-type="bibr" rid="bibr11-0272989X12441398">11</xref></sup> In the end, the choice may rest upon a subjective assessment of the relative magnitude and import of nongeneralizability versus confounding.</p>
<p>In situations where RCTs are preferred or the only available study is an RCT, researchers may still be worried about biases in study estimates, as Campos and others illustrate. For example, when randomization does not result in balance for all observed covariates, it is possible that differences in treatment effects may be due to this imbalance. Even when randomization is successful, if study blinding is incomplete, selection issues can arise in which the de facto treatment assignments are not balanced with respect to observed or unobserved covariates, again influencing the observed treatment effects. Campos and others provide one practical approach to these challenges and also cite methods developed by Eddy.<sup><xref ref-type="bibr" rid="bibr12-0272989X12441398">12</xref></sup> A particularly challenging case is when biased selection leads to differential follow-up and hence differential missingness of outcomes across arms. While imputation methods are often employed in such situations,<sup><xref ref-type="bibr" rid="bibr13-0272989X12441398">13</xref>,<xref ref-type="bibr" rid="bibr14-0272989X12441398">14</xref></sup> incorrect assumptions about the process generating the missingness can introduce bias. Manski describes bounding methods that require fewer strong assumptions about the generating process of missingness but often lead to large uncertainty intervals.<sup><xref ref-type="bibr" rid="bibr15-0272989X12441398">15</xref></sup></p>
<p>When individual-level data from multiple studies are available, one could imagine employing bias correction methods for each individual study and then using standard meta-analytic methods to combine their estimates. It is unclear whether the bias correction approaches could be directly incorporated into individual-level meta-analytic techniques<sup><xref ref-type="bibr" rid="bibr16-0272989X12441398">16</xref></sup> to maximize the available information while simultaneously reducing biases. This is a potentially important area of future research.</p>
<p>When multiple studies are available but their individual-level data are not, direct adjustments for bias may not be possible. Consistency across multiple studies may then be the best feasible criterion for inclusion in model-based analyses. Direct comparison of study outcomes to assess consistency is often possible. When it is not, the model itself may play a role in the assessment, examining whether inputs from one study lead to model-generated outcomes that are consistent with other studies. It is still possible that both the input study and the comparator study are similarly biased, in which case consistency assessment is not helpful. However, when a larger number of studies are available, the possibility of correlated biases seems less likely. This approach applies to techniques developed for model evaluation and external validation.<sup><xref ref-type="bibr" rid="bibr17-0272989X12441398">17</xref>,<xref ref-type="bibr" rid="bibr18-0272989X12441398">18</xref></sup></p>
<p>The rigor and applicability of comparative effectiveness and cost-effectiveness analyses can benefit from methods that appropriately link unbiased study estimates to decision-analytic mathematical models. Campos and others provide a cautionary tale to decision analysts about assessing empirical studies prior to including their unadjusted estimates in mathematical models. Their bias correction approach contributes to the growing body of methods literature in this area.</p>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="bibr1-0272989X12441398">
<label>1.</label>
<citation citation-type="gov">
<comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://www.gao.gov/press/pcori_2011jan21.html">http://www.gao.gov/press/pcori_2011jan21.html</ext-link></comment>. <access-date>Accessed 2 Jan 2012</access-date>.</citation>
</ref>
<ref id="bibr2-0272989X12441398">
<label>2.</label>
<citation citation-type="web">
<comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://www.pcori.org/funding-opportunities/past-funding-opportunities/">http://www.pcori.org/funding-opportunities/past-funding-opportunities/</ext-link></comment>. <access-date>Accessed 2 Jan 2012</access-date>.</citation>
</ref>
<ref id="bibr3-0272989X12441398">
<label>3.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Neumann</surname><given-names>PJ</given-names></name>
<name><surname>Fang</surname><given-names>CH</given-names></name>
<name><surname>Cohen</surname><given-names>JT</given-names></name>
</person-group>. <article-title>30 years of pharmaceutical cost-utility analyses: growth, diversity and methodological improvement</article-title>. <source>Pharmacoeconomics</source>. <year>2009</year>;<volume>27</volume>(<issue>10</issue>):<fpage>861</fpage>–<lpage>72</lpage>.</citation>
</ref>
<ref id="bibr4-0272989X12441398">
<label>4.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Campos</surname><given-names>NG</given-names></name>
<name><surname>Castle</surname><given-names>PE</given-names></name>
<name><surname>Schiffman</surname><given-names>M</given-names></name>
<name><surname>Kim</surname><given-names>JJ</given-names></name>
</person-group>. <article-title>Policy implications of adjusting randomized trial data for economic evaluations: A demonstration from the ASCUS-LSIL triage study</article-title>. <source>Med Decis Making</source>. <year>2012</year>;<volume>32</volume>:<fpage>400</fpage>–<lpage>427</lpage>.</citation>
</ref>
<ref id="bibr5-0272989X12441398">
<label>5.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weinstein</surname><given-names>MC</given-names></name>
<name><surname>O’Brien</surname><given-names>B</given-names></name>
<name><surname>Hornberger</surname><given-names>J</given-names></name>
<etal/>
</person-group>.; <article-title>ISPOR Task Force on Good Research Practices–Modeling Studies. Principles of good practice for decision analytic modeling in health-care evaluation : report of the ISPOR Task Force on Good Research Practices–Modeling Studies. Value Health</article-title>. <year>2003</year>;<volume>6</volume>(<issue>1</issue>):<fpage>9</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr6-0272989X12441398">
<label>6.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Philips</surname><given-names>Z</given-names></name>
<name><surname>Ginnelly</surname><given-names>L</given-names></name>
<name><surname>Sculpher</surname><given-names>M</given-names></name>
<etal/>
</person-group>. <article-title>Review of guidelines for good practice in decision-analytic modelling in health technology assessment</article-title>. <source>Health Technol Assess</source>. <year>2004</year>;<volume>8</volume>(<issue>36</issue>):<fpage>iii</fpage>–<lpage>iv</lpage>, <fpage>ix</fpage>–<lpage>xi</lpage>, <fpage>1</fpage>–<lpage>158</lpage>.</citation>
</ref>
<ref id="bibr7-0272989X12441398">
<label>7.</label>
<citation citation-type="book">
<person-group person-group-type="editor">
<name><surname>Egger</surname><given-names>M</given-names></name>
<name><surname>Davey Smith</surname><given-names>G</given-names></name>
<name><surname>Altman</surname><given-names>D</given-names></name>
</person-group>, eds. <article-title>Systematic Reviews in Health Care: Meta-analysis in Context</article-title>. <publisher-loc>London</publisher-loc>: <publisher-name>BMJ Books</publisher-name>; <year>2001</year>.</citation>
</ref>
<ref id="bibr8-0272989X12441398">
<label>8.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sutton</surname><given-names>AJ</given-names></name>
<name><surname>Abrams</surname><given-names>KR</given-names></name>
</person-group>. <article-title>Bayesian methods in meta-analysis and evidence synthesis</article-title>. <source>Stat Methods Med Res</source>. <year>2001</year>;<volume>10</volume>(<issue>4</issue>):<fpage>277</fpage>–<lpage>303</lpage>.</citation>
</ref>
<ref id="bibr9-0272989X12441398">
<label>9.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Danaei</surname><given-names>G</given-names></name>
<name><surname>Lawes</surname><given-names>CM</given-names></name>
<name><surname>Vander Hoorn</surname><given-names>S</given-names></name>
<name><surname>Murray</surname><given-names>CJ</given-names></name>
<name><surname>Ezzati</surname><given-names>M</given-names></name>
</person-group>. <article-title>Global and regional mortality from ischaemic heart disease and stroke attributable to higher-than-optimum blood glucose concentration: comparative risk assessment</article-title>. <source>Lancet</source>. <year>2006</year>;<volume>368</volume>(<issue>9548</issue>):<fpage>1651</fpage>–<lpage>9</lpage>.</citation>
</ref>
<ref id="bibr10-0272989X12441398">
<label>10.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rosenbaum</surname><given-names>PR</given-names></name>
<name><surname>Rubin</surname><given-names>DB</given-names></name>
</person-group>. <article-title>The central role of the propensity score in observational studies for causal effects</article-title>. <source>Biometrika</source>. <year>1983</year>;<volume>70</volume>(<issue>1</issue>):<fpage>41</fpage>–<lpage>55</lpage>.</citation>
</ref>
<ref id="bibr11-0272989X12441398">
<label>11.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pearl</surname><given-names>J</given-names></name>
</person-group>. <article-title>Causality: Models, Reasoning, and Inference</article-title>. <edition>2nd ed.</edition> <publisher-loc>New York</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>; <year>2009</year>.</citation>
</ref>
<ref id="bibr12-0272989X12441398">
<label>12.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Eddy</surname><given-names>DM</given-names></name>
</person-group>. <article-title>The confidence profile method: a Bayesian method for assessing health technologies</article-title>. <source>Oper Res</source>. <year>1989</year>;<volume>37</volume>:<fpage>210</fpage>–<lpage>28</lpage>.</citation>
</ref>
<ref id="bibr13-0272989X12441398">
<label>13.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Rubin</surname><given-names>DB</given-names></name>
</person-group>. <article-title>Inference and missing data</article-title>. <source>Biometrika</source>. <year>1976</year>;<volume>63</volume>(<issue>3</issue>):<fpage>581</fpage>–<lpage>92</lpage>.</citation>
</ref>
<ref id="bibr14-0272989X12441398">
<label>14.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Efron</surname><given-names>B</given-names></name>
</person-group>. <article-title>Missing data, imputation, and the bootstrap</article-title>. <source>J Am Stat Assoc</source>. <year>1994</year>;<volume>89</volume>:<fpage>463</fpage>–<lpage>79</lpage>.</citation>
</ref>
<ref id="bibr15-0272989X12441398">
<label>15.</label>
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Manski</surname><given-names>CF</given-names></name>
</person-group>. <article-title>Identification for Prediction and Decision</article-title>. <publisher-loc>Cambridge</publisher-loc>: <publisher-name>Harvard University Press</publisher-name>; <year>2007</year>.</citation>
</ref>
<ref id="bibr16-0272989X12441398">
<label>16.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Simmonds</surname><given-names>MC</given-names></name>
<name><surname>Higgins</surname><given-names>JP</given-names></name>
<name><surname>Stewart</surname><given-names>LA</given-names></name>
<name><surname>Tierney</surname><given-names>JF</given-names></name>
<name><surname>Clarke</surname><given-names>MJ</given-names></name>
<name><surname>Thompson</surname><given-names>SG</given-names></name>
</person-group>. <article-title>Meta-analysis of individual patient data from randomized trials: a review of methods used in practice</article-title>. <source>Clin Trials</source>. <year>2005</year>;<volume>2</volume>(<issue>3</issue>):<fpage>209</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr17-0272989X12441398">
<label>17.</label>
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Eddy</surname><given-names>DM</given-names></name>
<name><surname>Hollingworth</surname><given-names>W</given-names></name>
<name><surname>Caro</surname><given-names>JJ</given-names></name>
<name><surname>Tsevat</surname><given-names>J</given-names></name>
<name><surname>McDonald</surname><given-names>KM</given-names></name>
<name><surname>Wong</surname><given-names>JB</given-names></name>
</person-group>. <article-title>Model transparency and validation: a report of the ISPOR-SMDM Modeling Good Research Practices Task Force Working Group, part 4</article-title>. <comment>Available from: <ext-link ext-link-type="uri" xlink:href="http://www.ispor.org/workpaper/modeling_methods/DRAFT-Modeling-Task-Force_Validation-and-Transparency-Report.pdf">http://www.ispor.org/workpaper/modeling_methods/DRAFT-Modeling-Task-Force_Validation-and-Transparency-Report.pdf</ext-link></comment></citation>
</ref>
<ref id="bibr18-0272989X12441398">
<label>18.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Goldhaber-Fiebert</surname><given-names>JD</given-names></name>
<name><surname>Stout</surname><given-names>NK</given-names></name>
<name><surname>Goldie</surname><given-names>SJ</given-names></name>
</person-group>. <article-title>Empirically evaluating decision-analytic models</article-title>. <source>Value Health</source>. <year>2010</year>;<volume>13</volume>(<issue>5</issue>):<fpage>667</fpage>–<lpage>74</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>