<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">HFS</journal-id>
<journal-id journal-id-type="hwp">sphfs</journal-id>
<journal-title>Human Factors: The Journal of Human Factors and Ergonomics Society</journal-title>
<issn pub-type="ppub">0018-7208</issn>
<issn pub-type="epub">1547-8181</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0018720811433052</article-id>
<article-id pub-id-type="publisher-id">10.1177_0018720811433052</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Displays and Controls</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Developing and Testing a Human-Based Gesture Vocabulary for Tabletop Systems</article-title>
</title-group>
<contrib-group content-type="issue">
<contrib contrib-type="guest-editor">
<name><surname>Cooke</surname><given-names>Nancy J.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Duchon</surname><given-names>Andrew</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Gorman</surname><given-names>Jamie C.</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Keyton</surname><given-names>Joann</given-names></name>
</contrib>
<contrib contrib-type="guest-editor">
<name><surname>Miller</surname><given-names>Anne</given-names></name>
</contrib>
</contrib-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Urakami</surname><given-names>Jacqueline</given-names></name>
</contrib>
<aff id="aff1-0018720811433052">Keio University, Endo Fujisawa, Japan</aff>
</contrib-group>
<author-notes>
<corresp id="corresp1-0018720811433052">Jacqueline Urakami, Keio University, Shonan Fujisawa Campus, Endo Fujisawa 5322, Japan; e-mail: <email>jacqueline.waniek@gmail.com</email>.</corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>8</month>
<year>2012</year>
</pub-date>
<volume>54</volume>
<issue>4</issue>
<issue-title>Special Section: Methods for the Analysis of Communication</issue-title>
<fpage>636</fpage>
<lpage>653</lpage>
<history>
<date date-type="received">
<day>3</day>
<month>6</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>11</month>
<year>2011</year>
</date>
</history>
<permissions>
<copyright-statement>© 2012, Human Factors and Ergonomics Society</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Human Factors and Ergonomics Society</copyright-holder>
</permissions>
<abstract>
<sec id="section1-0018720811433052">
<title>Objective:</title>
<p>The goal was to study the natural and intuitive use of surface gestures for the development of a tabletop system. Furthermore, the effect of expertise on choice of gestures was examined.</p>
</sec>
<sec id="section2-0018720811433052">
<title>Background:</title>
<p>It is still not well understood what kinds of gestures novice users choose when they interact with gesture recognition systems.</p>
</sec>
<sec id="section3-0018720811433052">
<title>Method:</title>
<p>First, novices’ and experts’ choice of gestures for a tabletop system was compared in a quasiexperimental design. Second, memorability of novices’ and experts’ gesture sets derived from the first study was compared in an experimental study. Third, memorization of hand shape and motion path was examined in a further experiment.</p>
</sec>
<sec id="section4-0018720811433052">
<title>Results:</title>
<p>Data revealed user preferences for specific hand shapes and motion paths. Choice of gestures was affected by the size of the manipulated object, expertise, and nature of the command (direct manipulation of objects vs. assessment of abstract functions). Follow-up experiments revealed that the novices’ gesture set was better memorized than were the experts’ gesture set. Furthermore, the motion path of a gesture is better memorized than the specific hand shapes for a gesture.</p>
</sec>
<sec id="section5-0018720811433052">
<title>Conclusion:</title>
<p>Expertise affects the choice of gesture to a certain degree. It is therefore essential to involve novice users in the development of gesture vocabularies. Gestures for technical systems should be simple and should involve distinctive motion patterns instead of focusing on specific hand shapes or number of fingers. Abstract or symbolic gestures should be avoided.</p>
</sec>
<sec id="section6-0018720811433052">
<title>Application:</title>
<p>Results of the study can be applied to the development of surface gestures for tabletop systems.</p>
</sec>
</abstract>
<kwd-group>
<kwd>ergonomic design</kwd>
<kwd>user study</kwd>
<kwd>surface gestures</kwd>
<kwd>expertise</kwd>
<kwd>hand shape</kwd>
<kwd>motion path</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<sec id="section7-0018720811433052" sec-type="intro">
<title>Introduction</title>
<p>Recent research has focused on the development of systems that can be controlled by hand gestures (<xref ref-type="bibr" rid="bibr1-0018720811433052">Buchinger, Hotop, Hlavacs, Simone, &amp; Ebrahimi, 2010</xref>; <xref ref-type="bibr" rid="bibr7-0018720811433052">Freeman, Benko, Morris, &amp; Wigdor, 2009</xref>; <xref ref-type="bibr" rid="bibr10-0018720811433052">Koike, Nishikawa, &amp; Fukuchi, 2009</xref>; <xref ref-type="bibr" rid="bibr16-0018720811433052">Oka, Sato, &amp; Koike, 2002</xref>; <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock, Morris, &amp; Wilson, 2009</xref>). Hand gestures are natural, universal, and a direct, immediate form of communication (<xref ref-type="bibr" rid="bibr9-0018720811433052">Kendon, 2004</xref>) and therefore an attractive method for interacting with technological systems. However, most systems are designed with technological considerations in mind, such as recognition accuracy (<xref ref-type="bibr" rid="bibr7-0018720811433052">Freeman et al., 2009</xref>; <xref ref-type="bibr" rid="bibr11-0018720811433052">Malik, Ranja, &amp; Balakrishnan, 2005</xref>). This design results in rather artificial sets of gestures that are difficult to learn and to perform. To develop systems that do not violate basic ergonomic principles, such as ease of use, learnability, or memorability, it is necessary to study what gestures people will naturally and intuitively perform without having to consider recognition accuracy of the system or other technological constraints.</p>
<p>So far, the majority of research in gesture communication is done in the area of human-to-human communication (see, e.g., <xref ref-type="bibr" rid="bibr2-0018720811433052">Duncan, Cassell, &amp; Levy, 2007</xref>; <xref ref-type="bibr" rid="bibr9-0018720811433052">Kendon, 2004</xref>; <xref ref-type="bibr" rid="bibr14-0018720811433052">McNeill, 2005</xref>; <xref ref-type="bibr" rid="bibr17-0018720811433052">Streeck, 2009</xref>). It is still not well understood how gesture communication works when the counterpart is a technological system instead of a human being. Systematic user studies are needed to reveal underlying principles of surface gestures that can be applied to the development of user-friendly systems.</p>
<p>A primary objective of the study was to determine common principles in gestures by studying the natural and intuitive use of hand gestures for controlling a map application of a tabletop system. Human-based gestures were generated by letting participants perform gestures freely without having to consider any technical constraints for gesture recognition. Because of the removal of technological barriers, it was expected that participants would intuitively choose gestures that are natural and universal to them. A map application was chosen because gesture sets should be tailored to a specific application (<xref ref-type="bibr" rid="bibr15-0018720811433052">Nielsen, Stoerring, Moeslund, &amp; Granum, 2004</xref>). Furthermore, electronic maps have a wide range of applications, and even technological novices are at least familiar with the use of regular maps.</p>
<p>A further objective was to study whether the choice of gesture is affected by expertise. Developers of gesture recognition systems have to consider user preferences on one side and technical restrictions on recognition accuracy on the other side. Even though the choice of hand gestures might have been perfectly natural for the developer, the gestures might not fit with the users’ expectations or semantic interpretations. To test the assumption that experts tend to be affected by the technological context of a gesture recognition system, technical novices as well as technical experts were recruited for the gesture studies reported in this article.</p>
<sec id="section8-0018720811433052">
<title>Classifying Gestures</title>
<p>A problem in analyzing gesture communication in technological systems is that no commonly accepted classification of gestures exists. Many studies involve classification systems from human gesture research (see <xref ref-type="bibr" rid="bibr9-0018720811433052">Kendon, 2004</xref>). However, even this area of research does not have a permanent, established category for gestures. Many classification systems for gestures in speech are based on the work of <xref ref-type="bibr" rid="bibr3-0018720811433052">Efron (1941)</xref>. Efron differentiated three perspectives, the spatiotemporal, the interlocutional and the linguistic. The spatiotemporal perspective describes gestures according to five categories: (a) size and radius of movement, (b) form of movement, (c) plane of movement, (d) body parts involved, and (e) tempo of gesture. The interlocutional perspective focuses on the involvement of other people. The linguistic perspective differentiates between gestures that are logical (dependent on speech) or objective (independent from speech). Objective gestures are further categorized into deictic (pointing), physiographic (referents), iconographic (form or spatial relations), and kinetographic (bodily actions). Furthermore, Efron distinguishes symbolic or emblematic gestures, which are gestures that have a standardized meaning within a specific community.</p>
<p><xref ref-type="bibr" rid="bibr4-0018720811433052">Ekman and Friesen (1969)</xref> based their taxonomy on <xref ref-type="bibr" rid="bibr3-0018720811433052">Efron’s (1941)</xref> work and offered five categories: emblems (gestures that are a direct verbal translation), illustrators (movements that are tied to speech), affect displays (facial expressions of emotion), regulators (regulating speech acts), and adaptors. While Efron’s categorization system refers to the entire body, <xref ref-type="bibr" rid="bibr13-0018720811433052">McNeill (1992)</xref> based his categorization on hands and arms, formulating three categories: (a) iconic gestures that represent objects or specific actions; (b) metaphoric gestures that display an image, shape, or movement; and (c) deictic gestures.</p>
<p>Generally, it can be said that gesture taxonomies are provisional working instruments that are tailored to or useful for specific research interests (<xref ref-type="bibr" rid="bibr13-0018720811433052">McNeill, 1992</xref>). Communicative goals, the local setting, and the type of social activity constitute the meaning of gestures as well (<xref ref-type="bibr" rid="bibr17-0018720811433052">Streeck, 2009</xref>). Therefore, classification systems for gestures in human communication can be only partially applied to the description of interactive surface gestures. For the development of gesture recognition systems, it is necessary to have a detailed description of hand shape, finger use, and motion patterns. <xref ref-type="bibr" rid="bibr8-0018720811433052">Hauptmann and McAvinney (1993)</xref>, for example, focused on number of fingers, duration of gesture, repetition, motion type, count of fingers and hand in motion, movement, and palm position. <xref ref-type="bibr" rid="bibr15-0018720811433052">Nielsen et al. (2004)</xref> described frequency of gestures; fingers used; shape of hand, such as pointing, fist, or thumb-up gestures; and whether the gesture is static or dynamic (involves motion or not).</p>
<p><xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref> developed a unique taxonomy that includes categories from the description of surface gestures as well as categories of classification systems from human discourse research. Wobbrock’s classification uses four dimensions, called form, nature, binding, and flow. The form dimension gives a detailed description of finger use and hand shape (static vs. dynamic poses) and the movement of the hand (path). The nature dimension classifies symbolic, physical, metaphorical, and abstract gestures. The binding dimension describes object manipulation and world reference. Gestures can refer to the surrounding world or can be independent of the world around the user. The flow dimension differentiates between continuous and discrete responses. <xref ref-type="bibr" rid="bibr7-0018720811433052">Freeman et al. (2009)</xref> built their taxonomy on Wobbroeck’s form category by differentiating it furthermore into registration pose, continuation pose, and movement pose. Registration pose describes single versus multiple use of fingers and hands. Continuation pose describes whether hand poses change (dynamic) or not (static), and movement describes whether the hands stays in one place (no path) or moves (path).</p>
<p>The purpose of our research is to study freely performed surface gestures. (This research was supported by the Yasumura and Koike laboratory and therefore “we” will be used throughout the text.) Because the participants’ choice of gestures is not limited by technological constraints, a large variety of gestures is likely to occur. Therefore, observational data will be evaluated on the basis of categories of the classification systems from <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref> and <xref ref-type="bibr" rid="bibr7-0018720811433052">Freeman et al. (2009)</xref>, since those taxonomies focus on a detailed description of finger and hand use on one side and include more abstract categories, such as symbolic meaning and world reference, on the other. By using classification categories from human communication research as well as from the development of surface gesture vocabularies, we hope to describe the gestures performed in our study thoroughly.</p>
</sec>
<sec id="section9-0018720811433052">
<title>Surface Gestures as Input Commands</title>
<p>Communicating with gestures is a universal and natural form of expression. We assume that gesture communication with technology follows the same basic principles as gesture communication with humans. Consequently, it should be possible to identify common, universal characteristics of people’s choice of gestures when interacting with technology. The results of previous research in gesture interaction with technology support our assumption. Several studies have reported a great consistency among gestures performed by participants. <xref ref-type="bibr" rid="bibr5-0018720811433052">Epps, Lichman, and Wu (2006)</xref> found that participants selected the same type of hand shape for the same type of task. <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref> reported that participants reused the same gestures for similar operations. A cross-cultural comparison by <xref ref-type="bibr" rid="bibr12-0018720811433052">Mauney, Howarth, Wirtanen, and Capra (2010)</xref> revealed only few differences between participants from nine countries. Higher agreement was found for actions performed through direct manipulation, whereas lower agreement existed for actions that were more symbolic.</p>
<p>In general, the gesture most often performed by participants in gesture studies was pointing (<xref ref-type="bibr" rid="bibr5-0018720811433052">Epps et al., 2006</xref>; <xref ref-type="bibr" rid="bibr6-0018720811433052">Fikkert, Paul, Rauwerda, Breit, &amp; Nijholt, 2009</xref>; <xref ref-type="bibr" rid="bibr8-0018720811433052">Hauptmann &amp; McAvinney, 1993</xref>; <xref ref-type="bibr" rid="bibr15-0018720811433052">Nielsen et al., 2004</xref>; <xref ref-type="bibr" rid="bibr18-0018720811433052">Voida, Podlaseck, Kjeldsen, &amp; Pinhanez, 2005</xref>), wherein the index finger is usually preferred (<xref ref-type="bibr" rid="bibr5-0018720811433052">Epps et al., 2006</xref>; <xref ref-type="bibr" rid="bibr8-0018720811433052">Hauptmann &amp; McAvinney, 1993</xref>). Pointing is often used for actions, such as drawing or selecting an object. Other often-observed hand gestures were a flat hand (<xref ref-type="bibr" rid="bibr5-0018720811433052">Epps et al., 2006</xref>; <xref ref-type="bibr" rid="bibr8-0018720811433052">Hauptmann &amp; McAvinney, 1993</xref>), grabbing (<xref ref-type="bibr" rid="bibr5-0018720811433052">Epps et al., 2006</xref>; <xref ref-type="bibr" rid="bibr6-0018720811433052">Fikkert et al., 2009</xref>), and releasing (<xref ref-type="bibr" rid="bibr5-0018720811433052">Epps et al., 2006</xref>). For dichotomous actions, such as shrink-enlarge or open-close, people seem to prefer reversible gestures, such as closing and opening the fist (<xref ref-type="bibr" rid="bibr6-0018720811433052">Fikkert et al., 2009</xref>; <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al., 2009</xref>).</p>
<p>In summary, it can be said that people agree to a certain extent in what kinds of gestures they use. People use a limited number of different hand shapes and prefer certain motion paths to carry out specific actions.</p>
<p>That experts are not necessarily the best designers for gesture vocabularies has been shown in previous studies. For example, technology-driven systems often involve a different number of fingers as input (e.g., <xref ref-type="bibr" rid="bibr11-0018720811433052">Malik et al., 2005</xref>), because the number of fingers can be easily recognized. However, a recent study by <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref> revealed that participants do not care about the number of fingers used to carry out surface gestures. Furthermore, Wobbrock et al. compared an experts’ gesture set with a nonexperts’ gesture set and found that 19% of the experts’ gestures were never performed by a novice. On the other hand, the nonexperts of the study generated a wider scope of gestures than did the experts. <xref ref-type="bibr" rid="bibr12-0018720811433052">Mauney et al. (2010)</xref> reported that users’ experience with gesture interfaces strongly affected the selection of gestures. Overall, previous research suggests that experts consider automatically certain constraints and limitations of gesture recognition systems when selecting gestures.</p>
<p>The goal of the study reported in this article is to develop a human-based gesture set for a tabletop system and to understand the underlying principles of gesture use when communicating with technological systems. We compared gestures performed for predefined commands by nontechnical and technical experts to find out whether expertise affects the choice of gestures. In a follow-up study, the memorability of nontechnical and technical experts’ gesture sets was compared. We expected that the memorability of the nontechnical experts’ gesture set would be better than that of the experts’ gesture set, because nontechnical experts are less affected by technological constraints when choosing gestures and might therefore pick more natural and intuitive gestures. Furthermore, a second follow-up study was conducted to test whether people memorize the hand shape or the motion path of gestures better. We expected that the motion path would be better memorized than the hand shape because previous research suggests that users do not focus on hand shape when gestures are performed.</p>
</sec>
</sec>
<sec id="section10-0018720811433052">
<title>Experiment 1: Gesture Study</title>
<sec id="section11-0018720811433052">
<title>Method</title>
<sec id="section12-0018720811433052">
<title>Participants</title>
<p>We recruited 20 participants from the general public for the nontechnical group (called novices). None of the participants had previous experience with gesture recognition systems or had studied or worked in a technical field. Of the novices, 6 participants were male and 14 female; their average age was 48 years (<italic>M</italic> = 47.80, <italic>SD</italic> = 14.16).</p>
<p>We recruited 11 participants from Chofu University of Electronic Communication for the technical expert group (called experts). Participants were students of media and information systems and were familiar with gesture recognition systems. Of the experts, 10 participants were male and 1 was female; their average age was 24 years (<italic>M</italic> = 23.55, <italic>SD</italic> = 1.75).</p>
</sec>
<sec id="section13-0018720811433052">
<title>Demonstration</title>
<p>An animated PowerPoint presentation was created for demonstrating 17 commands. Commands were shown in the following fixed order, starting with simple commands that required direct manipulation of the map and finishing with more abstract commands: <italic>select destination, select area, select start and goal, draw route, plan route, undo, delete, move map, rotate map, zoom in, zoom out, open menu, close menu, help, change view, print map</italic>, and <italic>screenshot</italic>. Each slide had a title with the name of the command and showed the same map. The effect of each command was demonstrated with the use of animations. For example, the map was enlarged for the <italic>zoom in</italic> command, or a route was drawn on the map for the <italic>draw route</italic> command. Each demonstration lasted for about 10 s. The slide show started with two examples used as a practice run for participants.</p>
</sec>
<sec id="section14-0018720811433052">
<title>Questionnaire</title>
<p>The questionnaire contained eight questions about age, educational background, dominant hand, use of maps, use of navigation systems, and experience with gesture interfaces.</p>
</sec>
<sec id="section15-0018720811433052">
<title>Gesture categorization</title>
<p>We developed a unique classification system based on the taxonomy from <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref> and <xref ref-type="bibr" rid="bibr7-0018720811433052">Freeman et al. (2009)</xref>; see <xref ref-type="table" rid="table1-0018720811433052">Table 1</xref>.</p>
<table-wrap id="table1-0018720811433052" position="float">
<label>Table 1:</label>
<caption>
<p>Gesture Taxonomy</p>
</caption>
<graphic alternate-form-of="table1-0018720811433052" xlink:href="10.1177_0018720811433052-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left" colspan="4">Taxonomy of Gestures</th>
</tr>
</thead>
<tbody>
<tr>
<td>Form</td>
<td>Shape</td>
<td>Dominant vs. nondominant hand</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>One hand vs. both hands</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>Single finger vs. multiple fingers</td>
<td/>
</tr>
<tr>
<td/>
<td/>
<td>Hand shape description</td>
<td/>
</tr>
<tr>
<td/>
<td>Movement</td>
<td>No path and static pose</td>
<td>Hand does not move, hand pose is not changed</td>
</tr>
<tr>
<td/>
<td/>
<td>Path and static pose</td>
<td>Hand moves, pose is not changed</td>
</tr>
<tr>
<td/>
<td/>
<td>No path and dynamic pose</td>
<td>Hand is held in one location, pose is changed</td>
</tr>
<tr>
<td/>
<td/>
<td>Path and dynamic pose</td>
<td>Hand moves, hand pose is changed</td>
</tr>
<tr>
<td/>
<td/>
<td>Motion path description</td>
<td/>
</tr>
<tr>
<td/>
<td>Touch</td>
<td>Surface touch vs. no touch</td>
<td>Table is touched by fingers or entire hand</td>
</tr>
<tr>
<td/>
<td>Complexity</td>
<td>Single vs. multiple gestures</td>
<td>Command is executed by one gesture or multiple gestures</td>
</tr>
<tr>
<td>Nature</td>
<td>Symbolic</td>
<td>Gesture represents a symbol</td>
<td>Write a question mark for <italic>help</italic></td>
</tr>
<tr>
<td/>
<td>Physical</td>
<td>An object is physically manipulated</td>
<td>Make a gesture as if turning a map for <italic>rotate map</italic></td>
</tr>
<tr>
<td/>
<td>Metaphorical</td>
<td>Gesture represents a metaphor</td>
<td>Make a gesture as if opening a book for <italic>open menu</italic></td>
</tr>
<tr>
<td/>
<td>Abstract</td>
<td>Gesture is neither symbolic, physical, nor metaphorical but arbitrary</td>
<td>Double-click on the table for <italic>change view</italic></td>
</tr>
<tr>
<td>Binding</td>
<td>Object-centric</td>
<td>Refers only to an object</td>
<td>Enlarge map by drawing hands apart</td>
</tr>
<tr>
<td/>
<td>World dependent</td>
<td>Refers to the surrounding world</td>
<td>Throw paper to an imaginary printer for <italic>print out</italic></td>
</tr>
<tr>
<td/>
<td>World independent</td>
<td>Refers neither to an object nor to the surrounding world but can occur anywhere</td>
<td>Raise the hand for <italic>help</italic></td>
</tr>
<tr>
<td/>
<td>Mixed dependencies</td>
<td>Gesture contains object-centric, world-dependent, or world-independent aspects</td>
<td/>
</tr>
</tbody>
</table>
</table-wrap>
<p>Two independent raters categorized gestures according to three dimensions: form, nature and binding. The form dimension combines categories from <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref> and <xref ref-type="bibr" rid="bibr7-0018720811433052">Freeman et al. (2009)</xref> and gives a physical description of the gesture, focusing on hand shape, movement, touch, and complexity. Movements are differentiated into motions of the entire hand (path or no path) and changes of the hand pose (static or dynamic). A gesture is categorized as complex when a series of different gestures is performed to carry out a command, for example, “Put hand on table and then snap fingers.”</p>
<p>The nature and binding dimensions are analogous to the classification system defined by <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref>.</p>
</sec>
<sec id="section16-0018720811433052">
<title>Experimental setting and procedure</title>
<p>The gesture set was developed according to the procedure described by <xref ref-type="bibr" rid="bibr15-0018720811433052">Nielsen et al. (2004)</xref>. First, we selected appropriate commands for a map application of a tabletop system. The goal was that the gesture input for these commands improves the usability of the application. A total of 10 human engineering students, who knew basic principles of universal design and cognitive engineering but were novices in regard to tabletop and gesture recognition systems, created a list of commands. The 17 most-often-mentioned commands were chosen for the gesture study. Second, we created computer-based demonstrations for each command that showed what kind of action is performed and to what result this action leads. Third, novices as well as experts performed gestures for each command. The participants’ performance was videotaped and afterward evaluated according to our predefined classification system.</p>
<p>Participants were tested individually. Commands were presented on a 15.4-in. computer screen that was placed on the right-hand side of the participants. The participants were sitting on a table. A black sheet of dimensions 50 cm × 40 cm was placed before them. Participants were asked to imagine that the black sheet was the map they saw on the computer screen and that they should carry out all gestures above this black sheet. A video camera was placed in front of the participants to record the gestures.</p>
<p>First, the participants had to fill in a questionnaire with demographic data and their experience with technology and usage of maps. Afterward, the researcher explained the purpose of the research and instructed participants that they can perform any kind of gesture that comes into their mind. Two examples were shown for which the participants had to perform gestures, and the researcher made sure that the participants understood the instructions. Afterward, each command was shown individually and the participants were asked to perform a gesture that could be used to carry out this command. Each session took about 25 min.</p>
</sec>
<sec id="section17-0018720811433052">
<title>Design</title>
<p>A quasiexperimental design with the two groups, nontechnical and technical (novices and experts), was conducted.</p>
</sec>
</sec>
<sec id="section18-0018720811433052">
<title>Results and Discussion</title>
<p>The questionnaire about the participants’ experience in the use of maps, navigation systems, and gesture interfaces revealed that novices were more experienced in the use of regular paper maps than were experts (novices, <italic>M</italic> = 2.40, <italic>SD</italic> = 0.68; experts, <italic>M</italic> = 1.73, <italic>SD</italic> = 0.47), <italic>F</italic>(1, 29) = 8.48, <italic>p</italic> &lt; .01, <italic>d</italic> = 1.15. However, experts were more experienced in the use of digital maps (experts, <italic>M</italic> = 3.36, <italic>SD</italic> = 0.92; novices, <italic>M</italic> = 2.30, <italic>SD</italic> = 0.80), <italic>F</italic>(1, 29) = 11.22, <italic>p</italic> &lt; .01, <italic>d</italic> = 1.23, and gesture interfaces (experts, <italic>M</italic> = 2.91, <italic>SD</italic> = 0.70; novices, <italic>M</italic> = 1.05, <italic>SD</italic> = .22), <italic>F</italic>(1, 29) = 121; <italic>p</italic> &lt; .01, <italic>d</italic> = 3.58, than were novices.</p>
<p>Participants performed gestures for 17 commands. The command <italic>screenshot</italic> was unknown to most of the novices. Therefore, data for this command are excluded from the analysis. In addition, 1 novice could not perform a gesture for the command <italic>help</italic>, and 3 novices could not think of a gesture for the command <italic>change view</italic>. Consequently, 316 gestures were analyzed for novices, and 176 gestures were analyzed for experts. Gestures were evaluated according to the categories described in <xref ref-type="table" rid="table1-0018720811433052">Table 1</xref> by two trained raters. Interrater reliability was high, Cohen’s κ = .79. Differences between raters were resolved by discussion. The results of the evaluation are summarized in <xref ref-type="table" rid="table2-0018720811433052">Tables 2</xref> and <xref ref-type="table" rid="table3-0018720811433052">3</xref>.</p>
<table-wrap id="table2-0018720811433052" position="float">
<label>Table 2:</label>
<caption>
<p>Novices’ and Experts’ Gestures in the Category <italic>Form</italic> (in percentages)</p>
</caption>
<graphic alternate-form-of="table2-0018720811433052" xlink:href="10.1177_0018720811433052-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Form</th>
<th align="center">Novices</th>
<th align="center">Experts</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Shape</td>
</tr>
<tr>
<td> Dominant hand</td>
<td>99.05</td>
<td>98.86</td>
</tr>
<tr>
<td colspan="3"> One or both hands</td>
</tr>
<tr>
<td>  One</td>
<td>61.39</td>
<td>73.86</td>
</tr>
<tr>
<td>  Both</td>
<td>38.61</td>
<td>26.14</td>
</tr>
<tr>
<td colspan="3"> Single vs. multiple fingers</td>
</tr>
<tr>
<td>  One</td>
<td>43.67</td>
<td>49.77</td>
</tr>
<tr>
<td>  Two</td>
<td>7.59</td>
<td>14.77</td>
</tr>
<tr>
<td>  Four</td>
<td>0.63</td>
<td>0.00</td>
</tr>
<tr>
<td>  Five</td>
<td>48.10</td>
<td>35.80</td>
</tr>
<tr>
<td colspan="3"> Hand shape description</td>
</tr>
<tr>
<td>  Pointing</td>
<td>47.33</td>
<td>48.52</td>
</tr>
<tr>
<td>  Flat hand</td>
<td>40.21</td>
<td>30.77</td>
</tr>
<tr>
<td>  Pinching</td>
<td>7.12</td>
<td>2.37</td>
</tr>
<tr>
<td>  Grabbing</td>
<td>1.42</td>
<td>2.37</td>
</tr>
<tr>
<td>  Fist</td>
<td>2.49</td>
<td>4.73</td>
</tr>
<tr>
<td>  Thumb and index finger spread</td>
<td>1.42</td>
<td>11.24</td>
</tr>
<tr>
<td colspan="3">Movement</td>
</tr>
<tr>
<td> No path and static pose</td>
<td>3.5</td>
<td>4</td>
</tr>
<tr>
<td> Path and static pose</td>
<td>90.5</td>
<td>90.3</td>
</tr>
<tr>
<td> No path and dynamic pose</td>
<td>0</td>
<td>2.3</td>
</tr>
<tr>
<td> Path and dynamic pose</td>
<td>6</td>
<td>3.4</td>
</tr>
<tr>
<td colspan="3">Touch</td>
</tr>
<tr>
<td> Touching table</td>
<td>48.42</td>
<td>60.23</td>
</tr>
<tr>
<td colspan="3">Complexity</td>
</tr>
<tr>
<td> Single gesture</td>
<td>84.18</td>
<td>87.50</td>
</tr>
<tr>
<td> Multiple gestures</td>
<td>15.82</td>
<td>12.50</td>
</tr>
</tbody>
</table>
</table-wrap>
<table-wrap id="table3-0018720811433052" position="float">
<label>Table 3:</label>
<caption>
<p>Novice and Expert Gesture Sets</p>
</caption>
<graphic alternate-form-of="table3-0018720811433052" xlink:href="10.1177_0018720811433052-table3.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<thead>
<tr>
<th align="left">Command</th>
<th align="center">Novices</th>
<th align="center">Experts</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Select destination</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Index finger</td>
<td>Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>Point at sheet</td>
<td>Point at sheet</td>
</tr>
<tr>
<td colspan="3">Select area</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Index finger</td>
<td>Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>Draw a circle</td>
<td>Draw a circle</td>
</tr>
<tr>
<td colspan="3">Select start and goal</td>
</tr>
<tr>
<td> Hand shape</td>
<td>One hand: Index finger</td>
<td>Two hands: Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>Point at start and goal</td>
<td>Point with right index finger at start and with left index finger at goal</td>
</tr>
<tr>
<td colspan="3">Draw route</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Index finger</td>
<td>Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>Draw a line</td>
<td>Draw a line</td>
</tr>
<tr>
<td colspan="3">Plan route</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Index finger</td>
<td>Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>Start-goal-route</td>
<td>Start-route-goal</td>
</tr>
<tr>
<td colspan="3">Undo</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Flat hand, palm down</td>
<td>Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>Wipe</td>
<td>Wipe</td>
</tr>
<tr>
<td colspan="3">Delete</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Both hands: Flat hand, palm down</td>
<td>Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>One big wipe, hands crossing</td>
<td>Draw <italic>X</italic></td>
</tr>
<tr>
<td colspan="3">Move map</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Both hands: Flat hand, palm down</td>
<td>One hand: Flat hand, palm down</td>
</tr>
<tr>
<td> Motion</td>
<td>Move from body to front</td>
<td>Move from body to front</td>
</tr>
<tr>
<td colspan="3">Rotate map</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Both hands: Flat hand, palm down</td>
<td>One hand: Held like grabbing something</td>
</tr>
<tr>
<td> Motion</td>
<td>Rotate hands simultaneously</td>
<td>Rotate hand in wrist</td>
</tr>
<tr>
<td colspan="3">Zoom in</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Both hands: Flat hand, palm down</td>
<td>Both hands: Thumb and index finger spread</td>
</tr>
<tr>
<td> Motion</td>
<td>Spread</td>
<td>Spread</td>
</tr>
<tr>
<td colspan="3">Zoom out</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Both hands: Flat hand, palm down</td>
<td>Both hands: Thumb and index finger spread</td>
</tr>
<tr>
<td> Motion</td>
<td>Draw hands together</td>
<td>Draw hands together</td>
</tr>
<tr>
<td colspan="3">Open menu</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Flat hand, palm down</td>
<td>Flat hand, palm down</td>
</tr>
<tr>
<td> Motion</td>
<td>Move hand to the right and turn palm up as if opening a book</td>
<td>Put hand on the sheet</td>
</tr>
<tr>
<td colspan="3">Close menu</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Flat hand, palm up</td>
<td>Flat hand, palm down</td>
</tr>
<tr>
<td> Motion</td>
<td>Move hand to the left and turn palm down as if closing a book</td>
<td>Make a fist</td>
</tr>
<tr>
<td colspan="3">Help</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Flat hand, palm down</td>
<td>Index finger</td>
</tr>
<tr>
<td> Motion</td>
<td>Raise hand</td>
<td>Write question mark</td>
</tr>
<tr>
<td colspan="3">Change view</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Flat hand, palm down</td>
<td>Flat hand, palm down</td>
</tr>
<tr>
<td> Motion</td>
<td>Grabbing and move hand upward</td>
<td>Move hand to the right and turn palm up as if turning a page</td>
</tr>
<tr>
<td colspan="3">Print map</td>
</tr>
<tr>
<td> Hand shape</td>
<td>Both hands: Flat hand, palm down</td>
<td>One hand: Flat hand, palm down</td>
</tr>
<tr>
<td> Motion</td>
<td>Move to the front</td>
<td>Grabbing and make a movement as if throwing something</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The focus of our analysis is on the qualitative descriptions of gestures demonstrated by novices and experts. Analyses were carried out by computing Fisher’s exact test with a two-sided alpha level of .05 because data are unequally distributed, and the expected value for some cell frequencies was less than 5. Effect size was estimated with either the Phi coefficient (ϕ) for 2 × 2 tables or Cramer’s Phi coefficient (ϕ<sub>c</sub>) for tables greater than 2 × 2.</p>
<sec id="section19-0018720811433052">
<title>Hand dominance</title>
<p>Of the novices, 19 were right-handed and 1 was left-handed. Of the experts, 10 were right-handed and 1 was left-handed. Novices as well as experts used their dominant hand to perform approximately 99% of the gestures.</p>
</sec>
<sec id="section20-0018720811433052">
<title>Using one versus both hands</title>
<p>Novices as well as experts used one hand more often than both hands to perform gestures. Nearly 100% of novices and experts used one hand to perform the commands <italic>select destination, select area</italic>, and <italic>draw route</italic>. The majority of participants used both hands for <italic>zoom in</italic> and <italic>zoom out</italic>. More than 60% used one hand for the gestures <italic>plan route, undo, open menu, close menu, help</italic>, and <italic>change view</italic>. Approximately 50% of participants used one hand for the gesture <italic>rotate</italic>. Differences between novices and experts were revealed in <italic>select start and goal, delete, move map</italic>, and <italic>print</italic>. For <italic>delete</italic> and <italic>move map</italic>, the majority of novices used two hands, whereas experts used only one hand. The reverse result was found for <italic>print</italic>. Furthermore, 50% of the experts used two hands for <italic>select start and goal</italic>.</p>
<p>When both hands were used, they were used mainly in a synchronized manner, such as wiping both hands over the table for <italic>delete</italic> or moving both hands parallel to each other toward the front for <italic>move</italic>. In the case of asynchronous gestures, one hand, usually the nondominant hand, functioned as an anchor for the other hand. For example, the dominant hand was rotated around the nondominant hand for <italic>rotate</italic>; or first, the palms of both hands were touching, then the nondominant hand was held static while the dominant hand made a movement as if opening a book for <italic>menu open</italic>. Novices and experts differed significantly in their use of single versus both hands, <italic>p</italic> = .01, ϕ = .13.</p>
<p>The results show that novices seem to prefer one-hand gestures for commands that involve the manipulation of smaller objects on the map, such as selecting a destination or drawing a route, but prefer two-hand gestures for manipulations of the entire map, such as zooming in or out and moving the map. Experts seem to prefer one-hand gestures in general. The use of two hands for <italic>select start and goal</italic> might be attributed to the fact that it would be easier for a gesture recognition system to differentiate between start and goal when two different fingers are used instead the same finger for both.</p>
</sec>
<sec id="section21-0018720811433052">
<title>Single versus multiple fingers</title>
<p>Participants performed most of the gestures using either one finger or five fingers. Nearly all of the one- finger gestures were performed with use of the index finger. Only 2 novices used the thumb, and 3 experts performed a gesture using only the middle finger. Experts used two-finger gestures more often than did novices. These two-finger gestures involved using the index finger and middle finger, thumb and index finger, or thumb and middle finger. Using the thumb and index finger was far more often observed among experts (14.20%) than among novices (5.06%). In comparison, four-finger gestures involving the index, middle, ring, and pinky fingers were very rare, performed only twice by novices. Differences between novices and experts in use of single versus multiple fingers were significant, <italic>p</italic> = .01, ϕ<sub>c</sub> = .15.</p>
<p>Observational data suggest that experts differentiate more in how many fingers they use for carrying out a gesture. This difference might be attributed to the fact that the number of fingers can be easily recognized by gesture recognition systems. Novices, on the other hand, do not seem to care about the number of fingers they are using. This finding agrees with the data reported by <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref>, who showed that people do not place much importance on the number of fingers used to perform gestures.</p>
</sec>
<sec id="section22-0018720811433052">
<title>Complexity</title>
<p>Most of the gestures performed were simple single gestures (novices, 84.18%; experts, 87.5%). Multiple gestures were mainly observed for the commands <italic>select start and goal</italic> and <italic>plan route</italic> because of the nature of these commands; for example, when planning a route, the start, the goal, and the route itself had to be added to the map, which leads to a sequence of different gestures. Novices and experts did not differ in their use of simple versus multiple gestures, <italic>p</italic> = .35, ϕ = .05.</p>
</sec>
<sec id="section23-0018720811433052">
<title>Touch</title>
<p>Participants could freely choose whether to perform the gesture on the sheet or above the sheet. Novices directly touched the sheet to perform the gesture in 48.42% of the cases, whereas experts touched the sheet in 60.23% of the cases. Touching could be observed mainly among novices for <italic>select destination, select area, select start and goal, draw route</italic>, and <italic>plan route</italic>. Gestures without touching were observed for <italic>undo, delete, move map, rotate map, zoom in, zoom out, open menu, close menu, help, change view</italic>, and <italic>print</italic>. Results for experts were more diverse. The majority of experts preferred touching the sheet for <italic>select destination, select start and goal, draw route</italic>, and <italic>plan route</italic> and preferred a gesture with which they did not touch the sheet for <italic>print</italic>. Fisher’s test supports the finding that experts directly touch the surface more often than do novices, <italic>p</italic> = .02, ϕ = .11.</p>
<p>The observation of novices’ gestures leads us to assume that novices prefer to directly touch the surface for commands that involve the direct manipulation of objects on the map, such as <italic>select destination</italic> or <italic>draw route</italic>. Commands that involve the manipulation of the entire map or that have a more abstract meaning, such as <italic>zoom in, zoom out, help</italic>, or <italic>change view</italic>, tend to be carried out in the 3-dimensional space above the map.</p>
</sec>
<sec id="section24-0018720811433052">
<title>Hand shape</title>
<p>Participants mainly used six hand shapes: pointing, flat hand with fingers stretched, pinching, grabbing, spreading the thumb and index finger, and making a fist. Pointing and flat hand were used most often compared with the other hand shapes. Flat hand was performed with different orientations of the palm, such as palm down, palms facing, palm up, palm facing the body, and palm facing the front. Palms facing down was performed most often (novices, 63.72%; experts, 76.92%), followed by palms facing (novices, 19.47%; experts, 17.31%). Pinching, grabbing, and making a fist were observed in only a few cases. Spreading the thumb and index finger was used more often by experts (11.24%) than by novices (1.42%).</p>
<p>Pointing was mainly observed for commands that involved the direct manipulation of an object on the map, such as <italic>select destination, select area, select start and goal, draw route</italic>, and <italic>plan route</italic>. Using the entire hand as a flat hand was most often observed for commands that involved the manipulation of the entire map, such as <italic>move, zoom in</italic>, and <italic>zoom out</italic>, or for more abstract commands, such as <italic>open menu, close menu, help, change view</italic>, and <italic>print</italic>. The greatest variation in hand shapes could be observed for <italic>move map, open menu</italic>, and <italic>change view</italic> in the novice group and for <italic>zoom out, open menu, close menu</italic>, and <italic>print</italic> in the expert group. The use of different hand shapes varied significantly between novices and experts, <italic>p</italic> = .00, ϕ<sub>c</sub> = .25. The data support the findings of previous research by <xref ref-type="bibr" rid="bibr5-0018720811433052">Epps et al. (2006)</xref>, <xref ref-type="bibr" rid="bibr6-0018720811433052">Fikkert et al. (2009)</xref>, <xref ref-type="bibr" rid="bibr8-0018720811433052">Hauptmann and McAvinney (1993)</xref>, <xref ref-type="bibr" rid="bibr15-0018720811433052">Nielsen et al. (2004)</xref>, and <xref ref-type="bibr" rid="bibr18-0018720811433052">Voida et al. (2005)</xref>, in which it was reported that pointing with the index finger is the most often observed gesture in user studies.</p>
<p>Observational data show that people do not vary the hand shapes they use for gestures much. Participants used only a limited number of hand shapes for the various commands. Furthermore, similar hand shapes were used to carry out different gestures, which suggests that hand shape itself is not a good indicator of the message that is communicated by a gesture.</p>
</sec>
<sec id="section25-0018720811433052">
<title>Movement</title>
<p>Approximately 90% of all the gestures performed fell into the category <italic>path and static pose</italic>. Independent of expertise or the command that had to be performed, participants chose most often a gesture that involved a movement of the hand while keeping the hand pose static. The categories <italic>no path and static pose</italic> and <italic>path and dynamic pose</italic> were observed in only a few cases among novices and experts, whereas gestures that fell into the category <italic>no path and dynamic pose</italic> were only observed among experts. Fisher’s test indicates that novices and experts differed in the observed movement pattern, <italic>p</italic> = .04, ϕ<sub>c</sub> = .13.</p>
<p>Results show that the gestures performed mostly involved some kind of movement of the hand or the entire arm. Gestures in which the changing of the hand shape played an important role were rare.</p>
</sec>
<sec id="section26-0018720811433052">
<title>Movement path</title>
<p>Whereas similar hand shapes were used across different commands, movement paths were very distinct and specific for each command. Even though some participants used different hand shapes to carry out a specific command, they still revealed similar movement paths. For example, the command <italic>rotate</italic> was often performed with the use of a circular movement. This circular movement was performed either with the index finger, with the entire hand, or with two hands rotating simultaneously. Participants carried out the command <italic>zoom in</italic> by spreading both hands with the palms either down or facing or by spreading just the thumb and index finger. The movement path often mimicked the effect of the command, such as sliding hands above the table for <italic>move</italic>, drawing a line for <italic>draw route</italic>, rotating hands for <italic>rotate</italic>, or stretching hands apart for <italic>zoom in</italic>.</p>
<p>Participants carried out dichotomous commands often by using reversible gestures. For example, participants performed <italic>zoom in</italic> and <italic>zoom out</italic> by spreading the hands or fingers and bringing the hands or fingers together. The commands <italic>open menu</italic> and <italic>close menu</italic> were performed with a gesture that mimicked the opening or closing of a book. These findings agree with <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref> and <xref ref-type="bibr" rid="bibr6-0018720811433052">Fikkert et al. (2009)</xref>, who reported that participants used reversible gestures for dichotomous commands.</p>
<p>Since movements were very distinct for each command, they are described in more detail for the gesture sets of novices and experts in <xref ref-type="table" rid="table3-0018720811433052">Table 3</xref>.</p>
</sec>
<sec id="section27-0018720811433052">
<title>Agreement score</title>
<p>Two agreement scores were calculated on the basis of the number of identical hand shapes and movement paths performed for each command (see <xref ref-type="fig" rid="fig1-0018720811433052">Figures 1</xref> and <xref ref-type="fig" rid="fig2-0018720811433052">2</xref>). The agreement scores reflected the degree of consensus in the hand shapes and movement paths used among novices and experts and were adapted from <xref ref-type="bibr" rid="bibr19-0018720811433052">Wobbrock et al. (2009)</xref>. Novices showed in only 3 out of 16 cases a higher agreement score for hand shape than for movement path. The agreement score for motion path outnumbered the score for hand shape in 7 cases. For six commands, the agreement scores were nearly the same. On the other hand, experts reached in 5 cases a higher agreement score for hand shape than for motion path. A higher agreement score for motion path than for hand shape occurred in only 4 cases, and in 7 cases the scores were nearly the same.</p>
<fig id="fig1-0018720811433052" position="float">
<label>Figure 1.</label>
<caption>
<p>Agreement scores for hand shape and motion path for novices.</p>
</caption>
<graphic xlink:href="10.1177_0018720811433052-fig1.tif"/>
</fig>
<fig id="fig2-0018720811433052" position="float">
<label>Figure 2.</label>
<caption>
<p>Agreement scores for hand shape and motion path for experts.</p>
</caption>
<graphic xlink:href="10.1177_0018720811433052-fig2.tif"/>
</fig>
<p><italic>T</italic> tests for independent samples (comparing experts and novices) and dependent samples (comparing hand shape and motion path) were conducted. A comparison of agreement scores for experts and novices did not reveal a significant difference: hand shape, <italic>t</italic>(30) = .89, <italic>p</italic> = .38, <italic>d</italic> = .34; motion path, <italic>t</italic>(30) = .08, <italic>p</italic> = .93, <italic>d</italic> = .04. Furthermore, agreement scores for motion path and hand shape did not differ significantly either, <italic>t</italic>(31) = .45, <italic>p</italic> = .65, <italic>dz</italic> = .08. The later result might be attributed to the fact that there are large differences in agreement scores depending on the command. Therefore, we conducted <italic>t</italic> tests for dependent samples comparing agreement scores between hand shape and motion path for three different categories of commands: (a) manipulation of small objects (e.g., s<italic>elect destination, select area, draw route, select start and goal, plan route, undo</italic>), (b) manipulation of bigger objects or the entire map (e.g., <italic>delete, move, rotate, zoom in, zoom out</italic>), and (c) accessing of certain functions (e.g., <italic>open menu, close menu, help, print</italic>).</p>
<p>Commands for the manipulation of small objects reached the highest agreement scores for hand shape (<italic>M</italic> = .78, <italic>SD</italic> = .22) and motion path (<italic>M</italic> = .64, <italic>SD</italic> = .29). However, no significant differences could be revealed, <italic>t</italic>(11) = 1.60, <italic>p</italic> = .14, <italic>dz</italic> = .46. Commands for the manipulation of bigger objects or the entire map revealed significantly higher scores for motion path (<italic>M</italic> = .56, <italic>SD</italic> = .24) than for hand shape (<italic>M</italic> = .29, <italic>SD</italic> = .08), <italic>t</italic>(9) = 3.46, <italic>p</italic> &lt; .01, <italic>dz</italic> = 1.08. Commands for accessing certain functions reached the lowest agreement scores (<italic>M</italic> = .24, <italic>SD</italic> = .08; <italic>M</italic> = .22, <italic>SD</italic> = .10, for hand shape and motion path, respectively). But no significant differences could be revealed, <italic>t</italic>(9) = 1.49, <italic>p</italic> = .17, <italic>dz</italic> = .40. Experts compared with novices got clearly higher scores for <italic>zoom in</italic> and <italic>zoom out</italic> probably because of their knowledge of gesture recognition devices in which those commands are already implemented. Lower scores for motion path for <italic>select start and goal</italic> are attributable to the fact that participants used either one or both hands for entering this command.</p>
<p>The data suggest that movement path plays an important role in conveying the meaning in gesture communication. The motion of the hand describes a specific action that participants wish to be carried out by the technical system. Novices, especially, seem to agree more in the motion path performed than in the hand shape used. In a follow-up study, we examined how much attention is paid to hand shape and motion path when gestures have to be memorized.</p>
</sec>
<sec id="section28-0018720811433052">
<title>Nature</title>
<p>Most gestures were of physical nature. Those gestures involved the physical manipulation of an object (see <xref ref-type="table" rid="table4-0018720811433052">Table 4</xref>). Physical gestures were mainly observed for <italic>select destination, select area, select start and goal, draw route, plan route, move, rotate, zoom in</italic>, and <italic>zoom out</italic>. The second-largest category was metaphorical and the third abstract. Gestures of a metaphorical as well as an abstract nature were observed for <italic>undo, delete, open menu, close menu, help, change view</italic>, and <italic>print</italic>. Typical examples for gestures used as metaphors were “wiping” for <italic>delete</italic>, turning the palms of the hand up as if opening a book for <italic>menu open</italic>, raising the hand for <italic>help</italic>, or making a movement as if putting paper into a printer for <italic>print</italic>. Observed abstract gestures were snapping fingers for <italic>change view</italic> and double-clicking on the sheet or clapping hands for <italic>menu open</italic>. Symbolic gestures were used the least and involved gestures such as making a OK sign with thumb and index finger for <italic>menu open</italic> or writing a question mark for <italic>help</italic>. Interestingly, symbolic gestures were seen nearly twice as often among experts as among novices. The differences in the nature of gestures between novices and experts were significant, <italic>p</italic> = .02, ϕ<sub>c</sub> = .14.</p>
<table-wrap id="table4-0018720811433052" position="float">
<label>Table 4:</label>
<caption>
<p>Novices’ and Experts’ Gestures in the Categories <italic>Nature</italic> and <italic>Binding</italic> (in percentages)</p>
</caption>
<graphic alternate-form-of="table4-0018720811433052" xlink:href="10.1177_0018720811433052-table4.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="char" char="."/>
<col align="char" char="."/>
</colgroup>
<thead>
<tr>
<th align="left">Category</th>
<th align="center">Novices</th>
<th align="center">Experts</th>
</tr>
</thead>
<tbody>
<tr>
<td colspan="3">Nature</td>
</tr>
<tr>
<td> Symbolic</td>
<td>5.4</td>
<td>9.7</td>
</tr>
<tr>
<td> Physical</td>
<td>52.8</td>
<td>55.1</td>
</tr>
<tr>
<td> Metaphorical</td>
<td>27.5</td>
<td>17.0</td>
</tr>
<tr>
<td> Abstract</td>
<td>14.2</td>
<td>18.2</td>
</tr>
<tr>
<td colspan="3">Binding</td>
</tr>
<tr>
<td> Object-centric</td>
<td>55.1</td>
<td>60.8</td>
</tr>
<tr>
<td> World dependent</td>
<td>25.6</td>
<td>29.5</td>
</tr>
<tr>
<td> World independent</td>
<td>19.0</td>
<td>9.7</td>
</tr>
<tr>
<td> Mixed dependencies</td>
<td>0.3</td>
<td>0</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>The data reveal that physical gestures are mainly used for commands that change the appearance of the map, such as adding information (e.g., <italic>select destination, draw</italic>), changing the orientation (e.g., <italic>rotate</italic>), or changing the scale of the map (e.g., <italic>zoom in, zoom out</italic>). Metaphorical and abstract gestures were more likely used for accessing specific functions, such as <italic>open menu, close menu</italic>, or <italic>help</italic>.</p>
</sec>
<sec id="section29-0018720811433052">
<title>Binding</title>
<p>Object-centric gestures make up the largest category, followed by world-dependent and world-independent gestures (see <xref ref-type="table" rid="table4-0018720811433052">Table 4</xref>). Again, object-centric gestures were more likely chosen for commands that change the appearance of the map, such as <italic>select destination, select area, select start and goal, draw route, plan route, move, rotate, zoom in</italic>, and <italic>zoom out</italic>. World-dependent gestures were more often observed for <italic>change view</italic> and <italic>print</italic> and world-independent gestures for <italic>menu open, menu close</italic>, and <italic>help</italic>. Novices performed twice as many world-independent gestures than did experts, whereas experts showed slightly more object-centric and world-dependent gestures, <italic>p</italic> = .25, ϕ<sub>c</sub> = .13.</p>
</sec>
<sec id="section30-0018720811433052">
<title>Summary</title>
<p>The data revealed preferences in people’s use of hand gestures. Most of the time, gestures were performed with the dominant hand. Participants usually moved the entire hand while the hand shape was kept static. Simple single gestures were preferred to sequences of gestures. Hand shapes varied only a little, whereas motion paths were quite unique, often directly mimicking the effect of the command.</p>
<p>Observational data showed systematic variations in the hand gestures performed depending on the command. Commands can be differentiated into three categories: (a) manipulation of small objects (<italic>select destination, select area, select start and goal, draw route, plan route, undo</italic>), (b) manipulation of larger objects or the entire map (<italic>delete, move map, rotate map, zoom in, zoom out</italic>), and (c) accessing of more abstract functions (<italic>open menu, close menu, help, change view, print</italic>).</p>
<p>Gestures for the first category were object-centric and of a physical nature. Participants mainly used a one-hand pointing gesture and agreed highly in hand shape and motion path. Also, the sheet was touched most of the time for these gestures. Typical gestures for the second category involved both hands with a flat hand shape. Hands were moved synchronously and usually did not touch the sheet during performance. These gestures were object-centric and of a physical nature as well. Agreement scores for hand shape and motion path were lower than for the first category. However, participants agreed more in motion path than in hand shape. Gestures for the third category showed the largest variation, also reflected in the low agreement scores for hand shape and motion path. A flat hand was most often used for these gestures, and the hand or hands did not touch the sheet. In this last category, gestures were of a metaphorical or abstract nature and either world dependent or world independent.</p>
<p>There were a few distinct differences between novices and experts as well. Novices, compared with experts, used both hands more often, especially for commands that changed the appearance of the entire map. In most cases, both hands were moved in a synchronized way with identical hand shapes. Using the hand shape of a flat hand or a pinching gesture could be more often observed among novices than among experts. On the other hand, experts used the unique hand shape of spreading the thumb and index finger, which was not observed among novices. Furthermore, experts used symbolic gestures more often than did novices. Observed differences between experts and novices were supported by statistic analyses. However, results of the statistic analyses should be interpreted with care because the difference in number of analyzed gestures between novices and experts and the unequal distribution of data can be problematic. Also, only the comparisons of agreement scores reached medium effect sizes, whereas most of the other analyses revealed lower effects below .15.</p>
</sec>
</sec>
</sec>
<sec id="section31-0018720811433052">
<title>Experiment 2: Comparing Memorization of Novices’ and Experts’ Gesture Sets</title>
<p>We believe that gesture input systems will be more readily available in the future and that consequently, the majority of users will be novices who do not understand the underlying technology. Therefore, designers of gesture sets must consider the abilities and limitations of novice users. The follow-up study compared the memorization of the novices’ and the experts’ gesture sets from the previous study. We were interested in not only which gesture set was remembered better but also whether novice users showed systematic recognition errors that allow assumptions about users’ preferences. We assumed that natural and universal gestures would be more easily memorized by novices than would artificial or arbitrary gestures because users prefer natural and easy-to-use gestures (<xref ref-type="bibr" rid="bibr6-0018720811433052">Fikkert et al., 2009</xref>).</p>
<sec id="section32-0018720811433052">
<title>Method</title>
<sec id="section33-0018720811433052">
<title>Participants</title>
<p>For the second experiment, 26 students (15 male, 11 female) of Tama University participated as part of their class curriculum. Students were on average 20 years old (<italic>M</italic> = 20.85, <italic>SD</italic> = 1.46). None of these students participated in the previous experiment. Participants had no previous experience with gesture interfaces.</p>
</sec>
<sec id="section34-0018720811433052">
<title>Material</title>
<p>On the basis of observational data, we generated gesture sets for novices and experts (see <xref ref-type="table" rid="table4-0018720811433052">Table 4</xref>). The gesture that was most often performed by either novices or experts for each command was chosen. The commands <italic>select destination, select area</italic>, and <italic>draw route</italic> were not used in the memorization test, since novices and experts demonstrated the same gestures for these commands, resulting in a total of 14 gestures for the test.</p>
</sec>
<sec id="section35-0018720811433052">
<title>Demo video</title>
<p>Each gesture was performed by the researcher and videotaped. The videos were used to create two slide shows, one to demonstrate the novice gesture set and one to demonstrate the expert gesture set. The demonstration of each command started with a slide with the command’s name, followed by a demonstration of the gesture. The demo video took 10 min.</p>
</sec>
<sec id="section36-0018720811433052">
<title>Test video</title>
<p>The test video showed both the experts’ and the novices’ gestures for the commands in random order. The test of each command started with a slide with the command’s name, followed by a demonstration of the experts’ gestures and the novices’ gestures for the command in random order.</p>
</sec>
<sec id="section37-0018720811433052">
<title>Procedure and design</title>
<p>Participants were randomly assigned to two groups. The demo video of either the novice or expert gesture set was shown to each group. Participants were instructed to watch the demo carefully and to memorize each gesture. They were also told that a memory test would follow afterward. After the demo was shown, participants’ attention was distracted for 10 min by a presentation about an unrelated topic. Then the test video was shown. Participants were instructed to watch the two gestures for each command carefully and then to select which of these gestures had been shown in the demo video before. The demo and test videos were shown on a big projector screen. The experiment took about 30 min.</p>
</sec>
</sec>
<sec id="section38-0018720811433052">
<title>Results and Discussion</title>
<p>In general, participants memorized the novices’ as well as the experts’ gesture set very well. Participants with the novice gesture set correctly memorized approximately one gesture more than did participants with the expert gesture set (novice set, <italic>M</italic> = 13.31, <italic>SD</italic> = 1.03; expert set, <italic>M</italic> = 12.15, <italic>SD</italic> = 1.57). A <italic>t</italic> test for independent samples revealed that this difference was significant, <italic>t</italic>(24) = 2.21, <italic>p</italic> &lt; .05, <italic>d</italic> = .87. Even though participants had to identify the correct command out of just two possibilities, the large effect size indicates an advantage for the memorization of the novice’s gesture set compared with the expert’s gesture set.</p>
<p>Participants with the expert set made the most mistakes for <italic>open menu, close menu</italic>, and <italic>plan route</italic>. Participants with the novice gesture set made the most mistakes for <italic>move map</italic>. Participants with the expert set carried out <italic>open menu</italic> by showing a flat hand and <italic>close menu</italic> by showing a fist. Participants with the novice set used a gesture that mimicked the opening or closing of a book. The results indicate that arbitrary gestures, such as making a fist, are difficult to remember since they are not meaningfully related to the command. The problem with the <italic>plan route</italic> command was the order of input. It appears that an input order of start-goal-route is more natural for people than an input order of start-route-goal. The only difference for the <italic>move map</italic> command between the novice and expert gesture sets was the use of one or two hands. Results suggest that using just one hand for carrying out the command is preferable to using two.</p>
</sec>
</sec>
<sec id="section39-0018720811433052">
<title>Experiment 3: Comparing Memorization for Hand Shape and Motion Path of Gestures</title>
<p>The results of the gesture study suggested that the movement path plays an important role in the use of gestures as a communication tool. Each gesture can be basically described by the specific hand shape of the gesture and the movement path of the hand or finger. Again, since we believe that future tabletop applications will be mainly used by novices, we examined whether novice users focus on the hand shape or the motion path of a gesture when they are required to memorize a gesture set. Specifically, we tested whether novices more often falsely selected a gesture that shows a different hand shape with the same motion path as the original gesture than they did a gesture that shows the same hand shape with a different motion path.</p>
<sec id="section40-0018720811433052">
<title>Method</title>
<sec id="section41-0018720811433052">
<title>Participants</title>
<p>For the third experiment, 15 students (10 male, 5 female) of Keio University participated as part of their class curriculum. Students were on average 21 years old (<italic>M</italic> = 21.20, <italic>SD</italic> = 1.15). None of the participants took part in the previous studies. Participants had no previous experience with gesture interfaces.</p>
</sec>
<sec id="section42-0018720811433052">
<title>Material</title>
<p>On the basis of the data from the previous two gesture studies, we created a sample gesture set with 16 distinctive gestures. Gestures were mainly taken from the novice gesture set (see <xref ref-type="table" rid="table3-0018720811433052">Table 3</xref>) because the previous experiment showed that those gestures were better remembered than were gestures from the expert gesture set. However, 5 out of 16 gestures had to be slightly changed for the purpose of our study. The main reasons for the changes was to make the gestures more distinct from each other and to be able to create sets of alternative gestures. For <italic>rotate</italic>, we used a pointing hand shape instead of flat hands. In the <italic>zoom out</italic> command, the palms of the hands were facing each other instead of facing down. For <italic>open menu</italic>, both hands were used instead of just one. We chose the experts’ gesture for <italic>change view</italic> instead of the novices’ gesture because the agreement score of novices for this gesture was very low. And for <italic>print</italic>, a hand shape as if pinching something was used instead of a flat hand.</p>
</sec>
<sec id="section43-0018720811433052">
<title>Demo video</title>
<p>Each gesture was performed by the researcher and videotaped. These videos were used to create a slide show. First, the name of the command was shown on a separate slide, followed by a demonstration of the gesture for the command. The demo video took about 10 min.</p>
</sec>
<sec id="section44-0018720811433052">
<title>Test video</title>
<p>The hand shape and motion path of the original gesture were manipulated to create three distracting gestures (DGs) for each command. The set DG1 showed a gesture with the same hand shape as the original gesture but with a different motion path. Set DG2 showed a gesture with a different hand shape from the original gesture but the same motion path. Set DG3 showed a gesture with a different hand shape and a different motion path. See <xref ref-type="table" rid="table5-0018720811433052">Table 5</xref> for an example. Each gesture was performed by the researcher and videotaped. The test video first showed a slide with the name of the command, followed by a demonstration of the original gesture and the three DGs. The original gestures and DGs were shown in random order for each command. The test video took about 20 min.</p>
<table-wrap id="table5-0018720811433052" position="float">
<label>Table 5:</label>
<caption>
<p>Example Distractor Gestures (DGs) for Memory Test</p>
</caption>
<graphic alternate-form-of="table5-0018720811433052" xlink:href="10.1177_0018720811433052-table5.tif"/>
<table>
<colgroup>
<col align="left"/>
<col align="left"/>
<col align="left"/>
</colgroup>
<tbody>
<tr>
<td>Original gesture</td>
<td>Hand shape</td>
<td>Both hands flat, palms down</td>
</tr>
<tr>
<td/>
<td>Motion path</td>
<td>Hands are spread apart</td>
</tr>
<tr>
<td>DG1: Same hand shape/different motion path</td>
<td>Hand shape</td>
<td>Both hands flat, palms down</td>
</tr>
<tr>
<td/>
<td>Motion path</td>
<td>Palms are turned up</td>
</tr>
<tr>
<td>DG2: Different hand shape/same motion path</td>
<td>Hand shape</td>
<td>Pointing gesture with both hands</td>
</tr>
<tr>
<td/>
<td>Motion path</td>
<td>Hands are spread</td>
</tr>
<tr>
<td>DG3: Different hand shape/different motion path</td>
<td>Hand shape</td>
<td>Pointing gesture with both hands</td>
</tr>
<tr>
<td/>
<td>Motion path</td>
<td>Palms are turned up</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
<sec id="section45-0018720811433052">
<title>Experimental setting and procedure</title>
<p>Participants were instructed to watch the demo video carefully and to memorize each gesture. They were also told that a memory test would follow. The videos were shown on a large projector screen in a classroom. After the demo video, the participants’ attention was distracted for 10 min by a short presentation about an unrelated topic. Then the test video was shown. Participants were instructed to watch the four gestures for each command carefully and then to select the gesture that had been shown in the demo video before. The experiment took about 40 min.</p>
</sec>
<sec id="section46-0018720811433052">
<title>Design</title>
<p>A within-subjects design was used for the recognition task. We tested how often participants falsely selected</p>
<list id="list1-0018720811433052" list-type="alpha-lower">
<list-item><p>a gesture with the same hand shape but a different motion path (DG1),</p></list-item>
<list-item><p>a gesture with a different hand shape but the same motion path (DG2), and</p></list-item>
<list-item><p>a gesture with a different hand shape and a different motion path (DG3).</p></list-item></list>
</sec>
</sec>
<sec id="section47-0018720811433052">
<title>Results and Discussion</title>
<p>Out of the 16 gestures, participants recognized on average 13 gestures correctly (<italic>M</italic> = 12.80, <italic>SD</italic> = 3.53). Participants falsely selected DG2 gestures (<italic>M</italic> = 2.47, <italic>SD</italic> = 2.56) more often than DG1 (<italic>M</italic> = 1.00, <italic>SD</italic> = 0.93) and DG3 gestures (<italic>M</italic> = 0.67, <italic>SD</italic> = 1.40). A one-way ANOVA for repeated measurements revealed that memorization of the gesture sets was significantly different, <italic>F</italic>(3) = 68.58, <italic>p</italic> =.00, <italic>F</italic> = 2.21. In follow-up <italic>t</italic> tests for dependent samples with Bonferroni correction, we tested which groups differed significantly. Results showed that the differences between DG2 and DG1 and between DG2 and DG3 were significant, <italic>t</italic>(14) = 2.47, <italic>p</italic> = .01, <italic>dz</italic> = .64, and <italic>t</italic>(14) = 2.44, <italic>p</italic> = .01, <italic>dz</italic> = .63, respectively. DG1 and DG3 did not differ significantly from each other, <italic>t</italic>(14) = 0.92, <italic>p</italic> = .37, <italic>dz</italic> = .24.</p>
<p>Results indicate that motion path is indeed an important component for memorizing gestures. Participants seemed to focus on the motion path for each gesture rather than on the hand shape when they had to memorize the gesture set. Participants were also asked after the study whether they focused on hand shape or motion path when they had to memorize the gestures. Out of 15 participants, 11 reported that they focused on the motion path and 1 participant focused on hand shape. Also, 1 participant did not answer this question, and 2 participants chose both hand shape and motion path.</p>
</sec>
</sec>
<sec id="section48-0018720811433052" sec-type="discussion">
<title>General Discussion</title>
<p>Common principles for surface gestures were derived from the observation of novices’ and experts’ gestures and the subsequent testing of the gestures’ memorability. What was special about our study was that participants could perform gestures freely without having to consider technological barriers.</p>
<p>Our method of developing a human-based gesture vocabulary resulted in a rich set of data. People were able to generate surface gestures for most of the commands. Performing gestures seemed to be easier for commands that involved the direct manipulation of objects than for commands that accessed more abstract functions. Even though there were no restrictions on the choice of gestures, observational data revealed certain preferences and systematic patterns in the use of hand, chosen hand shapes, and motion paths. Furthermore, observational data showed differences in people’s choice of gestures depending on the command that had to be executed. People’s choice of gestures appears to be affected by the size of the object that has to be manipulated and whether the command involves the direct manipulation of an object or the accessing of a more abstract function of the map.</p>
<p>In general, observational data as well as the results of the memorization tests suggest that surface gestures should focus on the motion pattern of the hand or fingers and not depend on the use of specific hand shapes or fingers. In particular, people who have no experience with gesture interfaces do not care about which fingers or the number of fingers they use to carry out a gesture but, rather, seem to agree and to remember specific motion patterns of the hand. If possible, these motion patterns should reflect the action of the command that is carried out. Otherwise, the gesture should be based on a common metaphor instead of having a symbolic meaning or being abstract.</p>
<p>Commands that involve the direct manipulation of an object appear to be good candidates for being executed by hand gestures, whereas commands for accessing more abstract functions, such as help or support functions or opening a menu, should not entirely depend on gesture input. For these commands, alternative input methods, such as voice input or buttons, should be provided as well.</p>
<p>Comparing experts’ and novices’ gestures suggests that the experts’ choice of gestures is affected by considerations of the recognition technology. Novices do not have this knowledge and therefore do not consider it in their choice of gestures. Experts tend to use more abstract, artificial gestures and gestures that are peculiar to the use of certain fingers or the shaping of certain fingers, whereas novices do not care about hand shape or which fingers they are using. Experts also use more gestures that represent symbols, but novices prefer gestures that represent metaphors. Those differences in the choice of gestures might also explain why the novice gesture set was remembered better than the expert gesture set.</p>
<p>Although the implementation of novices’ gestures into a technical system might prove problematic, the recognition system, although required to be accurate, must also allow for a certain tolerance to error and variation. Designers have to find a good balance between acceptable recognition accuracy and gestures that are natural and intuitive for novice users. General recommendations that can be derived from our study are as follows:</p>
<list id="list2-0018720811433052" list-type="order">
<list-item><p>Keep it simple; do not use a sequence of gestures, complex combinations of different fingers, or symbols. When both hands are used for input, hands should be moved in a synchronized way.</p></list-item>
<list-item><p>Focus on motion paths, not on hand shapes; do not depend on specific hand shapes or specific fingers for input. Use motion patterns that mimic the change on the surface.</p></list-item>
<list-item><p>Implement gestures that have a physical or metaphorical nature instead of gestures that are abstract or symbolic.</p></list-item>
<list-item><p>Gestures should be either object- or world-centric. Metaphorical gestures can be world independent; however, mixed dependencies should be avoided.</p></list-item></list>
<p>This study provided a deeper insight into people’s use of surface gestures. Knowing people’s preferences for surface gestures and discovering common principles of gesture use will aid in the development of applications that do not violate ergonomic principles. Further work should focus on similarities and differences of gestures for certain types of commands and compare gesture use for different applications. In general, it can be said that gestures are affected by the context in which they appear. In future studies, researchers should test whether the same commands executed in different contexts lead to a similar use of gestures or not.</p>
<p>A problem of our study was that novices were on average older than experts. We found it very difficult to find elderly people who had experience with gesture interfaces, navigation systems, or computer maps or younger people who lack this experience. Furthermore, there was only one female expert, whereas more than half of the novices were female. One could argue that differences between novices and experts are attributed to the age difference and unequal gender distribution between groups. Our data samples did not show systematic differences in the performance of gestures between older and younger novices or between males and females. Also, participants in the follow-up study were of similar age as participants in the expert group. Those participants memorized the novices’ gesture set better than the experts’ gesture set. However, since we cannot rule out a possible effect of age and gender, those factors should be controlled in further studies.</p>
<p>A further limitation of our study was that we used only novices and no experts in the follow-up studies. A gesture vocabulary for a specific application has to be tailored to the capabilities and limitations of its eventual users. We focused on novices because they represent an average user of our tabletop systems. An average user would be somebody who does not have any knowledge about the technical requirements of gesture recognition systems. However, in the context of studying people’s use of gestures, it would be interesting to see how experts’ knowledge affects the memorization of gesture sets or whether experts focus on hand shape or the motion pattern of a gesture.</p>
<p>We compared the memorability of novices’ and experts’ gesture sets by a recognition test. In this way, we could directly contrast novices’ and experts’ gestures and find out specific preferences by users. However, this approach has certain limitations as well. When working with a tabletop system, users must be able to perform the gestures accurately. It is therefore necessary to conduct recall tests in which participants have to perform learned gestures. Moreover, having participants perform the gestures provides additional data about physical stress, comfort, and physical and cognitive load. Right now we are in the process of implementing the gesture sets into a tabletop system and plan to conduct further evaluations.</p>
</sec>
<sec id="section49-0018720811433052">
<title>Key Points</title>
<list id="list3-0018720811433052" list-type="bullet">
<list-item><p>First, a gesture study comparing gesture use of novices and experts was conducted.</p></list-item>
<list-item><p>Second, an experimental study comparing memorization for novices and experts was carried out.</p></list-item>
<list-item><p>Third, an experimental study comparing memorization for hand shape and motion path of gestures was conducted.</p></list-item>
</list>
</sec>
</body>
<back>
<ack><p>I thank Professor Yasumura and his laboratory from Keio University and Professor Koike and his laboratory from Chofu University of Electronic Communication for their continuing advice and support.</p></ack>
<bio>
<p>Jacqueline Urakami obtained a doctorate degree in psychology from Chemnitz University of Technology, Germany, in 2002. She is currently working as a post-doc researcher at Keio University, Japan.</p>
</bio>
<ref-list>
<title>References</title>
<ref id="bibr1-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Buchinger</surname><given-names>S.</given-names></name>
<name><surname>Hotop</surname><given-names>E.</given-names></name>
<name><surname>Hlavacs</surname><given-names>H.</given-names></name>
<name><surname>Simone</surname><given-names>F. D.</given-names></name>
<name><surname>Ebrahimi</surname><given-names>T.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Gesture and touch controlled video player interface for mobile devices</article-title>. In <conf-name>MM ’10: Proceedings of the International Conference on Multimedia</conf-name> (pp. <fpage>699</fpage>–<lpage>702</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr2-0018720811433052">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Duncan</surname><given-names>S. D.</given-names></name>
<name><surname>Cassell</surname><given-names>J.</given-names></name>
<name><surname>Levy</surname><given-names>E. T.</given-names></name>
</person-group> (<year>2007</year>). <source>Gesture and the dynamic dimension of language: Essays in honor of David McNeill</source>. <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>John Benjamins</publisher-name>.</citation>
</ref>
<ref id="bibr3-0018720811433052">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Efron</surname><given-names>D.</given-names></name>
</person-group> (<year>1941</year>). <source>Gesture and environment</source>. <publisher-loc>Oxford, UK</publisher-loc>: <publisher-name>King’s Crown Press</publisher-name>.</citation>
</ref>
<ref id="bibr4-0018720811433052">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W. V.</given-names></name>
</person-group> (<year>1969</year>). <article-title>The repertoire of nonverbal behavior: Categories, origins, usage, and coding</article-title>. <source>Semiotica</source>, <volume>1</volume>, <fpage>49</fpage>–<lpage>98</lpage>.</citation>
</ref>
<ref id="bibr5-0018720811433052">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Epps</surname><given-names>J.</given-names></name>
<name><surname>Lichman</surname><given-names>S.</given-names></name>
<name><surname>Wu</surname><given-names>M.</given-names></name>
</person-group> (<year>2006</year>). <article-title>A study of hand shape use in tabletop gesture interaction</article-title>. In <source>CHI ’06 extended abstracts on human factors in computing systems</source> (pp. <fpage>748</fpage>–<lpage>753</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr6-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Fikkert</surname><given-names>W.</given-names></name>
<name><surname>Vet Paul</surname><given-names>v. d.</given-names></name>
<name><surname>Rauwerda</surname><given-names>H.</given-names></name>
<name><surname>Breit</surname><given-names>T.</given-names></name>
<name><surname>Nijholt</surname><given-names>A.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Gestures to intuitively control large displays</article-title>. <conf-name>Proceedings of the 7th International Gesture Workshop</conf-name>. <fpage>199</fpage>–<lpage>204</lpage>. <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer Press</publisher-name></citation>
</ref>
<ref id="bibr7-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Freeman</surname><given-names>D.</given-names></name>
<name><surname>Benko</surname><given-names>H.</given-names></name>
<name><surname>Morris</surname><given-names>M. R.</given-names></name>
<name><surname>Wigdor</surname><given-names>D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>ShadowGuides: Visualizations for in-situ learning of multi-touch and whole-hand gestures</article-title>. In <conf-name>ITS ’09: Proceedings of the ACM International Conference on Interactive Tabletops and Surfaces</conf-name> (pp. <fpage>165</fpage>–<lpage>172</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr8-0018720811433052">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hauptmann</surname><given-names>A. G.</given-names></name>
<name><surname>McAvinney</surname><given-names>P.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Gestures with speech for graphic manipulation</article-title>. <source>International Journal of Man-Machine Studies</source>, <volume>38</volume>, <fpage>231</fpage>–<lpage>249</lpage>.</citation>
</ref>
<ref id="bibr9-0018720811433052">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kendon</surname><given-names>A.</given-names></name>
</person-group> (<year>2004</year>). <source>Gesture: Visible action as utterance</source>. <publisher-loc>Cambridge, UK</publisher-loc>: <publisher-name>Cambridge University Press</publisher-name>.</citation>
</ref>
<ref id="bibr10-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Koike</surname><given-names>H.</given-names></name>
<name><surname>Nishikawa</surname><given-names>W.</given-names></name>
<name><surname>Fukuchi</surname><given-names>K.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Transparent 2-D markers on an LCD tabletop system</article-title>. <conf-name>Proceedings of the 27th International Conference on Human Factors in Computing Systems</conf-name>, <fpage>163</fpage>–<lpage>172</lpage>. <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr11-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Malik</surname><given-names>S.</given-names></name>
<name><surname>Ranja</surname><given-names>A.</given-names></name>
<name><surname>Balakrishnan</surname><given-names>R.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Interacting with large displays from a distance with vision-tracked multi-finger gestural input</article-title>. In <conf-name>Proceedings of the 18th Annual ACM Symposium on User Interface Software and Technology</conf-name> (pp. <fpage>43</fpage>–<lpage>52</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr12-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Mauney</surname><given-names>D.</given-names></name>
<name><surname>Howarth</surname><given-names>J.</given-names></name>
<name><surname>Wirtanen</surname><given-names>A.</given-names></name>
<name><surname>Capra</surname><given-names>M.</given-names></name>
</person-group> (<year>2010</year>, <month>April</month>). <article-title>Cultural similarities and differences in user-defined gestures for touchscreen user interfaces</article-title>. <conf-name>Paper presented at the CHI</conf-name>, <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr13-0018720811433052">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McNeill</surname><given-names>D.</given-names></name>
</person-group> (<year>1992</year>). <source>Hand and mind: What gestures reveal about thought</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr14-0018720811433052">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>McNeill</surname><given-names>D.</given-names></name>
</person-group> (<year>2005</year>). <source>Gesture and thought</source>. <publisher-loc>Chicago, IL</publisher-loc>: <publisher-name>University of Chicago Press</publisher-name>.</citation>
</ref>
<ref id="bibr15-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Nielsen</surname><given-names>M.</given-names></name>
<name><surname>Stoerring</surname><given-names>M.</given-names></name>
<name><surname>Moeslund</surname><given-names>T. B.</given-names></name>
<name><surname>Granum</surname><given-names>E.</given-names></name>
</person-group> (<year>2004</year>). <article-title>A procedure for developing intuitive and ergonomic gesture interfaces for HCI</article-title>. In <conf-name>Proceedings of the 5th International Gesture Workshop</conf-name> (pp. <fpage>409</fpage>–<lpage>420</lpage>). <publisher-loc>Berlin</publisher-loc>: <publisher-name>Springer Press</publisher-name>.</citation>
</ref>
<ref id="bibr16-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Oka</surname><given-names>K.</given-names></name>
<name><surname>Sato</surname><given-names>Y.</given-names></name>
<name><surname>Koike</surname><given-names>H.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Real-time tracking of multiple fingertips and gesture recognition for augmented desk interface systems</article-title>. <comment>Proceedings of the 5th IEEE International Conference on automatic face gesture recognition</comment>. <fpage>64</fpage>–<lpage>71</lpage>. <publisher-loc>New York</publisher-loc>: <publisher-name>IEEE Press</publisher-name>.</citation>
</ref>
<ref id="bibr17-0018720811433052">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Streeck</surname><given-names>J.</given-names></name>
</person-group> (<year>2009</year>). <source>Gesturecraft: The manufacture of meaning</source>. <publisher-loc>Amsterdam, Netherlands</publisher-loc>: <publisher-name>John Benjamins</publisher-name>.</citation>
</ref>
<ref id="bibr18-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Voida</surname><given-names>S.</given-names></name>
<name><surname>Podlaseck</surname><given-names>M.</given-names></name>
<name><surname>Kjeldsen</surname><given-names>R.</given-names></name>
<name><surname>Pinhanez</surname><given-names>C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>A study on the manipulation of 2D objects in a projector/camera-based augmented reality environment</article-title>. In <conf-name>CHI ’05: Proceedings of the SIGCHI Conference on Human factors in Computing Systems</conf-name> (pp. <fpage>611</fpage>–<lpage>620</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
<ref id="bibr19-0018720811433052">
<citation citation-type="confproc">
<person-group person-group-type="author">
<name><surname>Wobbrock</surname><given-names>J. O.</given-names></name>
<name><surname>Morris</surname><given-names>M. R.</given-names></name>
<name><surname>Wilson</surname><given-names>A. D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>User-defined gestures for surface computing</article-title>. In <conf-name>Proceedings of the 27th International Conference on Human Factors in Computing Systems</conf-name> (pp. <fpage>1083</fpage>–<lpage>1092</lpage>). <publisher-loc>New York</publisher-loc>: <publisher-name>ACM Press</publisher-name>.</citation>
</ref>
</ref-list>
</back>
</article>