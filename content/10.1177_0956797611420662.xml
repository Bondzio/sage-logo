<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">PSS</journal-id>
<journal-id journal-id-type="hwp">sppss</journal-id>
<journal-id journal-id-type="nlm-ta">Psychol Sci</journal-id>
<journal-title>Psychological Science</journal-title>
<journal-subtitle>Research, Theory, &amp; Application in Psychology and Related Sciences</journal-subtitle>
<issn pub-type="ppub">0956-7976</issn>
<issn pub-type="epub">1467-9280</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0956797611420662</article-id>
<article-id pub-id-type="publisher-id">10.1177_0956797611420662</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Research Reports</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>0 + 1 &gt; 1</article-title>
<subtitle>How Adding Noninformative Sound Improves Performance on a Visual Task</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<name><surname>Kim</surname><given-names>Robyn</given-names></name>
<xref ref-type="aff" rid="aff1-0956797611420662">1</xref>
</contrib>
<contrib contrib-type="author">
<name><surname>Peters</surname><given-names>Megan A.K.</given-names></name>
<xref ref-type="aff" rid="aff1-0956797611420662">1</xref>
</contrib>
<contrib contrib-type="author" corresp="yes">
<name><surname>Shams</surname><given-names>Ladan</given-names></name>
<xref ref-type="aff" rid="aff1-0956797611420662">1</xref>
<xref ref-type="aff" rid="aff2-0956797611420662">2</xref>
<xref ref-type="aff" rid="aff3-0956797611420662">3</xref>
</contrib>
</contrib-group>
<aff id="aff1-0956797611420662"><label>1</label>Department of Psychology, University of California, Los Angeles</aff>
<aff id="aff2-0956797611420662"><label>2</label>Department of Biomedical Engineering, University of California, Los Angeles</aff>
<aff id="aff3-0956797611420662"><label>3</label>Interdepartmental Neuroscience Program, University of California, Los Angeles</aff>
<author-notes>
<corresp id="corresp1-0956797611420662">Ladan Shams, University of California, Los Angeles, Department of Psychology, Los Angeles, CA 90095-1563 E-mail: <email>ladan@psych.ucla.edu</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2012</year>
</pub-date>
<volume>23</volume>
<issue>1</issue>
<fpage>6</fpage>
<lpage>12</lpage>
<history>
<date date-type="received">
<day>7</day>
<month>1</month>
<year>2011</year>
</date>
<date date-type="accepted">
<day>15</day>
<month>7</month>
<year>2011</year>
</date>
</history>
<permissions>
<copyright-statement>© Association for Psychological Science 2012</copyright-statement>
<copyright-year>2012</copyright-year>
<copyright-holder content-type="society">Association for Psychological Science</copyright-holder>
</permissions>
<abstract>
<p>It is well known that the nervous system combines information from different cues within and across sensory modalities to improve performance on perceptual tasks. In this article, we present results showing that in a visual motion-detection task, concurrent auditory motion stimuli improve accuracy even when they do not provide any useful information for the task. When participants judged which of two stimulus intervals contained visual coherent motion, the addition of identical moving sounds to both intervals improved accuracy. However, this enhancement occurred only with sounds that moved in the same direction as the visual motion. Therefore, it appears that the observed benefit of auditory stimulation is due to auditory-visual interactions at a sensory level. Thus, auditory and visual motion-processing pathways interact at a sensory-representation level in addition to the level at which perceptual estimates are combined.</p>
</abstract>
<kwd-group>
<kwd>multisensory integration</kwd>
<kwd>motion perception</kwd>
<kwd>cross-modal interactions</kwd>
<kwd>perception</kwd>
<kwd>visual perception</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Recent studies have revealed that there are interactions between sensory modalities in a number of brain areas, including primary sensory cortical areas that were historically considered to be self-contained and modality specific (for reviews, see <xref ref-type="bibr" rid="bibr4-0956797611420662">Driver &amp; Noesselt, 2008</xref>; <xref ref-type="bibr" rid="bibr7-0956797611420662">Ghazanfar &amp; Schroeder, 2006</xref>; <xref ref-type="bibr" rid="bibr18-0956797611420662">Schroeder &amp; Foxe, 2005</xref>; <xref ref-type="bibr" rid="bibr24-0956797611420662">Shimojo &amp; Shams, 2001</xref>). In humans, it has been shown that activity in primary visual cortex is modulated by auditory stimuli (<xref ref-type="bibr" rid="bibr26-0956797611420662">Watkins, Shams, Josephs, &amp; Rees, 2007</xref>; <xref ref-type="bibr" rid="bibr27-0956797611420662">Watkins, Shams, Tanaka, Haynes, &amp; Rees, 2006</xref>), and the modulation of activity in occipital areas appears to occur with a short latency (<xref ref-type="bibr" rid="bibr8-0956797611420662">Giard &amp; Peronnet, 1999</xref>; <xref ref-type="bibr" rid="bibr21-0956797611420662">Shams, Iwaki, Chawla, &amp; Bhattacharya, 2005</xref>; <xref ref-type="bibr" rid="bibr22-0956797611420662">Shams, Kamitani, Thompson, &amp; Shimojo, 2001</xref>).</p>
<p>Yet behavioral studies of auditory-visual motion perception have failed to find evidence for sensory integration between the two modalities. For example, in a study of auditory speed perception, the authors concluded that there are interactions between the two modalities at a level lower than the decision-making level, but they did not find any change in auditory sensitivity induced by vision (<xref ref-type="bibr" rid="bibr13-0956797611420662">Lopez-Moliner &amp; Soto-Faraco, 2007</xref>). Other studies that examined motion detection using near-threshold auditory and visual stimuli (<xref ref-type="bibr" rid="bibr1-0956797611420662">Alais &amp; Burr, 2004a</xref>; <xref ref-type="bibr" rid="bibr30-0956797611420662">Wuerger, Hofbauer, &amp; Meyer, 2003</xref>) reported that the detection accuracy in auditory-visual conditions could be explained by statistical (probability summation) or maximum likelihood integration of the two modalities, rather than by sensory-level interactions. It is possible, however, that changes in sensory sensitivity are masked by larger effects caused by higher levels of interaction (e.g., <xref ref-type="bibr" rid="bibr1-0956797611420662">Alais &amp; Burr, 2004a</xref>; <xref ref-type="bibr" rid="bibr30-0956797611420662">Wuerger et al., 2003</xref>).</p>
<p>In the study reported here, we investigated whether there would be any interaction between the visual and auditory modalities if the auditory modality did not produce any information useful for the task. If the interaction between the two modalities in motion processing is confined to higher levels of processing, as suggested by results from previous studies (probability summation or maximum likelihood integration), then no benefit in performance should be observed as a result of adding a noninformative sound to a visual stimulus.</p>
<p>Integration of information across the two modalities at a perceptual or a decision-making level would require that an estimate relevant to the task is derived from each of the modalities; from these estimates, an overall estimate would be produced, resulting in a response. If one of the modalities does not produce an estimate, then that modality cannot contribute to the production of an overall estimate and the response. Therefore, a noninformative sound would not influence performance if the interaction between the two modalities occurs at a perceptual or a decision-making level. On the contrary, results showing that the noninformative sound is beneficial would suggest auditory-visual interactions at a level prior to perceptual integration of estimates or decision making (i.e., at a sensory-representation level).</p>
<sec id="section1-0956797611420662">
<title>Experiment 1</title>
<p>In Experiment 1, we examined auditory-visual interactions in a coherent-motion-detection task using a two-interval forced-choice paradigm. The task was to determine which of two stimulus intervals contained visual coherent motion. It is critical to note that, in addition to including classic unisensory and auditory-visual conditions, we investigated the influence of auditory motion on performance during this visual detection task when identical auditory motion was presented during the two intervals and the auditory stimuli therefore did not provide information useful for the detection task.</p>
<sec id="section2-0956797611420662">
<title>Method</title>
<sec id="section3-0956797611420662">
<title>Stimuli and design</title>
<p>Visual stimuli were dynamic dot patterns of low motion coherence (i.e., only a small minority of the dots moved in the same direction). Each pattern was displayed for an interval of 600 ms, and dot speed was 60°/s. We used only one coherent-motion direction: leftward. Colocalized auditory motion (with the same duration and speed as the visual motion) was created by varying the relative amplitude between speakers placed to the left and right of the monitor on which the visual stimuli were displayed. This process produced a strong percept of lateral motion.</p>
<p>For each subject, we determined the visual and auditory coherence levels required to achieve each of three levels of accuracy in the task. These coherence levels were determined by obtaining psychometric functions for each modality from separate calibration blocks that were run 1 to 2 days before the experimental session. For each subject, visual coherence levels that were chosen produced approximately 55% to 60%, 70% to 75%, and 80% to 85% correct detection in the calibration session; these levels corresponded to difficult, intermediate, and easy levels of difficulty, respectively. Auditory coherence levels that were selected produced approximately 75% and 90% correct detection.</p>
</sec>
<sec id="section4-0956797611420662">
<title>Procedure</title>
<p>The direction, speed, location, and timing (within each stimulus interval) parameters of the coherent visual motion were fixed across trials, and therefore there was little uncertainty about the motion signal. Each trial started with a fixation point, on which observers were instructed to fixate throughout the trial. Subjects performed a two-interval, forced-choice visual-coherent-motion-detection task, in which they were shown two displays in separate intervals: One interval (either the first or second) contained coherent visual motion, and the other contained only random motion (see <xref ref-type="fig" rid="fig1-0956797611420662">Fig. 1</xref>). At the end of each trial, observers were prompted to press one of two keys to indicate in which interval they perceived coherent motion. Feedback (“correct” or “incorrect”) was provided after each response to keep the participants engaged in the task over the course of the experiment. Subjects were told that although the task was visual, they should do their best to pay attention to both auditory and visual stimuli.</p>
<fig id="fig1-0956797611420662" position="float">
<label>Fig. 1.</label>
<caption><p>Trial types used in Experiment 1 (a–d) and Experiment 2 (b–d). In all trials, observers were presented with two sequential stimulus intervals in which they were shown patterns of moving dots (represented here with small arrows for purposes of illustration). In one interval, some of the dots moved in the same direction (highlighted here with shading), and in the other interval, the dots moved randomly. Speakers were placed to the left and right of the monitor on which the visual stimuli were presented, and observers heard a sound during each interval. The direction of the sound varied with the trial type. After the second interval, a response prompt asked observers to identify whether the first or the second interval contained coherent motion. In informative-congruent trials (a), the interval with visual coherent motion was accompanied by sound moving in the same direction, and the interval with random motion was accompanied by nonmoving sound. In noninformative-congruent trials (b), both intervals were accompanied by sound moving in the same direction as the visual coherent motion. In noninformative-incongruent trials (c), both intervals were accompanied by sound moving in the direction opposite that of the coherent visual motion. In visual-only trials (d), no sound accompanied the images.</p></caption>
<graphic xlink:href="10.1177_0956797611420662-fig1.tif"/>
</fig>
<p>Sixty-three subjects were divided into three groups. All three groups completed the same visual-only trials (in which there was no auditory stimulus; <xref ref-type="fig" rid="fig1-0956797611420662">Fig. 1d</xref>), but each group received a different type of audiovisual (AV) trial. Observers in the informative-congruent group received AV trials with sound that was informative for the task (i.e., sound was present in both stimulus intervals, but the sound moved only in the interval with coherent visual motion and in the direction of that motion; <xref ref-type="fig" rid="fig1-0956797611420662">Fig. 1a</xref>). Observers in the noninformative- congruent group received AV trials in which identical sound was presented in both intervals (i.e., the sound moved in the same direction as the coherent visual stimulus; <xref ref-type="fig" rid="fig1-0956797611420662">Fig. 1b</xref>). Because the sound moved identically in both intervals, it provided no indication as to which interval contained the visual coherent motion and thus provided no useful information for the detection task. If the observers in this group closed their eyes, they could not do the task. Observers in the noninformative-incongruent group received AV trials in which sound moved in both intervals but in the direction opposite that of the coherent visual motion (<xref ref-type="fig" rid="fig1-0956797611420662">Fig. 1c</xref>).</p>
<p>Each participant completed a total of 960 trials. Twenty-five percent of these trials were visual only (80 trials at each visual coherence level: easy, medium, and difficult), and 75% were AV trials (120 trials of each of two types at each of the three visual coherence levels). Trials were presented in pseudorandom order. See the Supplemental Material available online for additional details on the methodology and participants.</p>
</sec>
</sec>
<sec id="section5-0956797611420662">
<title>Results</title>
<p>Because the results from the two auditory-coherence levels were similar, we have collapsed these data. Although the three groups performed similarly on visual-only trials, the trials featuring AV stimuli yielded different results; the Group × Trial Type × Visual Difficulty interaction was significant, <italic>F</italic>(4, 120) = 8.4862, <italic>p</italic> &lt; .01. This effect was not influenced by the interval in which the visual target was presented (the four-way interaction was not significant). As expected, the Trial Type × Visual Difficulty interaction was significant in the group that completed informative-congruent trials, <italic>F</italic>(4, 40) = 15.15, <italic>p</italic> &lt; .01. Specifically, subjects performed better when the visual stimulus was accompanied by congruent informative sound than when it was not accompanied by sound (<xref ref-type="fig" rid="fig2-0956797611420662">Fig. 2a</xref>), both at the difficult level, <italic>t</italic>(20) = 5.69, <italic>p</italic> = .000, and at the medium level of difficulty, <italic>t</italic>(20) = 4.27, <italic>p</italic> = .0004 (two-tailed, paired <italic>t</italic> test using Bonferroni-corrected α = .05/9 = .0055).</p>
<fig id="fig2-0956797611420662" position="float">
<label>Fig. 2.</label>
<caption><p>Results of Experiments 1 and 2: mean percentage of correct responses as a function of visual difficulty level and trial type. Results of Experiment 1 are shown separately for the three participant groups. Each group completed visual-only trials in addition to one type of audiovisual trials: (a) informative congruent, (b) noninformative congruent, or (c) noninformative incongruent. Results of Experiment 2 (d) are shown for the single participant group, which completed visual-only, noninformative-congruent, and noninformative-incongruent trials. Error bars show withingroups standard errors. Asterisks denote significant differences between trial types (<italic>p</italic> &lt; .05, Bonferroni-corrected for multiple comparisons).</p></caption>
<graphic xlink:href="10.1177_0956797611420662-fig2.tif"/>
</fig>
<p>Surprisingly, visual motion detection was also significantly enhanced by the noninformative congruent sound, though to a lesser degree (<xref ref-type="fig" rid="fig2-0956797611420662">Fig. 2b</xref>). The Trial Type × Visual Difficulty interaction was significant, <italic>F</italic>(2, 40) = 3.60, <italic>p</italic> &lt; .05. Sound had an effect at the medium difficulty level, <italic>t</italic>(20) = 3.11, <italic>p</italic> = .005 (two-tailed, paired Bonferroni-corrected <italic>t</italic> test of visual-only vs. noninformative-congruent trials), but not at the other difficulty levels. In contrast, noninformative sound moving in the direction opposite that of visual motion did not yield any benefit (<xref ref-type="fig" rid="fig2-0956797611420662">Fig. 2c</xref>), nor did it cause a significant decrease in performance (<italic>p</italic> &gt; .05 for all difficulty levels) relative to visual-only trials. The size of the effect of noninformative congruent sound on the medium-difficulty visual task was moderate (Cohen’s <italic>d</italic> = 0.42).</p>
</sec>
</sec>
<sec id="section6-0956797611420662">
<title>Experiment 2</title>
<p>To scrutinize the relationship between the visual-only, noninformative-congruent, and noninformative-incongruent trials further, we conducted Experiment 2. In this experiment, the three trial types were presented in pseudorandom order to each observer, thus allowing comparison of all three trial types within the same individual.</p>
<sec id="section7-0956797611420662">
<title>Method</title>
<p>Nine new subjects participated in this experiment. All stimuli and procedures were the same as those in Experiment 1, with the following exceptions. A within-subjects design was used (i.e., there was only one group of subjects, and each participant was tested in all types of trial). Three trial types were included: visual only, noninformative congruent, and noninformative incongruent (<xref ref-type="fig" rid="fig1-0956797611420662">Fig. 1</xref>). A total of 360 trials of each type (i.e., 120 trials at each coherence level) were presented in pseudorandom order. To minimize learning during the experiment, we gave no feedback at the end of the trials. Only the auditory-coherence level corresponding to 90% correct was used. See the Supplemental Material for additional details on the methodology and participants.</p>
</sec>
<sec id="section8-0956797611420662">
<title>Results</title>
<p>The results of this experiment (<xref ref-type="fig" rid="fig2-0956797611420662">Fig. 2d</xref>) confirmed the findings of Experiment 1, showing a facilitation of performance exclusively in the noninformative-congruent trials at the intermediate visual-difficulty level, <italic>t</italic>(8) = 3.1763, <italic>p</italic> = .006 (paired one-tailed <italic>t</italic> test of the visual-only vs. noninformative-congruent trials, Bonferroni adjusted α = .05/6 = .008). Also consistent with the results from Experiment 1, the results of Experiment 2 showed that the effect size of noninformative congruent sound on the medium-difficulty visual task was moderate (Cohen’s <italic>d</italic> = 0.45). The interval in which the visual target was presented did not have any significant effects on performance (i.e., there were no significant interactions or main effects, <italic>p</italic> &gt; .05).</p>
</sec>
</sec>
<sec id="section9-0956797611420662" sec-type="discussion">
<title>General Discussion</title>
<p>The patterns of results found in the two experiments reported here are highly consistent despite significant changes in experimental design, such as within-subjects versus between-groups design and feedback versus no feedback. This consistency of results provides evidence for the robustness of the observed effects. Taken together, these data demonstrate that identical moving sound presented in two stimulus intervals can improve the detection of visual coherent motion in one of the intervals, even though it does not provide information about which interval contains the visual coherent motion. Further, this enhancement occurs only when auditory stimuli move in the same direction as visual stimuli.</p>
<p>How can noninformative sound improve performance? In theory, facilitation could be caused by auxiliary factors, but the pattern of results makes this possibility unlikely. For example, the absence of enhancement in the noninformative-incongruent trials rules out the possibility that the effect results from general attentional modulation by sound. It is also unlikely that the enhancement stems from uncertainty reduction by sound, for two reasons. First, the direction, speed, timing, and position of the visual coherent-motion signal were fixed, and observers had been familiarized with the signal during calibration; therefore, there was little uncertainty—if any—about the signal. Second, any putative reduction in uncertainty should have also occurred in the noninformative-incongruent trials.</p>
<p>Similarly, the enhancement cannot be due to any influence of auditory motion on eye movements. First, subjects were instructed to fixate. Second, the lifetime of each dot was no longer than 20 ms, thus not allowing for any benefit from smooth-pursuit eye movement. Third, the opposite-moving sound in the incongruent trials would have deteriorated performance, which was not the case. Therefore, it appears that the enhanced performance in the noninformative-congruent trials reflects sensory interactions between visual and auditory modalities. Indeed, the sensory interactions would be expected to be specific to concordant directions of visual and auditory motion for which integration would occur, and these interactions should not occur for strongly conflicting auditory and visual directions that would not get integrated (i.e., in the noninformative-incongruent trials; <xref ref-type="bibr" rid="bibr20-0956797611420662">Shams &amp; Beierholm, 2010</xref>). It is also interesting to note that this pattern of results is consistent with previously reported patterns of auditory modulation of visual-motion perceptual learning, in which congruent sound facilitates learning (<xref ref-type="bibr" rid="bibr10-0956797611420662">Kim, Seitz, &amp; Shams, 2008</xref>; <xref ref-type="bibr" rid="bibr19-0956797611420662">Seitz, Kim, &amp; Shams, 2006</xref>), and incongruent sound does not affect learning (<xref ref-type="bibr" rid="bibr10-0956797611420662">Kim et al., 2008</xref>).</p>
<p>It is important to note that the pattern of AV enhancement differed between the informative- and noninformative-congruent groups in Experiment 1; this suggests that different mechanisms underlie these two types of enhancement. The informative-congruent group’s pattern demonstrates inverse effectiveness (<xref ref-type="bibr" rid="bibr14-0956797611420662">Meredith &amp; Stein, 1983</xref>): that is, maximal benefit from sound for the weakest visual stimulus and little or no benefit for the easiest visual condition, which already produces high performance. In contrast, the noninformative-congruent group shows more benefit for the medium visual-difficulty level and no benefit for the weakest visual stimulus.</p>
<p>This pattern of effects can be qualitatively explained by a combination of two factors. The differential effect for the difficult (low-coherence) visual stimulus compared with the medium-coherence visual stimulus in the noninformative- congruent trials could potentially be due to a multiplicative interaction between the two modalities at a sensory level. In this type of interaction, if either modality is weak, the interaction itself is also weak and thus difficult to detect, which could explain why no benefit was observed at the difficult-visual-stimulus level. In contrast, the absence of an enhancement for the easy (high-coherence) visual stimulus may simply be due to a ceiling effect, as similarly no enhancement occurred at this level of difficulty in the informative-congruent trials. Therefore, the obtained pattern of results could conceivably be due to a subset of directionally tuned neurons in a visual motion-processing area (e.g., middle temporal, or MT) that are multiplicatively modulated by auditory neurons with the same directional preferences. Indeed, some functional neuroimaging studies have suggested that sound can modulate MT+ activation (<xref ref-type="bibr" rid="bibr16-0956797611420662">Poirier et al., 2005</xref>; <xref ref-type="bibr" rid="bibr17-0956797611420662">Scheef et al., 2009</xref>), and multiplicative modulation of neurons in MT has been reported to occur in the case of attention (<xref ref-type="bibr" rid="bibr25-0956797611420662">Treue &amp; Martínez Trujillo, 1999</xref>). Moreover, in monkeys, ventral intraparietal area receives input from both MT and auditory regions and contains auditory-visual neurons with directional motion selectivity (<xref ref-type="bibr" rid="bibr9-0956797611420662">Graziano, 2001</xref>; <xref ref-type="bibr" rid="bibr12-0956797611420662">Lewis &amp; Van Essen, 2000</xref>). However, further research is necessary to uncover the neural mechanisms underlying these interactions.</p>
<p>Because a two-interval forced-choice paradigm was used in this study, the improved accuracy in visual motion detection found in the presence of congruent but noninformative auditory motion cannot be attributed to a change in response criterion, and therefore it indicates improved sensitivity in visual detection. These results provide evidence for sensory interaction between auditory and visual motion-processing pathways. These findings may appear to conflict with results from previous studies that examined auditory-visual interactions in motion perception and did not find evidence of sensory integration. We suspect that the absence of evidence for sensory interactions in these previous studies is due to the fact that in those studies, both modalities are informative (i.e., the task could be performed on the basis of each modality alone). Therefore, the sensory interaction effects—which appear to be significantly smaller in size—are masked by the comparatively larger perceptual and decision-level interactions. Comparing <xref ref-type="fig" rid="fig2-0956797611420662">Figures 2a</xref> and <xref ref-type="fig" rid="fig2-0956797611420662">2b</xref>, it can be seen that the auditory-visual interactions as observed in the informative-congruent trials (<xref ref-type="fig" rid="fig2-0956797611420662">Fig. 2a</xref>) are much larger than the facilitatory effect seen in the noninformative-congruent trials (<xref ref-type="fig" rid="fig2-0956797611420662">Fig. 2b</xref>). The reason we were able to detect the sensory interactions between the two modalities is likely because the higher-level interactions are disabled by rendering sound noninformative for the task, in effect unmasking the smaller sensory effects.</p>
<p>The observed enhancement of visual detection accuracy in the presence of noninformative congruent auditory motion cannot be explained by the standard models of multisensory integration (e.g., <xref ref-type="bibr" rid="bibr2-0956797611420662">Alais &amp; Burr, 2004b</xref>; <xref ref-type="bibr" rid="bibr3-0956797611420662">Angelaki, Gu, &amp; DeAngelis, 2009</xref>; <xref ref-type="bibr" rid="bibr5-0956797611420662">Ernst &amp; Banks, 2002</xref>; <xref ref-type="bibr" rid="bibr6-0956797611420662">Ernst &amp; Bülthoff, 2004</xref>; <xref ref-type="bibr" rid="bibr28-0956797611420662">Witten &amp; Knudsen, 2005</xref>) depicted in <xref ref-type="fig" rid="fig3-0956797611420662">Figure 3a</xref>. These models are based on the assumption that sensory cues (e.g., auditory signal <italic>A</italic> and visual signal <italic>V</italic>) are caused by a source <italic>s</italic> in the environment, and given this source, that the sensory signals are conditionally independent of each other. This assumption of conditional independence is justified as follows: Up to the stage of processing in which the interaction between modalities occurs (which can be as low level as primary cortical or subcortical brain areas), it is assumed that the two pathways are separate and therefore the noise processes that corrupt the sensory signals are independent of each other. The absence of an arrow between <italic>A</italic> and <italic>V</italic> in <xref ref-type="fig" rid="fig3-0956797611420662">Figure 3a</xref> represents this conditional independence. The cross-modal interactions in these models occur during the process of inferring the value of <italic>s</italic> (e.g., direction of motion, distance, size) from sensory cues <italic>A</italic> and <italic>V</italic> (i.e., after an estimate of <italic>s</italic> is generated by each sensory modality).</p>
<fig id="fig3-0956797611420662" position="float">
<label>Fig. 3.</label>
<caption><p>Graphical models (<xref ref-type="bibr" rid="bibr15-0956797611420662">Pearl, 1988</xref>) typifying models of multisensory integration. The black and gray arrows represent the direction of generative processes and inference processes, respectively. In the standard model of multisensory integration (a), sensory cues, for example, auditory signal <italic>A</italic> and visual signal <italic>V</italic>, are caused by a source <italic>s</italic> in the environment, and given this source, the sensory signals are conditionally independent of each other. In an alternative model (b), there can be an interaction between the two modalities at the level of sensory representations, as indicated by the arrows between <italic>A</italic> and <italic>V</italic>.</p></caption>
<graphic xlink:href="10.1177_0956797611420662-fig3.tif"/>
</fig>
<p>In contrast, the results of the current study can most easily be explained by auditory-visual sensory interactions (<xref ref-type="fig" rid="fig3-0956797611420662">Fig. 3b</xref>), and for the sake of parsimony, that is the explanation we favor. In such a model, the sensory representations are not conditionally independent of each other, and interaction between the two modalities occurs at both the perceptual and sensory levels of processing. However, an alternative explanation based on perceptual interactions (i.e., without the link between <italic>A</italic> and <italic>V</italic>) cannot be ruled out entirely. For example, it is conceivable that a causal-inference model (<xref ref-type="bibr" rid="bibr11-0956797611420662">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bibr23-0956797611420662">Shams, Ma, &amp; Beierholm, 2005</xref>; <xref ref-type="bibr" rid="bibr29-0956797611420662">Wozny, Beierholm, &amp; Shams, 2008</xref>) operating on complex features or a combination of features (e.g., making an inference about the coherence level of motion as well as the direction of motion) could explain the observed interactions. Further research is required to explore these possibilities and to gain further insight into the computational as well as neural mechanisms of auditory-visual motion processing.</p>
</sec>
<sec id="section10-0956797611420662" sec-type="conclusions">
<title>Conclusion</title>
<p>Our results indicate that auditory motion can facilitate perception of visual motion, even when it does not provide information relevant for the task. This finding suggests that interaction between the two modalities in motion processing can occur at a sensory level in addition to higher computational levels.</p>
</sec>
</body>
<back>
<ack>
<p>We thank Gene Stoner, Stefan Schaal, and Julian Wallace for their insightful comments on the manuscript, and Dario Ringach and Zili Liu for helpful discussions.</p>
</ack>
<fn-group>
<fn fn-type="conflict">
<p>The authors declared that they had no conflicts of interest with respect to their authorship or the publication of this article.</p>
</fn>
<fn fn-type="supplementary-material">
<p>Additional supporting information may be found at <ext-link ext-link-type="uri" xlink:href="http://pss.sagepub.com/content/by/supplemental-data">http://pss.sagepub.com/content/by/supplemental-data</ext-link></p>
</fn>
</fn-group>
<ref-list>
<title>References</title>
<ref id="bibr1-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Alais</surname><given-names>D.</given-names></name>
<name><surname>Burr</surname><given-names>D.</given-names></name>
</person-group> (<year>2004a</year>). <article-title>No direction-specific bimodal facilitation for audiovisual motion detection</article-title>. <source>Cognitive Brain Research</source>, <volume>19</volume>, <fpage>185</fpage>–<lpage>194</lpage>.</citation>
</ref>
<ref id="bibr2-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Alais</surname><given-names>D.</given-names></name>
<name><surname>Burr</surname><given-names>D.</given-names></name>
</person-group> (<year>2004b</year>). <article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title>. <source>Current Biology</source>, <volume>14</volume>, <fpage>257</fpage>–<lpage>262</lpage>.</citation>
</ref>
<ref id="bibr3-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Angelaki</surname><given-names>D. E.</given-names></name>
<name><surname>Gu</surname><given-names>Y.</given-names></name>
<name><surname>DeAngelis</surname><given-names>G. C.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Multisensory integration: Psychophysics, neurophysiology, and computation</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>19</volume>, <fpage>452</fpage>–<lpage>458</lpage>.</citation>
</ref>
<ref id="bibr4-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Driver</surname><given-names>J.</given-names></name>
<name><surname>Noesselt</surname><given-names>T.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Multisensory interplay reveals crossmodal influences on “sensory-specific” brain regions, neural responses, and judgments</article-title>. <source>Neuron</source>, <volume>57</volume>, <fpage>11</fpage>–<lpage>23</lpage>.</citation>
</ref>
<ref id="bibr5-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ernst</surname><given-names>M. O.</given-names></name>
<name><surname>Banks</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2002</year>). <article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title>. <source>Nature</source>, <volume>415</volume>, <fpage>429</fpage>–<lpage>433</lpage>.</citation>
</ref>
<ref id="bibr6-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ernst</surname><given-names>M. O.</given-names></name>
<name><surname>Bülthoff</surname><given-names>H. H.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Merging the senses into a robust percept</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>8</volume>, <fpage>162</fpage>–<lpage>169</lpage>.</citation>
</ref>
<ref id="bibr7-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ghazanfar</surname><given-names>A.</given-names></name>
<name><surname>Schroeder</surname><given-names>C. E.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Is neocortex essentially multisensory?</article-title> <source>Trends in Cognitive Sciences</source>, <volume>10</volume>, <fpage>278</fpage>–<lpage>285</lpage>.</citation>
</ref>
<ref id="bibr8-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Giard</surname><given-names>M. H.</given-names></name>
<name><surname>Peronnet</surname><given-names>F.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Auditory-visual integration during multimodal object recognition in humans: A behavioral and electrophysiological study</article-title>. <source>Journal of Cognitive Neuroscience</source>, <volume>11</volume>, <fpage>473</fpage>–<lpage>490</lpage>.</citation>
</ref>
<ref id="bibr9-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Graziano</surname><given-names>M. S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>A system of multimodal areas in the primate brain</article-title>. <source>Neuron</source>, <volume>29</volume>, <fpage>4</fpage>–<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr10-0956797611420662">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Kim</surname><given-names>R. S.</given-names></name>
<name><surname>Seitz</surname><given-names>A. R.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Benefits of stimulus congruency for multisensory facilitation of visual learning</article-title>. <source>PLoS ONE</source>, <volume>3</volume>, <fpage>e1532</fpage>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/article/info:doi/10.1371/journal.pone.0001532">http://www.plosone.org/article/info:doi/10.1371/journal.pone.0001532</ext-link></comment></citation>
</ref>
<ref id="bibr11-0956797611420662">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Körding</surname><given-names>K.</given-names></name>
<name><surname>Beierholm</surname><given-names>U.</given-names></name>
<name><surname>Ma</surname><given-names>W. J.</given-names></name>
<name><surname>Tenenbaum</surname><given-names>J. M.</given-names></name>
<name><surname>Quartz</surname><given-names>S.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Causal inference in multisensory perception</article-title>. <source>PLoS ONE</source>, <volume>2</volume>, <fpage>e943</fpage>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.plosone.org/article/info:doi/10.1371/journal.pone.0000943">http://www.plosone.org/article/info:doi/10.1371/journal.pone.0000943</ext-link></comment></citation>
</ref>
<ref id="bibr12-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lewis</surname><given-names>J.</given-names></name>
<name><surname>Van Essen</surname><given-names>D.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Corticocortical connections of visual, sensorimotor and multimodal processing areas in parietal lobe of macaque monkey</article-title>. <source>Journal of Comparative Neurology</source>, <volume>428</volume>, <fpage>112</fpage>–<lpage>137</lpage>.</citation>
</ref>
<ref id="bibr13-0956797611420662">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Lopez-Moliner</surname><given-names>J.</given-names></name>
<name><surname>Soto-Faraco</surname><given-names>S.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Vision affects how fast we hear sounds move</article-title>. <source>Journal of Vision</source>, <volume>7</volume>(<issue>12</issue>), <comment>Article 6</comment>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/7/12/6">http://www.journalofvision.org/content/7/12/6</ext-link></comment></citation>
</ref>
<ref id="bibr14-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Meredith</surname><given-names>M. A.</given-names></name>
<name><surname>Stein</surname><given-names>B. E.</given-names></name>
</person-group> (<year>1983</year>). <article-title>Interactions among converging sensory inputs in the superior colliculus</article-title>. <source>Science</source>, <volume>221</volume>, <fpage>389</fpage>–<lpage>391</lpage>.</citation>
</ref>
<ref id="bibr15-0956797611420662">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Pearl</surname><given-names>J.</given-names></name>
</person-group> (<year>1988</year>). <source>Probabilistic reasoning in intelligent systems: Networks of plausible inference</source>. <publisher-loc>San Mateo, CA</publisher-loc>: <publisher-name>Morgan Kaufmann</publisher-name>.</citation>
</ref>
<ref id="bibr16-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Poirier</surname><given-names>C.</given-names></name>
<name><surname>Collignon</surname><given-names>O.</given-names></name>
<name><surname>Devolder</surname><given-names>A. G.</given-names></name>
<name><surname>Renier</surname><given-names>L.</given-names></name>
<name><surname>Vanlierde</surname><given-names>A.</given-names></name>
<name><surname>Tranduy</surname><given-names>D.</given-names></name>
<name><surname>Scheiber</surname><given-names>C.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Specific activation of the V5 brain area by auditory motion processing: An fMRI study</article-title>. <source>Cognitive Brain Research</source>, <volume>25</volume>, <fpage>650</fpage>–<lpage>658</lpage>.</citation>
</ref>
<ref id="bibr17-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Scheef</surname><given-names>L.</given-names></name>
<name><surname>Boecker</surname><given-names>H.</given-names></name>
<name><surname>Daamen</surname><given-names>M.</given-names></name>
<name><surname>Fehse</surname><given-names>U.</given-names></name>
<name><surname>Landsberg</surname><given-names>M. W.</given-names></name>
<name><surname>Granath</surname><given-names>D. O.</given-names></name>
<name><surname>Effenberg</surname><given-names>A. O.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Multimodal motion processing in area V5/MT: Evidence from an artificial class of audio-visual events</article-title>. <source>Brain Research</source>, <volume>1252</volume>, <fpage>94</fpage>–<lpage>104</lpage>.</citation>
</ref>
<ref id="bibr18-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schroeder</surname><given-names>C. E.</given-names></name>
<name><surname>Foxe</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Multisensory contributions to low-level, “unisensory” processing</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>15</volume>, <fpage>454</fpage>–<lpage>458</lpage>.</citation>
</ref>
<ref id="bibr19-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Seitz</surname><given-names>A. R.</given-names></name>
<name><surname>Kim</surname><given-names>R.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Sound facilitates visual learning</article-title>. <source>Current Biology</source>, <volume>16</volume>, <fpage>1422</fpage>–<lpage>1427</lpage>.</citation>
</ref>
<ref id="bibr20-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shams</surname><given-names>L.</given-names></name>
<name><surname>Beierholm</surname><given-names>U.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Causal inference in perception</article-title>. <source>Trends in Cognitive Sciences</source>, <volume>14</volume>, <fpage>425</fpage>–<lpage>432</lpage>.</citation>
</ref>
<ref id="bibr21-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shams</surname><given-names>L.</given-names></name>
<name><surname>Iwaki</surname><given-names>S.</given-names></name>
<name><surname>Chawla</surname><given-names>A.</given-names></name>
<name><surname>Bhattacharya</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Early modulation of visual cortex by sound: An MEG study</article-title>. <source>Neuroscience Letters</source>, <volume>378</volume>, <fpage>76</fpage>–<lpage>81</lpage>.</citation>
</ref>
<ref id="bibr22-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shams</surname><given-names>L.</given-names></name>
<name><surname>Kamitani</surname><given-names>Y.</given-names></name>
<name><surname>Thompson</surname><given-names>S.</given-names></name>
<name><surname>Shimojo</surname><given-names>S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Sound alters visual evoked potentials in humans</article-title>. <source>NeuroReport</source>, <volume>12</volume>, <fpage>3849</fpage>–<lpage>3852</lpage>.</citation>
</ref>
<ref id="bibr23-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shams</surname><given-names>L.</given-names></name>
<name><surname>Ma</surname><given-names>W. J.</given-names></name>
<name><surname>Beierholm</surname><given-names>U.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Sound-induced flash illusion as an optimal percept</article-title>. <source>NeuroReport</source>, <volume>16</volume>, <fpage>1923</fpage>–<lpage>1927</lpage>.</citation>
</ref>
<ref id="bibr24-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Shimojo</surname><given-names>S.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Sensory modalities are not separate modalities: Plasticity and interactions</article-title>. <source>Current Opinion in Neurobiology</source>, <volume>11</volume>, <fpage>505</fpage>–<lpage>509</lpage>.</citation>
</ref>
<ref id="bibr25-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Treue</surname><given-names>S.</given-names></name>
<name><surname>Martínez Trujillo</surname><given-names>J. C.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Feature-based attention influences motion processing gain in macaque visual cortex</article-title>. <source>Nature</source>, <volume>399</volume>, <fpage>575</fpage>–<lpage>579</lpage>.</citation>
</ref>
<ref id="bibr26-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Watkins</surname><given-names>S.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
<name><surname>Josephs</surname><given-names>O.</given-names></name>
<name><surname>Rees</surname><given-names>G.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Activity in human V1 follows multisensory perception</article-title>. <source>NeuroImage</source>, <volume>37</volume>, <fpage>572</fpage>–<lpage>578</lpage>.</citation>
</ref>
<ref id="bibr27-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Watkins</surname><given-names>S.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
<name><surname>Tanaka</surname><given-names>S.</given-names></name>
<name><surname>Haynes</surname><given-names>J.-D.</given-names></name>
<name><surname>Rees</surname><given-names>G.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Sound alters activity in human V1 in association with illusory visual perception</article-title>. <source>NeuroImage</source>, <volume>31</volume>, <fpage>1247</fpage>–<lpage>1256</lpage>.</citation>
</ref>
<ref id="bibr28-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Witten</surname><given-names>I. B.</given-names></name>
<name><surname>Knudsen</surname><given-names>E. I.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Why seeing is believing: Merging auditory and visual worlds</article-title>. <source>Neuron</source>, <volume>48</volume>, <fpage>489</fpage>–<lpage>496</lpage>.</citation>
</ref>
<ref id="bibr29-0956797611420662">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Wozny</surname><given-names>D. R.</given-names></name>
<name><surname>Beierholm</surname><given-names>U. R.</given-names></name>
<name><surname>Shams</surname><given-names>L.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Human trimodal perception follows optimal statistical inference</article-title>. <source>Journal of Vision</source>, <volume>8</volume>(<issue>3</issue>), <comment>Article 24</comment>. <comment>Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.journalofvision.org/content/8/3/24">http://www.journalofvision.org/content/8/3/24</ext-link></comment></citation>
</ref>
<ref id="bibr30-0956797611420662">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wuerger</surname><given-names>S. M.</given-names></name>
<name><surname>Hofbauer</surname><given-names>M.</given-names></name>
<name><surname>Meyer</surname><given-names>G. F.</given-names></name>
</person-group> (<year>2003</year>). <article-title>The integration of auditory and visual motion signals at threshold</article-title>. <source>Perception &amp; Psychophysics</source>, <volume>65</volume>, <fpage>1188</fpage>–<lpage>1196</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>