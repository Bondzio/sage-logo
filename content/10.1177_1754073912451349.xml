<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">EMR</journal-id>
<journal-id journal-id-type="hwp">spemr</journal-id>
<journal-title>Emotion Review</journal-title>
<issn pub-type="ppub">1754-0739</issn>
<issn pub-type="epub">1754-0747</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage UK: London, England</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/1754073912451349</article-id>
<article-id pub-id-type="publisher-id">10.1177_1754073912451349</article-id>
<article-categories><subj-group subj-group-type="heading"><subject>Special Section: Facial Expressions</subject></subj-group></article-categories>
<title-group>
<article-title>Effects of Dynamic Aspects of Facial Expressions: A Review</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Krumhuber</surname><given-names>Eva G.</given-names></name>
</contrib>
<contrib contrib-type="author">
<name><surname>Kappas</surname><given-names>Arvid</given-names></name>
<aff id="aff1-1754073912451349">School of Humanities and Social Sciences, Jacobs University Bremen, Germany</aff>
</contrib>
<contrib contrib-type="author">
<name><surname>Manstead</surname><given-names>Antony S. R.</given-names></name>
<aff id="aff2-1754073912451349">School of Psychology, Cardiff University, UK</aff>
</contrib>
</contrib-group>
<author-notes>
<corresp id="corresp1-1754073912451349">Eva Krumhuber, Research IV, Campus Ring 1, Jacobs University Bremen, 28759 Bremen, Germany. <italic>Email</italic>: <email>e.krumhuber@jacobs-university.de</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>1</month>
<year>2013</year>
</pub-date>
<volume>5</volume>
<issue>1</issue>
<fpage>41</fpage>
<lpage>46</lpage>
<permissions>
<copyright-statement>© The Author(s) 2013</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="sage">ISRE and SAGE</copyright-holder>
</permissions>
<abstract>
<p>A key feature of facial behavior is its dynamic quality. However, most previous research has been limited to the use of static images of prototypical expressive patterns. This article explores the role of facial dynamics in the perception of emotions, reviewing relevant empirical evidence demonstrating that dynamic information improves coherence in the identification of affect (particularly for degraded and subtle stimuli), leads to higher emotion judgments (i.e., intensity and arousal), and helps to differentiate between genuine and fake expressions. The findings underline that using static expressions not only poses problems of ecological validity, but also limits our understanding of what facial activity does. Implications for future research on facial activity, particularly for social neuroscience and affective computing, are discussed.</p>
</abstract>
<kwd-group>
<kwd>dynamics</kwd>
<kwd>facial expression</kwd>
<kwd>motion</kwd>
<kwd>temporal</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p>Facial behavior consists of dynamically changing configurations of morphological features as a function of unfolding patterns of underlying muscle activation. Since the publication of Darwin’s <italic>The Expression of the Emotions in Man and Animals</italic> (<xref ref-type="bibr" rid="bibr19-1754073912451349">1872</xref>) there has been scientific interest in facial configurations that are usually referred to as “emotional expressions.” Much of this research has focused on particular patterns at (or very near) the peak intensity of facial movement. There are two types of questions about these patterns that are particularly relevant for emotion research: (a) Do such peak expressions correlate with specific feeling states, physiological responses? (b) Are such peak expressions seen by observers as typical for the presence of a particular emotion?</p>
<p>The former question is relatively easy to answer. There is only a loose coupling between different components of emotions—facial behavior is not a reliable indicator of feeling states or vice versa (e.g., <xref ref-type="bibr" rid="bibr30-1754073912451349">Kappas, 2003</xref>; <xref ref-type="bibr" rid="bibr47-1754073912451349">Mauss &amp; Robinson, 2009</xref>). However, it is quite likely that the prediction of subjective experience can be improved when taking into account indicators of regulation, low-intensity behavior, and particularly dynamic measures. Answering the second question is more complicated. It relates to inferences that are drawn in interaction about the affective states of others. These inferences need not be correlated with the self-report of an interaction partner. Here the question is rather whether particular morphological features are associated with a particular perception or interpretation (see also <xref ref-type="bibr" rid="bibr52-1754073912451349">Russell, Bachorowski, &amp; Fernández-Dols, 2003</xref>). In this line of research, faces are typically presented in a visually isolated fashion (i.e., without a visual context) and without information about who is depicted and what the situation was at the time the picture was taken. To ensure that impressions or judgments are based on a particular facial pattern, researchers often employ actors to portray stereotypes of emotions, or specific predefined patterns of facial actions. In this case it is obvious that the encoders typically do not feel the emotions they are asked to portray. Judges are therefore asked to decode what the actor is supposed to be expressing. Overall, this research has been very successful in achieving its goals (see also <xref ref-type="bibr" rid="bibr52-1754073912451349">Russell et al., 2003</xref>). Thus, it could be shown that particular static patterns are associated with the attribution of specific affective states—even across cultural boundaries. Despite certain criticisms of the methodology used in this research (e.g., number of response alternatives provided; see <xref ref-type="bibr" rid="bibr51-1754073912451349">Russell, 1994</xref>), there is little doubt that there are stereotypical patterns of facial activation that are interpreted as representing particular states, for example, happiness, anger, or fear (see <xref ref-type="bibr" rid="bibr22-1754073912451349">Ekman, Friesen, &amp; Ellsworth, 1972</xref>).</p>
<p>In everyday language, as well as in the scientific discourse, the expression “to recognize an emotion” has been used confusingly but nevertheless consistently to mean <italic>attributing the label that the researcher intended</italic>, based on previous research and/or theory, rather than what the encoder actually felt at the moment. Of course, this also applies to research using synthetic or artificial stimuli, such as line drawings, which cannot logically refer to an underlying affective state.</p>
<p>The present article focuses on the question of whether and how the comparatively neglected dynamic aspect of facial behavior influences the perception of facial patterns, attribution of emotion category labels, along with other aspects of emotion, such as intensity, or authenticity. Facial motion may convey information not only about the presence of an emotional state, but also its unfolding and ending, which can provide strong signals of actions and intent for researchers interested in a nonintrusive measure of emotion, or for interaction partners who are using verbal and nonverbal streams of information in a communication process (<xref ref-type="bibr" rid="bibr33-1754073912451349">Kappas &amp; Descôteaux, 2003</xref>). Given that the visual system evolved under dynamic conditions (see <xref ref-type="bibr" rid="bibr24-1754073912451349">Gibson, 1966</xref>), it seems reasonable to assume that we are highly attuned to motion signals.</p>
<p>The primary objective of the present contribution is to review existing evidence on the role played by facial dynamics in the attributions of affective state. First, the effects of dynamic information are examined with respect to the recognition of emotions as belonging to the intended emotion category. This is followed by an overview of the effects of dynamics on emotion judgments more generally, and on behavioral responses and intentions. Finally, some conclusions concerning the role of dynamic aspects are drawn, finishing with a discussion of the long-term theoretical benefits of studying dynamic expressions.</p>
<sec id="section1-1754073912451349">
<title>Effects on Emotion Recognition Accuracy</title>
<p>Research using point-light or biological motion displays, line drawings, schematic and computer-animated faces suggests that movement enhances the accuracy of attribution of facial affect (<xref ref-type="bibr" rid="bibr6-1754073912451349">Bassili, 1978</xref>, <xref ref-type="bibr" rid="bibr7-1754073912451349">1979</xref>; <xref ref-type="bibr" rid="bibr11-1754073912451349">Bruce &amp; Valentine, 1988</xref>; <xref ref-type="bibr" rid="bibr61-1754073912451349">Wallraven, Breidt, Cunningham, &amp; Bülthoff, 2008</xref>; <xref ref-type="bibr" rid="bibr62-1754073912451349">Wehrle, Kaiser, Schmidt, &amp; Scherer, 2000</xref>). These benefits of dynamic information are most evident when static information is limited (e.g., through degradation in geometry, shape, or texture). The beneficial effects of movement are somewhat weaker or redundant in natural or unmodified faces when complete spatial and textural information is available (e.g., <xref ref-type="bibr" rid="bibr23-1754073912451349">Fiorentini &amp; Viviani, 2011</xref>; <xref ref-type="bibr" rid="bibr29-1754073912451349">Kamachi et al., 2001</xref>, Experiment 2). For example, comparing the identification of basic emotions from two stimulus types, <xref ref-type="bibr" rid="bibr34-1754073912451349">Kätsyri and Sams (2008)</xref> and <xref ref-type="bibr" rid="bibr21-1754073912451349">Ehrlich, Schiano, and Sheridan (2000)</xref> found a recognition advantage for motion in synthetic/schematic faces. However, no difference between static and dynamic displays was observed for natural faces. Similarly, in a study by <xref ref-type="bibr" rid="bibr17-1754073912451349">Cunningham and Wallraven (2009a)</xref> using stimuli of varying resolution (i.e., animated full-surface faces, wireframe faces, and point-light faces), dynamic information generally led to higher recognition performance than static displays, but this difference tended to be larger for point-light faces with low spatial resolution. Motion therefore confers particular benefits when static information is inefficient or unavailable, thereby mitigating the negative consequences of degradation.</p>
<p>As well as the compensating role played by dynamic information under compromised conditions, a benefit is evident when it comes to people who have neurological or developmental disorders (e.g., brain damage or autism). In neuropsychological studies, dynamic presentation significantly facilitated emotion identification in adults and children who were unable to identify or impaired in identifying intended emotional expressions from static displays (<xref ref-type="bibr" rid="bibr5-1754073912451349">Back, Ropar, &amp; Mitchell, 2007</xref>; <xref ref-type="bibr" rid="bibr25-1754073912451349">Harwood, Hall, &amp; Shinkfield, 1999</xref>), supporting the assumption that different neural pathways underpin responses to moving and static stimuli (<xref ref-type="bibr" rid="bibr1-1754073912451349">Adolphs, Tranel, &amp; Damasio, 2003</xref>; <xref ref-type="bibr" rid="bibr28-1754073912451349">Humphreys, Donnelly, &amp; Riddoch, 1993</xref>).</p>
<p>One possible explanation for the benefit conferred by dynamic displays is that a moving sequence consists of multiple static images. Thus the effect of dynamic displays could be attributed to an increase in the amount of static information. However, this is not the case. Using normal human faces, <xref ref-type="bibr" rid="bibr3-1754073912451349">Ambadar, Schooler, and Cohn (2005)</xref> showed that identification of subtle expressions was significantly better for moving sequences compared to “multistatic” images that contained the same number of frames, but with a mask interspersed between each frame in order to disrupt the apparent motion. Thus a dynamic sequence seems to provide a functionally distinct type of information that is not attributable to additional static cues.</p>
<p>However, the intensity of the facial expression moderates the extent to which moving displays result in emotion recognition benefits. Using both subtle and intense expressions, <xref ref-type="bibr" rid="bibr9-1754073912451349">Bould and Morris (2008)</xref> demonstrated that the motion advantage for dynamic as opposed to multistatic displays was reduced for expressions of higher intensity (see also <xref ref-type="bibr" rid="bibr29-1754073912451349">Kamachi et al., 2001</xref>, Experiment 2; <xref ref-type="bibr" rid="bibr62-1754073912451349">Wehrle et al., 2000</xref>). When expressions are intense, it therefore seems that static faces are already strong carriers of emotional signals by corresponding to the shared stereotypes, leaving little scope for improvement through the provision of dynamic information.</p>
<p>In the case of lower intensity expressions it is worth considering how the provision of dynamic information helps perceivers to identify the emotion in question. Clearly, dynamics should enable perceivers to observe how expressions change over time. However, the role of motion extends beyond the mere detection of what has changed in the face. As demonstrated by <xref ref-type="bibr" rid="bibr10-1754073912451349">Bould, Morris, and Wink (2008</xref>, Experiment 1), greater recognition benefits are afforded by dynamic moving sequences than by showing only the first (neutral) and final (peak) frame of an expression (but see <xref ref-type="bibr" rid="bibr3-1754073912451349">Ambadar et al., 2005</xref>, Experiment 2, for contrasting results). The critical advantage seems to lie in the perception of the <italic>direction</italic> in which facial expressions change. This is supported by evidence showing that people are sensitive (even haptically; see <xref ref-type="bibr" rid="bibr44-1754073912451349">Lederman et al., 2007</xref>) to temporal development and can accurately reproduce the temporal progression of a target person’s expression from a scrambled set of photographs (<xref ref-type="bibr" rid="bibr20-1754073912451349">Edwards, 1998</xref>). Such adherence to temporal characteristics was found to be most apparent in the early stages of the expression (see also <xref ref-type="bibr" rid="bibr45-1754073912451349">Leonard, Voeller, &amp; Kuldau, 1991</xref>). By distorting the temporal direction, <xref ref-type="bibr" rid="bibr18-1754073912451349">Cunningham and Wallraven (2009b</xref>, Experiments 3 &amp; 4) demonstrated that the recognition of dynamic expressions significantly decreased when the order of frames was scrambled or reversed (played backwards). Thus, the dynamic advantage does not seem to be solely due to the presence of motion signals, but also arises from diagnostic information embedded in the temporal sequence of the expression.</p>
<p>Moreover, the quality of this embedded information plays an important role in the visual processing of facial expressions. Recent evidence suggests that linear motion animation, in which facial changes occur in a linear manner (as in morphing), results in slower and less accurate emotion recognition, as well as lower judgments of intensity, sincerity, naturalness, and typicality, by comparison with nonlinear (i.e., naturally deforming) facial motions of the same expressions (<xref ref-type="bibr" rid="bibr15-1754073912451349">Cosker, Krumhuber, &amp; Hilton, 2010</xref>; <xref ref-type="bibr" rid="bibr61-1754073912451349">Wallraven et al., 2008</xref>, Experiment 1). Other studies have revealed that the speed with which the face moves significantly affects emotion identification. When speeding up or slowing down the velocity of dynamic change, observers’ performance and naturalness ratings varied in accordance with the type of emotion displayed (<xref ref-type="bibr" rid="bibr10-1754073912451349">Bould et al., 2008</xref>, Experiment 2; <xref ref-type="bibr" rid="bibr27-1754073912451349">Hill, Troje, &amp; Johnston, 2005</xref>; <xref ref-type="bibr" rid="bibr29-1754073912451349">Kamachi et al., 2001</xref>, Experiment 1; <xref ref-type="bibr" rid="bibr55-1754073912451349">Sato &amp; Yoshikawa, 2004</xref>). These findings therefore suggest that characteristics such as the direction, quality, and speed of motion are distinctive features of dynamic information that influence perceivers’ identification and discrimination of emotional expressions.</p>
</sec>
<sec id="section2-1754073912451349">
<title>Effects on Emotion Judgments and Behavioral Responses</title>
<p>Apart from their beneficial role in emotion recognition, facial dynamics have been shown to contribute to various aspects of emotion judgments. For example, there is consistent evidence that dynamic expressions are perceived as more <italic>intense</italic> and <italic>realistic</italic> than static expressions (<xref ref-type="bibr" rid="bibr8-1754073912451349">Biele &amp; Grabowska, 2006</xref>; <xref ref-type="bibr" rid="bibr17-1754073912451349">Cunningham &amp; Wallraven, 2009a</xref>; <xref ref-type="bibr" rid="bibr63-1754073912451349">Weyers, Mühlberger, Hefele, &amp; Pauli, 2006</xref>). This perception of greater emotional intensity might reflect the fact that dynamic change implies a forward shift in the direction of the observed motion (commonly known as “representational momentum”). In a study by <xref ref-type="bibr" rid="bibr65-1754073912451349">Yoshikawa and Sato (2008)</xref>, participants perceived the final image of a dynamic sequence as being more intense than it objectively was. Moreover, as the velocity of facial movement increased, the perceptual image of the facial expression intensified. Facial dynamics may therefore lead to stronger emotional perceptions by inducing larger forward displacements in the apparent motion. Similar effects of dynamic expressions have been demonstrated through ratings of experienced and recognized emotional arousal (<xref ref-type="bibr" rid="bibr53-1754073912451349">Sato, Fujimura, &amp; Suzuki, 2008</xref>; <xref ref-type="bibr" rid="bibr56-1754073912451349">Sato &amp; Yoshikawa, 2007a</xref>).</p>
<p>In addition to enhanced judgments of intensity and arousal, observers are sensitive to dynamic information when judging the <italic>authenticity</italic> of an expression. For example, shorter durations (i.e., onset, offset) and more irregular onset actions have been found to be associated with judgments of politeness (rather than amusement), and lower genuineness and spontaneity in the case of smile expressions (<xref ref-type="bibr" rid="bibr2-1754073912451349">Ambadar, Cohn, &amp; Reed, 2009</xref>; <xref ref-type="bibr" rid="bibr26-1754073912451349">Hess &amp; Kleck, 1994</xref>; <xref ref-type="bibr" rid="bibr37-1754073912451349">Krumhuber &amp; Kappas, 2005</xref>). There is also supportive evidence for the impact of temporal dynamics on person ratings (<xref ref-type="bibr" rid="bibr40-1754073912451349">Krumhuber, Manstead, &amp; Kappas, 2007</xref>) and behavioral intentions and decisions of the observer. Specifically, children showed increased verbal responsiveness, and adults made more favorable employment decisions and more cooperative choices in response to smiles that had longer (compared to shorter) onset and offset durations (<xref ref-type="bibr" rid="bibr12-1754073912451349">Bugental, 1986</xref>; <xref ref-type="bibr" rid="bibr39-1754073912451349">Krumhuber, Manstead, Cosker, Marshall, &amp; Rosin, 2009</xref>; <xref ref-type="bibr" rid="bibr41-1754073912451349">Krumhuber, Manstead, Cosker et al., 2007</xref>).</p>
<p>Several studies have reported stronger and more frequent emotion-specific <italic>reactions</italic> to dynamic as opposed to static expressions (<xref ref-type="bibr" rid="bibr53-1754073912451349">Sato et al., 2008</xref>; <xref ref-type="bibr" rid="bibr57-1754073912451349">Sato &amp; Yoshikawa, 2007b</xref>; <xref ref-type="bibr" rid="bibr63-1754073912451349">Weyers et al., 2006</xref>). These imitative responses, interpretable as facial mimicry, occurred spontaneously and rapidly (<xref ref-type="bibr" rid="bibr60-1754073912451349">Vinter, 1986</xref>) and were found to play a significant role in detecting the dynamic course of emotion facial expressions. For example, in a study by <xref ref-type="bibr" rid="bibr48-1754073912451349">Niedenthal, Brauer, Halberstadt, and Innes-Ker (2001)</xref>, participants who were prevented from mimicking took significantly longer to detect the point at which an emotional expression changed to a categorically different emotion (e.g., happiness changing into sadness, or vice versa), by comparison with when they were allowed to mimic. Other work has shown that spontaneous and deliberate smiles could be distinguished from each other on the basis of dynamic displays, but not static ones (<xref ref-type="bibr" rid="bibr38-1754073912451349">Krumhuber &amp; Manstead, 2009</xref>), and when participants could freely mimic the expressions (<xref ref-type="bibr" rid="bibr46-1754073912451349">Maringer, Krumhuber, Fischer, &amp; Niedenthal, 2011</xref>); when facial mimicry was blocked, perceivers’ ratings of the genuineness of smiles did not distinguish between those that were more or less authentic in their dynamic qualities. Facial mimicry may therefore help perceivers to detect the trajectory of dynamic displays and thereby facilitate the perception of the emotion in question. Together, these findings suggest that temporal dynamics convey unique information that is not only used for judging emotional expressions, but also drives behavior-specific responses and intentions in the perceiver.</p>
</sec>
<sec id="section3-1754073912451349">
<title>Summary and Outlook</title>
<p>A key feature of facial behavior is its dynamic nature. Historically, facial expressions have been studied as slices of behavior, frozen in time, presented out of context, portrayed by encoders who did not actually experience the emotions they are depicting. We have reviewed empirical evidence concerning the role played by dynamic features in the perception of facial emotional behavior. The beneficial effect of dynamic information on recognition accuracy, in the sense that decoders identified specific emotional states with greater coherence, was shown to be particularly apparent for degraded and subtle expressive patterns, and occurred over and above the additional static information contained in moving displays. The direction, quality, and speed of motion emerged as important components of this dynamic information, with significant advantages afforded by expressions which retained their original temporal sequences. Dynamic displays were also shown to enhance emotional judgments (i.e., intensity and arousal), as well as influencing inferences about emotion authenticity, such as whether an expression appears to be genuine or fake. Finally, these responses were found to be facilitated by spontaneous facial mimicry by those perceiving dynamic expressions.</p>
<p>Together, the findings provide strong support for the influential value of facial movements in emotional expressions. As emotions unfold over time, dynamic displays of the temporal sequence are not only of higher ecological validity, but also evoke differential neural activation. In several neuroimaging studies higher brain activity occurred in regions associated with the processing of social- (superior temporal sulci) and emotion-relevant information (amygdalae) when viewing dynamic rather than static expressive faces. Moreover, enhanced activation has been observed in areas linked to perceiving motion (middle temporal gyri) and form-related aspects of faces (fusiform gyri), as well as cognitive processes in general (inferior frontal gyri; <xref ref-type="bibr" rid="bibr35-1754073912451349">Kessler et al., 2011</xref>; <xref ref-type="bibr" rid="bibr36-1754073912451349">Kilts, Egan, Gideon, Ely, &amp; Hoffman, 2003</xref>; <xref ref-type="bibr" rid="bibr43-1754073912451349">LaBar, Crupain, Voyvodic, &amp; McCarthy, 2003</xref>; <xref ref-type="bibr" rid="bibr54-1754073912451349">Sato, Kochiyama, Yoshikawa, Naito, &amp; Matsumura, 2004</xref>; <xref ref-type="bibr" rid="bibr58-1754073912451349">Schultz &amp; Pilz, 2009</xref>; for a review see <xref ref-type="bibr" rid="bibr4-1754073912451349">Arsalidou, Morris, &amp; Taylor, 2011</xref>). This neuroscientific evidence points to a neural network that facilitates social interaction by helping us to understand others, identify their needs, and predict their actions. It is worth noting that the function of such a system may go well beyond a simple mirroring system that employs empathy as its key concept by blending “mind reading,” simulation, empathy, with projected behaviors that are also based on knowledge of others, the situational, and the social context. Here, much research is needed that addresses the complex reality of nonverbal behavior in context, rather than the identification of stereotypical static patterns of facial actions.</p>
<p>To advance scientific knowledge of the communicative and relational processes engaged by facial expressions, the systematic study of the role of dynamic information should be a focus in future research. This requires sophisticated approaches to the measurement and analysis of temporal aspects of facial displays, together with continuous measurements of self-report (e.g., Affect Rating Dial by <xref ref-type="bibr" rid="bibr50-1754073912451349">Ruef &amp; Levenson, 2007</xref>; Dynamic Decoding Device by <xref ref-type="bibr" rid="bibr59-1754073912451349">Tcherkassof, Bollon, Dubois, Pansu, &amp; Adam, 2007</xref>) that allow subjective ratings to be made over time, as well as physiological responses (see also <xref ref-type="bibr" rid="bibr47-1754073912451349">Mauss &amp; Robinson, 2009</xref>). As facial expressions commonly appear alongside other verbal and nonverbal cues (e.g., gaze, head orientation, gestures, speech), emotion perception needs to be considered as a process in which several dynamic acts are temporally integrated to produce meaning. It is these dynamic patterns of facial expressions that demand future attention and multilevel analysis (see <xref ref-type="bibr" rid="bibr42-1754073912451349">Krumhuber &amp; Scherer, 2011</xref>; <xref ref-type="bibr" rid="bibr64-1754073912451349">With &amp; Kaiser, 2011</xref>). The broader issue is what nonverbal behavior <italic>does</italic> in interaction (<xref ref-type="bibr" rid="bibr33-1754073912451349">Kappas &amp; Descoôteaux, 2003</xref>). Arguably, facial behavior can be understood as serving a variety of functions that include intra- and interpersonal emotional regulation (<xref ref-type="bibr" rid="bibr13-1754073912451349">Butler, 2011</xref>; <xref ref-type="bibr" rid="bibr32-1754073912451349">Kappas, 2011</xref>). From this perspective, the cohesion of subjective feeling state and expression may be often low because facial behavior is not a running commentary on what we feel, but also relates to conscious and unconscious attempts to influence what we feel, what others think we want, what we want others to feel, etcetera. In colloquial terms: facial behaviors <italic>do things to us and to others</italic>. We will fail to arrive at a proper understanding of what faces do if we continue to use static snapshots of faces as a paradigm for researching facial expressions.</p>
<p>With the emergence of technology in the field of affective computing, dynamic properties promise to be key factors in the automatic extraction and resynthesis of realistic human behavior. The first advances have already been made by incorporating dynamic data into the measurement and modeling of facial actions (e.g., <xref ref-type="bibr" rid="bibr16-1754073912451349">Cosker, Krumhuber, &amp; Hilton, 2011</xref>; <xref ref-type="bibr" rid="bibr49-1754073912451349">Pantic &amp; Patras, 2006</xref>; see also <xref ref-type="bibr" rid="bibr14-1754073912451349">Calvo &amp; D’Mello, 2010</xref>; <xref ref-type="bibr" rid="bibr31-1754073912451349">Kappas, 2010</xref>). It falls to future research to make use of these techniques and to treat dynamic information as an integral part of facial behavior. Once the tradition of employing highly intense and prototypical static expressions has been overcome, we will be able to grasp the true nature and function of facial actions as they take place in everyday interactions.</p>
</sec>
</body>
<back>
<ref-list>
<title>References</title>
<ref id="bibr1-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Adolphs</surname><given-names>R.</given-names></name>
<name><surname>Tranel</surname><given-names>D.</given-names></name>
<name><surname>Damasio</surname><given-names>A. R.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Dissociable neural systems for recognizing emotions</article-title>. <source>Brain and Cognition</source>, <volume>52</volume>, <fpage>61</fpage>–<lpage>69</lpage>.</citation>
</ref>
<ref id="bibr2-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ambadar</surname><given-names>Z.</given-names></name>
<name><surname>Cohn</surname><given-names>J. F.</given-names></name>
<name><surname>Reed</surname><given-names>L. I.</given-names></name>
</person-group> (<year>2009</year>). <article-title>All smiles are not created equal: Morphology and timing of smiles perceived as amused, polite, and embarrassed/nervous</article-title>. <source>Journal of Nonverbal Behavior</source>, <volume>33</volume>, <fpage>17</fpage>–<lpage>34</lpage>.</citation>
</ref>
<ref id="bibr3-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Ambadar</surname><given-names>Z.</given-names></name>
<name><surname>Schooler</surname><given-names>J.</given-names></name>
<name><surname>Cohn</surname><given-names>J.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Deciphering the enigmatic face: The importance of facial dynamics in interpreting subtle facial expressions</article-title>. <source>Psychological Science</source>, <volume>16</volume>, <fpage>403</fpage>–<lpage>410</lpage>.</citation>
</ref>
<ref id="bibr4-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Arsalidou</surname><given-names>M.</given-names></name>
<name><surname>Morris</surname><given-names>D.</given-names></name>
<name><surname>Taylor</surname><given-names>M. J.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Converging evidence for the advantage of dynamic facial expressions</article-title>. <source>Brain Topography</source>, <volume>24</volume>, <fpage>149</fpage>–<lpage>163</lpage>.</citation>
</ref>
<ref id="bibr5-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Back</surname><given-names>E.</given-names></name>
<name><surname>Ropar</surname><given-names>D.</given-names></name>
<name><surname>Mitchell</surname><given-names>P.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Do the eyes have it? Inferring mental states from animated faces in autism</article-title>. <source>Child Development</source>, <volume>78</volume>, <fpage>397</fpage>–<lpage>411</lpage>.</citation>
</ref>
<ref id="bibr6-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bassili</surname><given-names>J. N.</given-names></name>
</person-group> (<year>1978</year>). <article-title>Facial motion in the perception of faces and of emotional expression</article-title>. <source>Journal of Experimental Psychology: Human Perception and Performance</source>, <volume>4</volume>, <fpage>373</fpage>–<lpage>379</lpage>.</citation>
</ref>
<ref id="bibr7-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bassili</surname><given-names>J. N.</given-names></name>
</person-group> (<year>1979</year>). <article-title>Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>37</volume>, <fpage>2049</fpage>–<lpage>2058</lpage>.</citation>
</ref>
<ref id="bibr8-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Biele</surname><given-names>C.</given-names></name>
<name><surname>Grabowska</surname><given-names>A.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Sex differences in perception of emotion intensity in dynamic and static facial expressions</article-title>. <source>Experimental Brain Research</source>, <volume>26</volume>, <fpage>1</fpage>–<lpage>6</lpage>.</citation>
</ref>
<ref id="bibr9-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bould</surname><given-names>E.</given-names></name>
<name><surname>Morris</surname><given-names>N.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Role of motion signals in recognizing subtle facial expressions of emotion</article-title>. <source>British Journal of Psychology</source>, <volume>99</volume>, <fpage>167</fpage>–<lpage>189</lpage>.</citation>
</ref>
<ref id="bibr10-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bould</surname><given-names>E.</given-names></name>
<name><surname>Morris</surname><given-names>N.</given-names></name>
<name><surname>Wink</surname><given-names>B.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Recognising subtle emotional expressions: The role of facial movements</article-title>. <source>Cognition &amp; Emotion</source>, <volume>22</volume>, <fpage>1569</fpage>–<lpage>1587</lpage>.</citation>
</ref>
<ref id="bibr11-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Bruce</surname><given-names>V.</given-names></name>
<name><surname>Valentine</surname><given-names>T.</given-names></name>
</person-group> (<year>1988</year>). <article-title>When a nod’s as good as a wink: The role of dynamic information in facial recognition</article-title>. In <person-group person-group-type="editor">
<name><surname>Gruneberg</surname><given-names>M. M.</given-names></name>
<name><surname>Morris</surname><given-names>P. E.</given-names></name>
<name><surname>Sykes</surname><given-names>R. N.</given-names></name>
</person-group> (Eds.), <source>Practical aspects of memory: Current research and issues</source> (<volume>Vol. 1</volume>, pp. <fpage>169</fpage>–<lpage>174</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>John Wiley &amp; Sons</publisher-name>.</citation>
</ref>
<ref id="bibr12-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Bugental</surname><given-names>D. B.</given-names></name>
</person-group> (<year>1986</year>). <article-title>Unmasking the “polite smile”: Situational and personal determinants of managed affect in adult–child interaction</article-title>. <source>Personality and Social Psychology Bulletin</source>, <volume>12</volume>, <fpage>7</fpage>–<lpage>16</lpage>.</citation>
</ref>
<ref id="bibr13-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Butler</surname><given-names>E. A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Temporal interpersonal emotion systems: The “TIES” that form relationships</article-title>. <source>Personality and Social Psychology Review</source>, <volume>15</volume>, <fpage>367</fpage>–<lpage>393</lpage>.</citation>
</ref>
<ref id="bibr14-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Calvo</surname><given-names>R. A.</given-names></name>
<name><surname>D’Mello</surname><given-names>S. K.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Affect detection: An interdisciplinary review of models, methods, and their applications</article-title>. <source>IEEE Transactions on Affective Computing</source>, <volume>1</volume>, <fpage>18</fpage>–<lpage>37</lpage>.</citation>
</ref>
<ref id="bibr15-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cosker</surname><given-names>D.</given-names></name>
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Hilton</surname><given-names>A.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Perception of linear and nonlinear motion properties using a FACS validated 3D facial model</article-title>. In <person-group person-group-type="editor">
<name><surname>Gutierrez</surname><given-names>D.</given-names></name>
<name><surname>Kearney</surname><given-names>J.</given-names></name>
<name><surname>Banks</surname><given-names>J.</given-names></name>
<name><surname>Mania</surname><given-names>K.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the Symposium on Applied Perception in Graphics and Visualization (APGV)</source> (pp. <fpage>101</fpage>–<lpage>108</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation>
</ref>
<ref id="bibr16-1754073912451349">
<citation citation-type="web">
<person-group person-group-type="author">
<name><surname>Cosker</surname><given-names>D.</given-names></name>
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Hilton</surname><given-names>A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>A FACS valid 3D dynamic action unit database with applications to 3D dynamic morphable facial modeling</article-title>. In <person-group person-group-type="editor">
<name><surname>Metaxas</surname><given-names>D.</given-names></name>
<name><surname>Quan</surname><given-names>L.</given-names></name>
<name><surname>Sanfeliu</surname><given-names>A.</given-names></name>
<name><surname>van Gool</surname><given-names>L.</given-names></name>
</person-group> (Eds.), <conf-name>Proceedings of the 13th International Conference on Computer Vision (ICCV)</conf-name> (pp. <fpage>2296</fpage>–<lpage>2303</lpage>). Retrieved from <ext-link ext-link-type="uri" xlink:href="http://www.computer.org/portal/web/csdl/doi/10.1109/ICCV.2011.6126510">http://www.computer.org/portal/web/csdl/doi/10.1109/ICCV.2011.6126510</ext-link></citation>
</ref>
<ref id="bibr17-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Cunningham</surname><given-names>D. W.</given-names></name>
<name><surname>Wallraven</surname><given-names>C.</given-names></name>
</person-group> (<year>2009a</year>). <article-title>The interaction between motion and form in expression recognition</article-title>. In <person-group person-group-type="editor">
<name><surname>Bodenheimer</surname><given-names>B.</given-names></name>
<name><surname>O’Sullivan</surname><given-names>B.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the 6th Symposium on Applied Perception in Graphics and Visualization (APGV2009)</source> (pp. <fpage>41</fpage>–<lpage>44</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation>
</ref>
<ref id="bibr18-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Cunningham</surname><given-names>D. W.</given-names></name>
<name><surname>Wallraven</surname><given-names>C.</given-names></name>
</person-group> (<year>2009b</year>). <article-title>Dynamic information for the recognition of conversational expressions</article-title>. <source>Journal of Vision</source>, <volume>9</volume>, <fpage>1</fpage>–<lpage>17</lpage>.</citation>
</ref>
<ref id="bibr19-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Darwin</surname><given-names>C.</given-names></name>
</person-group> (<year>1872</year>). <source>The expression of the emotions in man and animals</source>. <publisher-loc>London, UK</publisher-loc>: <publisher-name>John Murray</publisher-name>.</citation>
</ref>
<ref id="bibr20-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Edwards</surname><given-names>K.</given-names></name>
</person-group> (<year>1998</year>). <article-title>The face of time: Temporal cues in facial expressions of emotion</article-title>. <source>Psychological Science</source>, <volume>9</volume>, <fpage>270</fpage>–<lpage>276</lpage>.</citation>
</ref>
<ref id="bibr21-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ehrlich</surname><given-names>S. M.</given-names></name>
<name><surname>Schiano</surname><given-names>D. J.</given-names></name>
<name><surname>Sheridan</surname><given-names>K.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Communicating facial affect: It’s not the realism, it’s the motion</article-title>. In <person-group person-group-type="editor">
<name><surname>Szwillus</surname><given-names>G.</given-names></name>
<name><surname>Turner</surname><given-names>T.</given-names></name>
</person-group> (Eds.), <source>Proceedings of the ACM CHI 2000 Conference on Human Factors in Computing Systems</source> (pp. <fpage>252</fpage>–<lpage>253</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>.</citation>
</ref>
<ref id="bibr22-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ekman</surname><given-names>P.</given-names></name>
<name><surname>Friesen</surname><given-names>W. V.</given-names></name>
<name><surname>Ellsworth</surname><given-names>P.</given-names></name>
</person-group> (<year>1972</year>). <source>Emotion in the human face: Guidelines for research and an integration of findings</source>. <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Pergamon Press</publisher-name>.</citation>
</ref>
<ref id="bibr23-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Fiorentini</surname><given-names>C.</given-names></name>
<name><surname>Viviani</surname><given-names>P.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Is there a dynamic advantage for facial expressions?</article-title> <source>Journal of Vision</source>, <volume>11</volume>, <fpage>1</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr24-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Gibson</surname><given-names>J. J.</given-names></name>
</person-group> (<year>1966</year>). <source>The senses considered as perceptual systems</source>. <publisher-loc>Boston, MA</publisher-loc>: <publisher-name>Houghton Mifflin</publisher-name>.</citation>
</ref>
<ref id="bibr25-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Harwood</surname><given-names>N. K.</given-names></name>
<name><surname>Hall</surname><given-names>L. J.</given-names></name>
<name><surname>Shinkfield</surname><given-names>A. J.</given-names></name>
</person-group> (<year>1999</year>). <article-title>Recognition of facial emotional expressions from moving and static displays by individuals with mental retardation</article-title>. <source>American Journal of Mental Retardation</source>, <volume>104</volume>, <fpage>270</fpage>–<lpage>278</lpage>.</citation>
</ref>
<ref id="bibr26-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hess</surname><given-names>U.</given-names></name>
<name><surname>Kleck</surname><given-names>R. E.</given-names></name>
</person-group> (<year>1994</year>). <article-title>The cues decoders use in attempting to differentiate emotion-elicited and posed facial expressions</article-title>. <source>European Journal of Social Psychology</source>, <volume>24</volume>, <fpage>367</fpage>–<lpage>381</lpage>.</citation>
</ref>
<ref id="bibr27-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Hill</surname><given-names>H. C. H.</given-names></name>
<name><surname>Troje</surname><given-names>N. F.</given-names></name>
<name><surname>Johnston</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Range- and domain-specific exaggeration of facial speech</article-title>. <source>Journal of Vision</source>, <volume>5</volume>, <fpage>793</fpage>–<lpage>807</lpage>.</citation>
</ref>
<ref id="bibr28-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Humphreys</surname><given-names>G. W.</given-names></name>
<name><surname>Donnelly</surname><given-names>N.</given-names></name>
<name><surname>Riddoch</surname><given-names>M. J.</given-names></name>
</person-group> (<year>1993</year>). <article-title>Expression is computed separately from facial identity, and it is computed separately for moving and static faces: Neuropsychological evidence</article-title>. <source>Neuropsychologia</source>, <volume>31</volume>, <fpage>173</fpage>–<lpage>181</lpage>.</citation>
</ref>
<ref id="bibr29-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kamachi</surname><given-names>M.</given-names></name>
<name><surname>Bruce</surname><given-names>V.</given-names></name>
<name><surname>Mukaida</surname><given-names>S.</given-names></name>
<name><surname>Gyoba</surname><given-names>J.</given-names></name>
<name><surname>Yoshikawa</surname><given-names>S.</given-names></name>
<name><surname>Akamatsu</surname><given-names>S.</given-names></name>
</person-group> (<year>2001</year>). <article-title>Dynamic properties influence the perception of facial expressions</article-title>. <source>Perception</source>, <volume>30</volume>, <fpage>875</fpage>–<lpage>887</lpage>.</citation>
</ref>
<ref id="bibr30-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kappas</surname><given-names>A.</given-names></name>
</person-group> (<year>2003</year>). <article-title>What facial activity can and cannot tell us about emotions</article-title>. In <person-group person-group-type="editor">
<name><surname>Katsikitis</surname><given-names>M.</given-names></name>
</person-group> (Ed.), <source>The human face: Measurement and meaning</source> (pp. <fpage>215</fpage>–<lpage>234</lpage>). <publisher-loc>Dordrecht, The Netherlands</publisher-loc>: <publisher-name>Kluwer Academic Publishers</publisher-name>.</citation>
</ref>
<ref id="bibr31-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kappas</surname><given-names>A.</given-names></name>
</person-group> (<year>2010</year>). <article-title>Smile when you read this, whether you like it or not: Conceptual challenges to affect detection</article-title>. <source>IEEE Transactions on Affective Computing</source>, <volume>1</volume>, <fpage>38</fpage>–<lpage>41</lpage>.</citation>
</ref>
<ref id="bibr32-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kappas</surname><given-names>A.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Emotion and regulation are one!</article-title> <source>Emotion Review</source>, <volume>3</volume>, <fpage>17</fpage>–<lpage>25</lpage>.</citation>
</ref>
<ref id="bibr33-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Kappas</surname><given-names>A.</given-names></name>
<name><surname>Descôteaux</surname><given-names>J.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Of butterflies and roaring thunder: Nonverbal communication in interaction and regulation of emotion</article-title>. In <person-group person-group-type="editor">
<name><surname>Philippot</surname><given-names>P.</given-names></name>
<name><surname>Coats</surname><given-names>E. J.</given-names></name>
<name><surname>Feldman</surname><given-names>R. S.</given-names></name>
</person-group> (Eds.), <source>Nonverbal behavior in clinical settings</source> (pp. <fpage>45</fpage>–<lpage>74</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr34-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kätsyri</surname><given-names>J.</given-names></name>
<name><surname>Sams</surname><given-names>M.</given-names></name>
</person-group> (<year>2008</year>). <article-title>The effect of dynamics on identifying basic emotions from synthetic and natural faces</article-title>. <source>International Journal of Human–Computer Studies</source>, <volume>66</volume>, <fpage>233</fpage>–<lpage>242</lpage>.</citation>
</ref>
<ref id="bibr35-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kessler</surname><given-names>H.</given-names></name>
<name><surname>Doyen-Waldecker</surname><given-names>C.</given-names></name>
<name><surname>Hofer</surname><given-names>C.</given-names></name>
<name><surname>Hoffmann</surname><given-names>H.</given-names></name>
<name><surname>Traue</surname><given-names>H. C.</given-names></name>
<name><surname>Abler</surname><given-names>B.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Neural correlates of the perception of dynamic versus static facial expressions of emotion</article-title>. <source>Psychosocial Medicine</source>, <volume>8</volume>, <fpage>1</fpage>–<lpage>8</lpage>.</citation>
</ref>
<ref id="bibr36-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Kilts</surname><given-names>C. D.</given-names></name>
<name><surname>Egan</surname><given-names>G.</given-names></name>
<name><surname>Gideon</surname><given-names>D. A.</given-names></name>
<name><surname>Ely</surname><given-names>T. D.</given-names></name>
<name><surname>Hoffman</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Dissociable neural pathways are involved in the recognition of emotion in static and dynamic facial expressions</article-title>. <source>Neuroimage</source>, <volume>18</volume>, <fpage>158</fpage>–<lpage>168</lpage>.</citation>
</ref>
<ref id="bibr37-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Kappas</surname><given-names>A.</given-names></name>
</person-group> (<year>2005</year>). <article-title>Moving smiles: The role of dynamic components for the perception of the genuineness of smiles</article-title>. <source>Journal of Nonverbal Behavior</source>, <volume>29</volume>, <fpage>3</fpage>–<lpage>24</lpage>.</citation>
</ref>
<ref id="bibr38-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Manstead</surname><given-names>A. S. R.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Can Duchenne smiles be feigned? New evidence on felt and false smiles</article-title>. <source>Emotion</source>, <volume>9</volume>, <fpage>807</fpage>–<lpage>820</lpage>.</citation>
</ref>
<ref id="bibr39-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Manstead</surname><given-names>A. S. R.</given-names></name>
<name><surname>Cosker</surname><given-names>D.</given-names></name>
<name><surname>Marshall</surname><given-names>D.</given-names></name>
<name><surname>Rosin</surname><given-names>P. L.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Effects of dynamic attributes of smiles in human and synthetic faces: A simulated job interview setting</article-title>. <source>Journal of Nonverbal Behavior</source>, <volume>33</volume>, <fpage>1</fpage>–<lpage>15</lpage>.</citation>
</ref>
<ref id="bibr40-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Manstead</surname><given-names>A. S. R.</given-names></name>
<name><surname>Cosker</surname><given-names>D.</given-names></name>
<name><surname>Marshall</surname><given-names>D.</given-names></name>
<name><surname>Rosin</surname><given-names>P. L.</given-names></name>
<name><surname>Kappas</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Facial dynamics as indicators of trustworthiness and cooperative behavior</article-title>. <source>Emotion</source>, <volume>7</volume>, <fpage>730</fpage>–<lpage>735</lpage>.</citation>
</ref>
<ref id="bibr41-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Manstead</surname><given-names>A. S. R.</given-names></name>
<name><surname>Kappas</surname><given-names>A.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Temporal aspects of facial displays in person and expression perception: The effects of smile dynamics, head-tilt and gender</article-title>. <source>Journal of Nonverbal Behavior</source>, <volume>31</volume>, <fpage>39</fpage>–<lpage>56</lpage>.</citation>
</ref>
<ref id="bibr42-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Affect bursts: Dynamic patterns of facial expression</article-title>. <source>Emotion</source>, <volume>11</volume>, <fpage>825</fpage>–<lpage>841</lpage>.</citation>
</ref>
<ref id="bibr43-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>LaBar</surname><given-names>K. S.</given-names></name>
<name><surname>Crupain</surname><given-names>M. J.</given-names></name>
<name><surname>Voyvodic</surname><given-names>J. T.</given-names></name>
<name><surname>McCarthy</surname><given-names>G.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Dynamic perception of facial affect and identity in the human brain</article-title>. <source>Cerebral Cortex</source>, <volume>13</volume>, <fpage>1023</fpage>–<lpage>1033</lpage>.</citation>
</ref>
<ref id="bibr44-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Lederman</surname><given-names>S. J.</given-names></name>
<name><surname>Klatzky</surname><given-names>R. L.</given-names></name>
<name><surname>Abramowicz</surname><given-names>A.</given-names></name>
<name><surname>Salsman</surname><given-names>K.</given-names></name>
<name><surname>Kitada</surname><given-names>R.</given-names></name>
<name><surname>Hamilton</surname><given-names>C.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Haptic recognition of static and dynamic expressions of emotion in the life face</article-title>. <source>Psychological Science</source>, <volume>18</volume>, <fpage>158</fpage>–<lpage>164</lpage>.</citation>
</ref>
<ref id="bibr45-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Leonard</surname><given-names>C. M.</given-names></name>
<name><surname>Voeller</surname><given-names>K. K.</given-names></name>
<name><surname>Kuldau</surname><given-names>J. M.</given-names></name>
</person-group> (<year>1991</year>). <article-title>When’s a smile a smile? Or how to detect a message by digitizing the signal</article-title>. <source>Psychological Science</source>, <volume>2</volume>, <fpage>166</fpage>–<lpage>172</lpage>.</citation>
</ref>
<ref id="bibr46-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Maringer</surname><given-names>M.</given-names></name>
<name><surname>Krumhuber</surname><given-names>E.</given-names></name>
<name><surname>Fischer</surname><given-names>A. H.</given-names></name>
<name><surname>Niedenthal</surname><given-names>P. M.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Beyond smile dynamics: Mimicry and beliefs in judgments of smiles</article-title>. <source>Emotion</source>, <volume>11</volume>, <fpage>181</fpage>–<lpage>187</lpage>.</citation>
</ref>
<ref id="bibr47-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Mauss</surname><given-names>I. B.</given-names></name>
<name><surname>Robinson</surname><given-names>M. D.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Measures of emotion: A review</article-title>. <source>Cognition &amp; Emotion</source>, <volume>23</volume>, <fpage>209</fpage>–<lpage>237</lpage>.</citation>
</ref>
<ref id="bibr48-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Niedenthal</surname><given-names>P. M.</given-names></name>
<name><surname>Brauer</surname><given-names>M.</given-names></name>
<name><surname>Halberstadt</surname><given-names>J. B.</given-names></name>
<name><surname>Innes-Ker</surname><given-names>Å. H.</given-names></name>
</person-group> (<year>2001</year>). <article-title>When did her smile drop? Facial mimicry and the influence of emotional state on the detection of change in emotional expression</article-title>. <source>Cognition &amp; Emotion</source>, <volume>15</volume>, <fpage>853</fpage>–<lpage>864</lpage>.</citation>
</ref>
<ref id="bibr49-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Pantic</surname><given-names>M.</given-names></name>
<name><surname>Patras</surname><given-names>I.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Dynamics of facial expression: Recognition of facial actions and their temporal segments from face profile image sequences</article-title>. <source>IEEE Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics</source>, <volume>36</volume>, <fpage>433</fpage>–<lpage>449</lpage>.</citation>
</ref>
<ref id="bibr50-1754073912451349">
<citation citation-type="book">
<person-group person-group-type="author">
<name><surname>Ruef</surname><given-names>A. M.</given-names></name>
<name><surname>Levenson</surname><given-names>R. W.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Continuous measurement of emotion</article-title>. In <person-group person-group-type="editor">
<name><surname>Coan</surname><given-names>J. A.</given-names></name>
<name><surname>Allen</surname><given-names>J. B.</given-names></name>
</person-group> (Eds.), <source>Handbook of emotion elicitation and assessment</source> (pp. <fpage>286</fpage>–<lpage>297</lpage>). <publisher-loc>New York, NY</publisher-loc>: <publisher-name>Oxford University Press</publisher-name>.</citation>
</ref>
<ref id="bibr51-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>J. A.</given-names></name>
</person-group> (<year>1994</year>). <article-title>Is there universal recognition of emotion from facial expression? A review of the cross-cultural studies</article-title>. <source>Psychological Bulletin</source>, <volume>115</volume>, <fpage>102</fpage>–<lpage>141</lpage>.</citation>
</ref>
<ref id="bibr52-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Russell</surname><given-names>J. A.</given-names></name>
<name><surname>Bachorowski</surname><given-names>J. A.</given-names></name>
<name><surname>Fernández-Dols</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2003</year>). <article-title>Facial and vocal expression of emotion</article-title>. <source>Annual Review of Psychology</source>, <volume>54</volume>, <fpage>329</fpage>–<lpage>349</lpage>.</citation>
</ref>
<ref id="bibr53-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Fujimura</surname><given-names>T.</given-names></name>
<name><surname>Suzuki</surname><given-names>N.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Enhanced facial EMG activity in response to dynamic facial expressions</article-title>. <source>International Journal of Psychophysiology</source>, <volume>70</volume>, <fpage>70</fpage>–<lpage>74</lpage>.</citation>
</ref>
<ref id="bibr54-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Kochiyama</surname><given-names>T.</given-names></name>
<name><surname>Yoshikawa</surname><given-names>S.</given-names></name>
<name><surname>Naito</surname><given-names>E.</given-names></name>
<name><surname>Matsumura</surname><given-names>M.</given-names></name>
</person-group> (<year>2004</year>). <article-title>Enhanced neural activity in response to dynamic facial expressions of emotion: An fMRI study</article-title>. <source>Cognitive Brain Research</source>, <volume>20</volume>, <fpage>81</fpage>–<lpage>91</lpage>.</citation>
</ref>
<ref id="bibr55-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Yoshikawa</surname><given-names>S.</given-names></name>
</person-group> (<year>2004</year>). <article-title>The dynamic aspects of emotional facial expressions</article-title>. <source>Cognition &amp; Emotion</source>, <volume>18</volume>, <fpage>701</fpage>–<lpage>710</lpage>.</citation>
</ref>
<ref id="bibr56-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Yoshikawa</surname><given-names>S.</given-names></name>
</person-group> (<year>2007a</year>). <article-title>Enhanced experience of emotional arousal in response to dynamic facial expressions</article-title>. <source>Journal of Nonverbal Behavior</source>, <volume>31</volume>, <fpage>119</fpage>–<lpage>135</lpage>.</citation>
</ref>
<ref id="bibr57-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Sato</surname><given-names>W.</given-names></name>
<name><surname>Yoshikawa</surname><given-names>S.</given-names></name>
</person-group> (<year>2007b</year>). <article-title>Spontaneous facial mimicry in response to dynamic facial expressions</article-title>. <source>Cognition</source>, <volume>104</volume>, <fpage>1</fpage>–<lpage>18</lpage>.</citation>
</ref>
<ref id="bibr58-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Schultz</surname><given-names>J.</given-names></name>
<name><surname>Pilz</surname><given-names>K. S.</given-names></name>
</person-group> (<year>2009</year>). <article-title>Natural facial motion enhances cortical responses to faces</article-title>. <source>Experimental Brain Research</source>, <volume>194</volume>, <fpage>465</fpage>–<lpage>475</lpage>.</citation>
</ref>
<ref id="bibr59-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Tcherkassof</surname><given-names>A.</given-names></name>
<name><surname>Bollon</surname><given-names>T.</given-names></name>
<name><surname>Dubois</surname><given-names>M.</given-names></name>
<name><surname>Pansu</surname><given-names>P.</given-names></name>
<name><surname>Adam</surname><given-names>J. M.</given-names></name>
</person-group> (<year>2007</year>). <article-title>Facial expressions of emotions: A methodological contribution to the study of spontaneous and dynamic emotional faces</article-title>. <source>European Journal of Social Psychology</source>, <volume>37</volume>, <fpage>1325</fpage>–<lpage>1345</lpage>.</citation>
</ref>
<ref id="bibr60-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Vinter</surname><given-names>A.</given-names></name>
</person-group> (<year>1986</year>). <article-title>The role of movement in eliciting early imitations</article-title>. <source>Child Development</source>, <volume>57</volume>, <fpage>66</fpage>–<lpage>71</lpage>.</citation>
</ref>
<ref id="bibr61-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wallraven</surname><given-names>C.</given-names></name>
<name><surname>Breidt</surname><given-names>M.</given-names></name>
<name><surname>Cunningham</surname><given-names>D. W.</given-names></name>
<name><surname>Bülthoff</surname><given-names>H. H.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Evaluating the perceptual realism of animated facial expressions</article-title>. <source>ACM Transactions on Applied Perception</source>, <volume>4</volume>, <fpage>1</fpage>–<lpage>20</lpage>.</citation>
</ref>
<ref id="bibr62-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Wehrle</surname><given-names>T.</given-names></name>
<name><surname>Kaiser</surname><given-names>S.</given-names></name>
<name><surname>Schmidt</surname><given-names>S.</given-names></name>
<name><surname>Scherer</surname><given-names>K. R.</given-names></name>
</person-group> (<year>2000</year>). <article-title>Studying the dynamics of emotional expression using synthesized facial muscle movements</article-title>. <source>Journal of Personality and Social Psychology</source>, <volume>78</volume>, <fpage>105</fpage>–<lpage>119</lpage>.</citation>
</ref>
<ref id="bibr63-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Weyers</surname><given-names>P.</given-names></name>
<name><surname>Mühlberger</surname><given-names>A.</given-names></name>
<name><surname>Hefele</surname><given-names>C.</given-names></name>
<name><surname>Pauli</surname><given-names>P.</given-names></name>
</person-group> (<year>2006</year>). <article-title>Electromyographic responses to static and dynamic avatar emotional facial expressions</article-title>. <source>Psychophysiology</source>, <volume>43</volume>, <fpage>1</fpage>–<lpage>4</lpage>.</citation>
</ref>
<ref id="bibr64-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>With</surname><given-names>S.</given-names></name>
<name><surname>Kaiser</surname><given-names>S.</given-names></name>
</person-group> (<year>2011</year>). <article-title>Sequential patterning of facial actions in the production and perception of emotional expressions</article-title>. <source>Swiss Journal of Psychology</source>, <volume>70</volume>, <fpage>241</fpage>–<lpage>252</lpage>.</citation>
</ref>
<ref id="bibr65-1754073912451349">
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Yoshikawa</surname><given-names>S.</given-names></name>
<name><surname>Sato</surname><given-names>W.</given-names></name>
</person-group> (<year>2008</year>). <article-title>Dynamic facial expressions of emotion induce representational momentum</article-title>. <source>Cognitive, Affective, &amp; Behavioral Neuroscience</source>, <volume>8</volume>, <fpage>25</fpage>–<lpage>31</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>