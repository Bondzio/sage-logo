<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" article-type="editorial" dtd-version="2.3" xml:lang="EN">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">MDM</journal-id>
<journal-id journal-id-type="hwp">spmdm</journal-id>
<journal-id journal-id-type="nlm-ta">Med Decis Making</journal-id>
<journal-title>Medical Decision Making</journal-title>
<issn pub-type="ppub">0272-989X</issn>
<issn pub-type="epub">1552-681X</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0272989X12458161</article-id>
<article-id pub-id-type="publisher-id">10.1177_0272989X12458161</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Editorials</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Transparently, with Validation</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author" xlink:type="simple">
<name name-style="western"><surname>Neil</surname><given-names>Nancy</given-names></name>
<degrees>PhD</degrees>
</contrib>
</contrib-group>
<pub-date pub-type="epub-ppub">
<month>9</month>
<year>2012</year>
</pub-date>
<volume>32</volume>
<issue>5</issue>
<issue-title>Special Issue: Recommendations of the ISPOR-SMDM Joint Modeling Good Research Practices Task Force</issue-title>
<fpage>660</fpage>
<lpage>662</lpage>
<custom-meta-wrap>
<custom-meta xlink:type="simple">
<meta-name>cover-date</meta-name>
<meta-value>September–October 2012</meta-value>
</custom-meta>
</custom-meta-wrap>
</article-meta>
</front>
<body>
<p>The ISPOR/SMDM guidelines for best practices in decision model transparency and validation, published in this issue, provide a solid scientific foundation for lively conversation about these important topics. No model should be approached imprudently, and these recommendations ably outline necessary components of critical appraisal. Still, there is much more to this topic: more to say about decision models and their purpose, more to consider about transparency, and more to do before we reach the point at which decision makers can reasonably determine whether a given model is “valid enough” for their purposes.</p>
<sec id="section1-0272989X12458161">
<title>Sometimes the Bottom Line Doesn’t Matter</title>
<p>It is worth noting that health care models provide decision makers with qualitative as well as quantitative information about decision problems. Certainly models for understanding health care decision problems should reflect our best science, and a user’s ability to have confidence in a model’s quantitative results is paramount when prediction is the point of the exercise. In some circumstances, though, users look to models for other reasons—to make the relationships in a system explicit, to simplify complexity in order to allow deeper understanding, or to facilitate thinking that improves discussion. In these instances there may be no right answer. There may be no bottom line, only possibilities and relative tradeoffs among alternatives. The raison d’être of many models is simply to help decision makers identify these tradeoffs and to support intelligent discussions about possibilities. Ultimately what matters for a model is that it asks good questions and helps users explore good answers.</p>
</sec>
<sec id="section2-0272989X12458161">
<title>Good Design Is Worth a Thousand Words</title>
<p>Transparency in health care decision models is often difficult to achieve because we tend to approach modeling in the wrong way. It takes enormous craft, discipline, and experience to construct a model, and modelers have developed meticulous, sophisticated methods to build them well. But modeling is not about methods. It’s about questions and answers. The methods are just the means.</p>
<p>If a model is intended to be evaluated, then make it evaluable. If it is intended to be used, then make it usable. Good design can facilitate this by enabling people to understand and process large amounts of information quickly, inviting them to think about substance without documentation standing in the way. Indeed, words can be strong deterrents to communication. Good design, in contrast, can communicate wordlessly, with clarity, precision, and efficiency. As a simple example, consider the wordlessness—and transparency—of <xref ref-type="fig" rid="fig1-0272989X12458161">Figure 1</xref>. Any written description of this system would, in all likelihood, be opaque to most readers.</p>
<fig id="fig1-0272989X12458161" position="float">
<label>Figure 1</label>
<caption><p>Hand wrap instructions.</p></caption>
<graphic alt-version="no" position="float" xlink:href="10.1177_0272989X12458161-fig1.tif" xlink:type="simple"/>
</fig>
<p>Good architects—people who make good buildings— know the difference between design and construction. Design explores different perspectives on the work— What will it do? Who is it for? In what context will it be used? How will we know whether it’s successful?—and integrates those perspectives into a plan. Construction is the act of building for reliability, integrity, and accuracy. Lacking design, construction can go wherever it pleases without regard for anyone but the creator. Good models require both good design and good construction. Without design, even the most well-constructed models can quickly become impenetrable academic exercises understood only by other modelers.</p>
<p>Social scientists have spent countless hours watching how people do things and studying how they think about what they do. Concepts from fields as diverse as cognitive psychology, learning theory, graphic design, visual analytics, and usability engineering, to name a few, have implications for enhancing the transparency of health care decision models beyond the written word. (Investigate these authors: Donald Norman, Edward Tufte, Herbert Simon, Jeff Johnson, Jennifer Tidwell, Mihaly Czikszentmihalyi, Scott Berkun, Stephen Few, Teresa Amabile.) For example, conceptualize a model to meet the need, not the other way around (marketing). Create a model with a logical flow that makes sense to the user (storytelling). Encourage new model users to persevere by helping them attain success from their initial actions (instant gratification). Allow users to proceed with an adequate but perhaps not comprehensive understanding of the model (satisficing). Make it easy to think through the component parts of the problem rather than the whole problem at one time (incremental construction). Organize information according to users’ vocabularies and ways of categorizing concepts (spatial memory). Simplify tasks by splitting them up into groups of operations, each of which can be addressed in a discrete mental space (chunking). Use visual cues to associate objects and ideas (grouping and alignment). Let go of preconceived notions about how decision models are supposed to look; any approach is acceptable as long as it solves the problem in an accurate, interesting way (form follows function). The challenge—and delight—is to realize the potential of these and other disciplines to transform a health care decision model from a rarefied academic undertaking into a real-world tool. This transformation will come when there is a commitment by health care decision modelers to work beyond the usual disciplines, embrace different approaches, and learn from the experiences of a variety of others.</p>
</sec>
<sec id="section3-0272989X12458161">
<title>Knowledge Use</title>
<p>The thorough evaluation/validation of a complex health care decision model requires careful scholarship. Some may argue that there is no substitute for formal training to perform such evaluations. Still, there are far more health care decision makers than there are formally trained health care decision-modeling experts. Best practice guidelines, such as those appearing in this issue, are important contributions from the formally trained to the less so. Such guidelines represent necessary knowledge. The next steps involve the adoption and diffusion of that knowledge.</p>
<p>Where are health care decision makers on the learning curve of model evaluation/validation? That is, how are models being evaluated currently, in practice, by users who are not formally trained? How might we expand the knowledge base of those who need to evaluate models, at least enough to help them determine whether a model should not be trusted? Can formally trained experts teach interested parties—in an effective, nontechnical, robust way—how to approach a model in order to determine whether it’s “valid enough” for their purposes? How can users know when a formally trained expert should be consulted and how that expert might be found?</p>
</sec>
<sec id="section4-0272989X12458161">
<title>“We Don’t Have Time to Review Them All”</title>
<p>Most of the time decision makers won’t really care about the technical details of model construction. Decision makers care more about a model <italic>having</italic> validity than they do about the statistical details that establish validity. Besides, even if decision makers have the necessary technical background and formal training, reviewing a model takes time—which is often in short supply. It’s only natural in such situations that decision makers might look to proxy indicators of validity, such as the model sponsor or source citation. This can be misleading, since industry-sponsored models are not necessarily biased and published models are not necessarily sound.</p>
<p>What might it look like to fashion mechanisms to bridge the gap between formal modeling theory and the practical demands of health care decision making? I’ve already mentioned outreach, but there may be other opportunities as well. One idea is development of a standardized template for a health care decision model dossier; such a dossier could reinforce standards of evidence and streamline the model evaluation process for decision makers and peer reviewers alike. Confidentiality agreements obtained prior to the delivery of a modeling dossier could protect intellectual property.</p>
<p>Another idea is to develop a certification process for model evaluators, perhaps with specialties in the assessment of a model’s concept/structure, implementation/validation, and technical/nontechnical documentation. Certified independent evaluators would assess one or more of these elements without knowing the model’s sponsor or results, without having a vested interest in the modeling decision, and without implying endorsement. Independent evaluators might offer constructive feedback at the time of model development (reviewing a model’s concept in advance of construction, for instance), contribute to peer review, prepare summary opinions, and/or pose clarifying questions to which a developer could respond in a modeling dossier. Developers and decision makers might consult an independent model evaluator with specific questions—or might become certified evaluators themselves, in the process raising the bar on the practice of health care decision modeling.</p>
<p>Commonly understood standards and expectations can be liberating for model developers, evaluators, and decision makers alike. The more broadly those standards are accepted and the more ways there are to ensure that expectations are met, the more likely it will be that health care decision models communicate clearly, efficiently, and accurately—that is, transparently, with validation.</p>
</sec>
</body>
</article>