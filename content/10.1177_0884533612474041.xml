<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE article
  PUBLIC "-//NLM//DTD Journal Publishing DTD v2.3 20070202//EN" "journalpublishing.dtd">
<article xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" xml:lang="en">
<front>
<journal-meta>
<journal-id journal-id-type="publisher-id">NCP</journal-id>
<journal-id journal-id-type="hwp">spncp</journal-id>
<journal-id journal-id-type="nlm-ta">Nutr Clin Pract</journal-id>
<journal-title>Nutrition in Clinical Practice</journal-title>
<issn pub-type="ppub">0884-5336</issn>
<issn pub-type="epub">1941-2452</issn>
<publisher>
<publisher-name>SAGE Publications</publisher-name>
<publisher-loc>Sage CA: Los Angeles, CA</publisher-loc>
</publisher>
</journal-meta>
<article-meta>
<article-id pub-id-type="doi">10.1177/0884533612474041</article-id>
<article-id pub-id-type="publisher-id">10.1177_0884533612474041</article-id>
<article-categories>
<subj-group subj-group-type="heading">
<subject>Techniques and Procedures</subject>
</subj-group>
</article-categories>
<title-group>
<article-title>Critical Reading and Critical Thinking—Study Design and Methodology</article-title>
<subtitle>A Personal Approach on How to Read the Clinical Literature</subtitle>
</title-group>
<contrib-group>
<contrib contrib-type="author" corresp="yes">
<name><surname>Lipman</surname><given-names>Timothy O.</given-names></name>
<degrees>MD</degrees>
</contrib>
</contrib-group>
<aff id="aff1-0884533612474041">Department of Veterans Affairs, Gastroenterology-Hepatology-Nutrition Section, Medical Service, Washington, DC</aff>
<author-notes>
<corresp id="corresp1-0884533612474041">Timothy O. Lipman, MD, Department of Veterans Affairs Medical Center, GI-Hepatology-Nutrition Section, Medical Service, 50 Irving St NW, Washington, DC 20422, USA. Email: <email>timothy.o.lipman@gmail.com</email></corresp>
</author-notes>
<pub-date pub-type="epub-ppub">
<month>4</month>
<year>2013</year>
</pub-date>
<volume>28</volume>
<issue>2</issue>
<issue-title>Research and Publishing</issue-title>
<fpage>158</fpage>
<lpage>164</lpage>
<permissions>
<copyright-statement>© 2013 American Society for Parenteral and Enteral Nutrition</copyright-statement>
<copyright-year>2013</copyright-year>
<copyright-holder content-type="society">The American Society for Parenteral and Enteral Nutrition</copyright-holder>
</permissions>
<abstract>
<p>The volume of medical literature grows exponentially. Yet we are faced with the necessity to make clinical decisions based on the availability and quality of scientific information. The general strength (reliability, robustness) of any interpretation that guides us in clinical decision making is dependent on how information was obtained. All information and medical studies and, consequently, all conclusions are not created equal. It is incumbent upon us to be able to assess the quality of the information that guides us in the care of our patients. Being able to assess medical literature critically requires use of critical reading and critical thinking skills. To achieve these skills, to be able to analyze medical literature critically, takes a combination of education and practice, practice, and more practice.</p>
</abstract>
<kwd-group>
<kwd>research</kwd>
<kwd>research design</kwd>
<kwd>publications</kwd>
<kwd>randomized controlled trials</kwd>
</kwd-group>
</article-meta>
</front>
<body>
<p><disp-quote>
<p>The conventional view serves to protect us from the painful job of thinking.</p>
<attrib>—John Kenneth Galbraith (1908–2006)</attrib>
</disp-quote></p>
<p><disp-quote>
<p>A great many people think they are thinking when they are really rearranging their prejudices.</p>
<attrib>—William James (1842–1910)</attrib>
</disp-quote>
</p>
<sec id="section1-0884533612474041">
<title>Learning Objectives</title>
<list id="list1-0884533612474041" list-type="bullet">
<list-item><p>Learn the components of a medical article</p></list-item>
<list-item><p>Learn how study design determines the conclusions that can be drawn</p></list-item>
<list-item><p>Learn what factors are important to assess quality in a clinical study</p></list-item></list>
<p>The volume of medical literature grows exponentially. Yet we are faced with the necessity to make clinical decisions based on the availability and quality of scientific information. The general strength (reliability, robustness) of any interpretation that guides us in clinical decision making is dependent on how information was obtained. All information and medical studies and, consequently, all conclusions are not created equal. It is incumbent upon us to be able to assess the quality of the information that guides us in the care of our patients. Being able to assess medical literature critically requires use of critical reading and critical thinking skills. To achieve these skills, to be able to analyze medical literature critically, takes a combination of education and practice, practice, and more practice.</p> <p>The objective of this article is to provide the reader with an approach to critical thinking as one reads a clinical journal article. I have tried to include what I consider the most important aspects of critical reading and study design that should be considered in approaches to assessing the medical literature. Although obviously the reader of this article is interested in artificial nutrition, the approach in outline here is not limited to clinical nutrition support but is applicable to all clinical literature.</p>
<p>This topic of critical reading and critical thinking has been a lifelong process for me. I started learning and practicing more than 35 years ago when I was a gastroenterology (GI) fellow. As a GI fellow, I was taught, prodded, and coaxed to read and think critically. This has been an ongoing learning experience, in which I continue to read and learn about various aspects of study design and methodology and their effect(s) on what we know. I have read books, studied journal articles, talked with colleagues, and more recently have become involved to a minor degree with the Cochrane Collaboration. There is no way that I can do justice to 35 years of learning and the abundant literature currently available, but what follows is a précis of how I approach a journal article.</p>
</sec>
<sec id="section2-0884533612474041">
<title>Overview of a Medical Article</title>
<p>There are a number of components to a medical article (see <xref ref-type="table" rid="table1-0884533612474041">Table 1</xref>). These components provide a crucial structural scaffolding upon which to base any critical analysis. These include the title and authors, documented conflicts of interest, the abstract, the Introduction section, the Methods section, the Results section, the Discussion section, and the references. Even before diving into reading an article, a few aspects of the paper are important to think about. Is the title neutral, or is it an attempt to “market” more than can be delivered in the body of the article? A good exercise, after reading an article, is to go back and look at the title. Does it reflect the article’s contents, or is it attempting to overpromise? Another task that may be difficult for the novice, or even the experienced reader, is to assess the author(s). Does the author(s) have a known track record for quality investigations in the past? This information may be difficult to determine only from an overview of a single article. Of considerable importance over the past several years is the following question: are there conflicts of interest? Data suggest that industry-sponsored studies may be more likely to find results favorable for the sponsored product than non–industry-sponsored studies. Is the study industry sponsored? Do the authors have a potential bias in having received industry support in the form of research funding, consulting fees, or speaker’s honoraria? Do the authors have other conflicts: an academic reputation to protect? Do they have some form of vested interest in the outcome—perhaps to confirm prior findings in which they have a reputation invested?</p>
<table-wrap id="table1-0884533612474041" position="float">
<label>Table 1.</label>
<caption><p>Key Components of a Medical Article.</p></caption>
<graphic alternate-form-of="table1-0884533612474041" xlink:href="10.1177_0884533612474041-table1.tif"/>
<table>
<colgroup>
<col align="left"/>
</colgroup>
<tbody>
<tr>
<td>• Title/author</td>
</tr>
<tr>
<td>• Sponsorship/conflict of interest</td>
</tr>
<tr>
<td>• Abstract</td>
</tr>
<tr>
<td>• Introduction</td>
</tr>
<tr>
<td>• Methods</td>
</tr>
<tr>
<td>• Results</td>
</tr>
<tr>
<td>• Discussion</td>
</tr>
<tr>
<td>• References</td>
</tr>
</tbody>
</table>
</table-wrap>
<p>Most readers tend to focus on the abstract, because reading it is quick, usually easy, and can indicate the “essence” of an article. These same readers may also look at the Discussion section, especially the conclusion(s). Such an approach is the least critical method for evaluating medical literature. An abstract attempts to catch the reader’s attention, whereas the Discussion section is the opportunity to put the “best face” on the author(s)’ data. More times than I have fingers on my hands I have found conclusions listed in the abstract that are not supported by any data in the article. The Introduction section usually provides the background and general rationale for the study. This section should detail briefly what is known about the topic, what may be controversial, and why the authors thought further research was necessary. The last sentence of the introduction is critical to assessing the quality of the report and to assert your critical thinking skills. It is here that the authors should explicitly state the question to be answered in the study, usually by providing a clearly defined hypothesis. At a minimum, there must be a clearly defined question to be answered. If the reader cannot determine what the specific question is that is under study, the remainder of the article is suspect. How can a question be answered in a study if the question is poorly formulated or not formulated at all? A basic question to be asked by the critical reader of the article is the following: is there a hypothesis formulated for which an answer is sought, have the authors gone on a fishing expedition, or do they even provide information as to why you should read the remainder of the article?</p>
<p>The critical reader concentrates on the Methods and the Results sections. The Methods section is one of the two most important parts of an article, because it is here that the authors define how they have approached answering the question that they asked. The Methods section details the study design and should give specific details about the study population. This information includes the source and selection of subjects and allocation of subjects to different interventions, if it is a comparative study. It should indicate how many subjects were anticipated to be entered and how that number was derived. Too few subjects in a study may result in several types of errors—either a false determination that a difference exists when it really does not or a presumed lack of difference when a difference truly exists. The Methods section also should state what is to be measured and how it will be measured—what was the primary end point or outcome measured—because it is around this information that the statistical tests involved are developed. While you, as a reader, are considering the measure of outcomes, ask yourself the following: is the study measuring a clinical outcome or a surrogate outcome (usually a laboratory value) that may or may not be related to important clinical outcomes? At the end of the Methods section, the reader should be able to understand what question is being asked, is there a prestudy hypothesis, are there enough subjects in the study, and what measurements were planned. Most important, by the end of the Methods section, the critical reader can ask if the methods presented truly describe the best means to answer the question(s) originally posed by the authors.</p>
<p>Equally important as the Methods section is the Results section. Here is presented what was found. I discuss results further below. Once the reader is finished with the Methods and the Results sections, he or she should be able to draw his or her own conclusions. Determine for yourself the answer(s) to the study hypothesis. Was the question answered? Do not be dependent on the authors’ determination of conclusions.</p>
<p>The Discussion section of an article is where the authors try to interpret the meaning of their data, compare them with other published data, and draw the reader’s attention. Remember that the discussion is the authors’ interpretation. The authors may often be trying to place the best possible “spin” on their results, may emphasize secondary outcomes, or otherwise place their findings in the best possible light. A good Discussion section should include at least 1 paragraph describing potential study limitations, due to the study design, study findings, or other reasons. The critical reader must decide if the results found justify the authors’ conclusions. The reader’s interpretation of the results may differ from those of the authors’.</p>
<p>Finally, the references can provide useful ancillary information. Obviously, the reference list provides the background for the study under question. Knowledge of the other literature by the reader is helpful, because one may discover that the referenced literature is unbalanced. Do the references come from the authors’ prior work? Are there competing data or contrary conclusions existent that have not been presented?</p>
</sec>
<sec id="section3-0884533612474041">
<title>Study Design</title>
<p>Knowledge about study design is important because the type of study determines what type of conclusions can be drawn—descriptive observations of nature or potential comparisons, which may represent associations or causality. If one seeks comparisons, virtually all study designs involving comparisons, with one exception, can only determine associations, not causality. The critical reader should understand the basics of study methodology and the type of conclusion that can be derived from a given design. Clinical studies can be characterized by several descriptors: descriptive vs explanatory, retrospective vs prospective, controlled vs uncontrolled, and observational vs experimental. Although several of these categories are mutually exclusive (one cannot have a retrospective experimental study), most studies involve several identifiers: descriptive-retrospective, explanatory-retrospective-observational-comparative, and so on.</p>
<p><italic>Observational studies</italic> involve the investigator observing nature, whereas <italic>experimental studies</italic> involve the investigator trying to control nature. Both are explanatory, but the former can only define associations, whereas the latter can determine causality. Observational studies tend to have at least some retrospective components. Methodological issues in observational studies involve the definition and selection of a study population, sampling procedures, comparability of cases to controls, and dependability of information that is subject to recall. Experimental studies similarly have design issues that will be discussed below.</p>
<p><italic>Descriptive studies</italic> usually consist of case reports, case series, or descriptive cohorts (populations). Descriptive studies document observations and experience. They may be a record of the natural history of disease or sharing of new experience. Descriptive studies can be used to establish <italic>incidence</italic> (the number of new cases of a condition developing per year) and <italic>prevalence</italic> (the number of cases of a condition present in a population at any given point in time). A descriptive study can be the start of explanations for pathophysiology, but descriptive studies make no comparisons and cannot purport to involve changing the natural history of a disease process.</p>
<p><italic>Explanatory studies</italic> involve comparisons and may seek associations or causes, better diagnoses, or predictors of outcome. Examples of explanatory studies include case-control, cross-sectional cohort, cohorts with historical controls, and interventional trials.</p>
<p><italic>Retrospective studies</italic> look to the past for information. Retrospective data may be obtained for both descriptive and explanatory studies. Data from the past have been collected before any questions have been asked; such data may be obtained from subject recall or medical records. The advantages of retrospective design include the ability to study rare diseases or conditions, less time needed to perform the study, the ability to explore multiple associations, and the ease of gathering information. Retrospective studies are subject to a variety of methodological errors, including inadequate or incorrect data obtained from the past. At best, retrospective studies can only suggest associations, not causality, and may be the stimulus to generate hypotheses for further testing.</p>
<p><italic>Prospective studies</italic> look to the future. The data to be collected are decided in advance and then collected as time moves forward. The data to be collected presumably are answering specific questions that have been established, and the data are collected in a systematic manner. Many observational cohorts are currently active and repeatedly publish new findings from their prospectively followed subjects. These studies often present information on nutrition, lifestyle, or drug toxicity. Data that are collected prospectively to answer questions that have not yet been asked, a so-called prospective observational cohort design, should always be judged critically, as there is no primary study hypothesis at the outset; even if comparisons are involved, prospective observational cohort studies can only suggest associations, never causality. Prospective experimental (interventional) studies with controls are the only type of study that can determine causality, not just associations. The advantage of a prospective controlled, clinical study is the greater assurance that the characteristic being examined preceded the outcome under study. Prospective clinical studies, however, can be expensive and time-consuming.</p>
<p><italic>Controls</italic> are necessary if the question(s) involve(s) comparisons. A control is an individual or group that differs in some way from the study group—does not have a risk factor, does not receive an intervention, and so on. If one wants to determine causality in defining an outcome difference, the control group should differ from the study group in only one aspect or variable. If there is more than one variable, then it could be any of the individual variables that are important for determining outcome. Controls may not always be necessary but in most circumstances are necessary. In the classic “cliff” example, if one jumps off of a 1000-foot cliff, death is certain; if 100 people jump off a 1000-foot cliff, 100 deaths are certain. One does not need to have a study group with parachutes and a control group without parachutes to know the outcome. On the other hand, a 10-foot jump would be different. We would not expect 100% mortality from such a jump. We could guess the outcome, or we could observe 100 subjects jump off of a 10-foot cliff to determine how many die, how many have broken limbs, how many have soft tissue internal injuries, and how many can walk away uninjured. But we might have to consider other variables. What is the landing surface? We would expect a different outcome with a 10-foot jump into deep water vs a 10-foot jump into a bed of 6-inch spikes. We still only have an observational, descriptive study, without an intervention to determine if an outcome can be changed. If we wanted to develop a study involving comparisons and possible interventions, it would then revolve around specific questions: is an 8-foot bungee cord effective in protecting healthy young adult men who jump off a 10-foot cliff onto a bed of 6-inch spikes? If we are interested in a group of wheelchair-bound seniors jumping 10 feet down into a 15-foot deep pool of water, our questions and interventions might be different. When, as in the case of most clinical situations, an absolutely predictable outcome is not available, there is need for a group in which the intervention is not provided—a control. We need to compare the intervention with no intervention or a different intervention to determine if there is a benefit derived from an intervention. I use the “cliff” example because this analogy has been used to deny the need for controlled trials of nutrition support. “Starvation” is said to be like jumping off the 1000-foot cliff. I would argue, without time for lengthy discussion in this article, that short-term “starvation” (1–2 weeks) is more akin to the low-cliff analogy and that the circumstances of time and intervention need to be assessed and controlled in a proper study.</p>
</sec>
<sec id="section4-0884533612474041">
<title>Bias</title>
<p>Bias is not evil in medical studies but reflects the tendency in all of us—clinicians, investigators, and patients—to want our preconceived notions to be effective. If we did not think an intervention were effective, we would not use it. The scientific rigor of a study is dependent on the effort and means that have been undertaken to eliminate bias; the less bias, the more rigorous the conclusions that can be derived from a study. In clinical studies, bias can be defined as an error caused by systematically favoring some outcomes over others; in other words, bias represents a form of systematic error. Bias is produced by any process or effect at any stage of a study, from design to execution to application of information from the study, that produces results or conclusions that differ <italic>systematically</italic> from the truth. Bias can be reduced only by proper study design and execution. Bias is a direct reflection of the methodological rigor or quality in a study—the higher or better or more rigorous the methodological quality, the lower the risk of bias. Thus, we talk about studies with high risk of bias (low methodological quality) or low risk of bias (high methodological quality).</p>
<p>I shall discuss some examples of bias below, but why worry about bias? What is the practical effect(s) resulting from bias in clinical studies? Studies with high risk of bias tend to demonstrate more favorable outcomes in whatever intervention or outcome is being studied; this has been demonstrated time after time. It is incumbent upon the critical reader to understand the potential sources of bias in a study and to recognize that differences found may be less than what they appear if the results derive from studies with high risk of bias.</p>
</sec>
<sec id="section5-0884533612474041">
<title>Randomized Controlled Trials</title>
<p>Why randomized controlled trials, and are all randomized controlled trials created equal? If we wish to determine that an intervention alters an outcome, we need a comparison group, which does not receive the intervention, and the intervention should be the only difference between the 2 groups. If there are 2 differences between the groups, then either of the differences, or variables, might be responsible for an outcome difference; if there are 3 variables, then any of the 3 variables might be responsible for any differences found; if there are multiple variables, then any of the multiple variables might be responsible for observed outcome differences. A randomized controlled trial should incorporate a variety of methodologies, which strengthen the quality of the trial and make more probable that the results found represent “truth.” In short, various failures of methodological rigor in randomized trials may reduce study quality and increase the risk of bias.</p>
<sec id="section6-0884533612474041">
<title>Randomization</title>
<p>Just calling a trial <italic>randomized</italic> does not ensure trial quality. Randomization itself is a formal process by which any member of a defined study population has an equal chance of being entered into one of the study groups. This is done with the expectation that there will be only 1 variable between 2 groups who are being tested from the same study population; that is, the groups are comparable. If there is more than 1 variable—that is, if the 2 groups differ by more than 1 factor—then one cannot be certain that it is the intervention that is responsible for any outcome differences. Usually, <xref ref-type="table" rid="table1-0884533612474041">Table 1</xref> in any article is the attempt to demonstrate that 2 groups are similar in all respects other than the intervention under study. It is possible to have proper randomization and still result in differences between groups—this may be called bad luck. How does the reader know that proper randomization occurred? Look for a statement in the Methods section as to the method of randomization—usually with the use of a computer generation scheme or a random numbers table, although even a simple coin flip may sometimes be adequate. Methods in which assignment to a group may seem random but is in fact not, because there may not be an equal chance of entering into either study group, include day of admission, ward assignment, or Social Security number; “randomization” by these methods is inadequate, may introduce bias, and has been termed <italic>quasi-randomization</italic>. Be suspicious of any clinical article that does not specifically state the method of randomization in the Methods section.</p>
</sec>
<sec id="section7-0884533612474041">
<title>Study Size</title>
<p>Beyond randomization, a number of other issues are important determinants of trial quality and are necessary to reduce bias. How many subjects should be included in the trial—a few, a lot, or however many can be accumulated during 1 year? In fact, the number of subjects entered into the trial should be estimated based on prior studies or estimates of expected outcomes. When reading an article, look in the Methods section to see if there was an a priori determination of the number of subjects to be included in the study; look for an effect size determination and, from this, a power calculation and finally a sample size determination. If these determinants are not explicitly described, beware; the study lacks rigor and the risk of bias is increased.</p>
</sec>
<sec id="section8-0884533612474041">
<title>Concealment of Allocation</title>
<p>Another important quality factor is concealed allocation. This is a concept that I find quite difficult, yet failure to ensure concealment of allocation can lead to biased outcome results. I have found a good explanation in the <italic>British Medical Journal:</italic> “Concealed allocation means that the person who determined if a participant was eligible for inclusion in the trial was unaware, when this decision was made, to which group the participant would be allocated. If allocation is not concealed, it may be possible (consciously or unconsciously) to influence the group to which a participant is allocated. This may occur either by changing the order in which participants are enrolled, or the order in which treatments are provided. This could produce systematic biases in an otherwise random allocation” (Altman DG. Statistics notes: concealing treatment allocation in randomized trials. <italic>BMJ</italic>. 2001;323:446-447). Concealed allocation differs from randomization and blinding. In short, it is a process by which the investigators cannot cheat by finding out to which group an individual has been randomized. Good methodological reporting in a clinical trial will describe in the Methods section how concealment of allocation was achieved.</p>
</sec>
<sec id="section9-0884533612474041">
<title>Blinding or Masking</title>
<p>The next important methodological quality factor beyond randomization is blinding, or masking (<italic>masking</italic> is another term sometimes used instead of <italic>blinding;</italic> I use them here interchangeably). Blinding is not the same as randomization, although the two are often confused. Randomization is a process for allocating subjects into different groups with resultant comparability. Blinding reflects the fact that those involved in the study, investigator(s) and/or subject, do not know what intervention the subject is receiving. Sometimes, this may be relatively easy, if the intervention involves taking a pill. If the pill has no taste or smell or side effects, it should be relatively easy to construct a placebo pill, which in all other aspects other than active contents is equivalent to the intervention pill. If one is testing an herbal product, which the study population and investigator believes is clinically effective, if the herbal product tastes different from the placebo, and the subjects and the investigator know the difference and can guess which group the subject is in, the blind is broken, and bias, or systematic error, is introduced. Sometimes, investigators will ask subjects to guess which group they are in and report this information as a result to verify that the blind was not violated. In other words, after completely adequate randomization and allocation concealment, if the subject or the investigator knows, or can guess, which intervention group he or she is in, a major source of bias is entered into the trial and methodological quality and rigor suffer.</p>
<p>Sometimes, the ability to conceal which group a subject is in is not obvious, but steps need to be taken to attempt to achieve masking. Many consider it impossible to mask studies involving nutrition support. A “placebo” nasogastric tube or “placebo parenteral nutrition solution” is considered either unethical or impractical. However, a clinical study involving tube feeding or parenteral nutrition may mask the assessors of outcomes, even if the subjects and clinicians are not “blinded” to the intervention.</p>
</sec>
<sec id="section10-0884533612474041">
<title>Complete Analysis of All Subjects Entered</title>
<p>Two other features of a quality study consist of complete follow-up and analysis of all entered subjects from the time of randomization. Sometimes this process is not intuitively obvious, and sometimes it is just difficult. If a subject is randomized to a treatment and never receives the treatment or even dies early after randomization, how should he or she be counted? It does not seem “fair” to penalize an intervention if a subject does not even participate after randomization. However, the overarching question in a study is not just to analyze the “success” of an intervention but to analyze the totality of an approach after a decision has been made to use that approach. If 50% of subjects pull out a nasogastric feeding tube in a hypothetical study comparing tube feeding to oral intake, for example, and do not complete the study, this failure to complete the study needs to be included in the final outcome. If, in our hypothetical example, only the subjects who successfully complete the tube feeding are included in the final results, this would represent a biased outcome, because the approach did not work in 50% of the subjects randomized to the tube feeding group. The inclusion of all subjects who have been randomized in the final analysis is termed an <italic>intent-to-treat</italic> analysis. The intent-to-treat analysis is done to prevent breaking the original randomization assignment and to account for all dropouts or subjects who might cross over to an alternate treatment group. If only the subjects who complete the trial are included, this is termed a <italic>per-protocol</italic> analysis, is a major source of bias (or systematic error), and represents poor methodological quality.</p>
<p>How does one determine that there is complete follow-up of subjects? A corollary to this is how does one know that the study was not stopped early because an investigator looked at early results and they appeared promising? If there was not a sample size calculation presented in the Methods section, be wary of bias and exaggerated outcome effects.</p>
<p>In sum, randomized controlled clinical trials are the gold standard for determining efficacy of an intervention. However, just as there are differences in the weight of gold, and miners need to be wary of “fool’s gold,” not all randomized trials are golden. The critical reader needs be aware of the methodological pitfalls that may occur in the best intentioned randomized controlled trial.</p>
</sec></sec>
<sec id="section11-0884533612474041">
<title>Statistics</title>
<p>I have always found statistics daunting. I have never been able to understand statistics presented as mathematical formulas or in lectures by statisticians. However, there are some core principles that I have learned over the years, which I find key in critically reading an article.</p>
<p>Foremost, “<italic>P</italic>” is not a god worthy of animal or human sacrifices. <italic>P</italic> is not holy. <italic>P</italic> does not represent absolute truth. <italic>P</italic> is a symbol for probability—the likelihood that an event is due to chance. Remember that if one flips a coin 10 times, one would expect a heads 5 out of 10 times; however, one would not be surprised if “heads” appeared 6 out of 10 times; this would be a chance occurrence. <italic>P</italic> is measuring chance. A <italic>P</italic> value is only measuring the likelihood that a difference is due to chance; in other words, chance is not a likely explanation for an observed difference or that a difference is unlikely to have been found because of sampling alone. A value of <italic>P</italic> = .05 is arbitrary, and <italic>P</italic> &lt; .05 does not necessarily represent clinical truth. A <italic>P</italic> &gt; .05 may still be clinically important. Is a <italic>P</italic> value of .049 really different than one of .051? Does the former represent a meaningful clinical difference, whereas the latter is of no clinical consequence? The critical reader should understand—obviously not. Ultimately, the statistical analysis is there to help the reader decide to what degree an outcome difference may be due to chance.</p>
<p>Without understanding the mathematics of statistics, I have learned the following: (1) the main statistical analysis should be based on an underlying prestudy hypothesis, power calculation, and sample size calculation. If these were not present at the beginning, the “statistical tests” and statistical outcomes should be highly suspect. (2) Be suspicious of any articles that present “fishing expeditions,” analyzing multiple variables without a prestudy hypothesis to determine which is “significant.” (3) The more measurements, the more likelihood that some observed difference will be present only due to chance. If you read an article that presents multiple <italic>P</italic> values, and there has been no statistical correction for multiple measures, be suspicious. (4) Look for “95% confidence intervals” rather than <italic>P</italic> values alone; a <italic>P</italic> only provides a point estimate for a difference, whereas the confidence interval provides a range of which true effects are statistically compatible with the data and which are not. (5) Beware of statistical analysis provided after data are available; this is “officially” termed <italic>post hoc analysis</italic> and, unofficially, <italic>data-dredging;</italic> if the data analysis was not planned prior to the intervention, then analysis of unexpected or unplanned findings can, at best, only be hypotheses for future studies. To learn more about statistics, see the article by Saracino and colleagues in this issue.<sup><xref ref-type="bibr" rid="bibr1-0884533612474041">1</xref></sup></p>
</sec>
<sec id="section12-0884533612474041" sec-type="conclusions">
<title>Conclusions</title>
<p>Critical reading and critical thinking are lifelong skills. I first started as a GI fellow and have been honing my skills and learning ever since. I still am discovering new aspects to enhance my critical reading skills. It is an ongoing process, and one that cannot be learned by reading a single article or going to one course. It requires practice and more practice.</p>
<p>I have 2 suggestions as to where or how to start. (1) Pick a journal article whose title intrigues you. Read only the Methods and Results sections. See if you can determine what was the question being asked and if the question was answered appropriately. Determine the methodological rigor—is it a low-quality or high-quality study, that is, a study with a high risk of bias or a low risk of bias? In your mind, write you own title, abstract, and Discussion section. How does your creation compare with the authors? (2) Take any aspect of this overview that you want to pursue—observational study, randomization, types of bias in clinical trials, and so on—and type this into your favorite search engine (Google, Bing, Yahoo Search, etc) and explore the results.</p>
<p><xref ref-type="table" rid="table2-0884533612474041">Table 2</xref> provides one approach or a checklist for critical reading. Do your own discovery. Enjoy your voyage of critical thinking and learning. Now is the best time to start.</p>
<table-wrap id="table2-0884533612474041" position="float">
<label>Table 2.</label>
<caption><p>Quality Checklist for Reading the Medical Literature.</p></caption>
<graphic alternate-form-of="table2-0884533612474041" xlink:href="10.1177_0884533612474041-table2.tif"/>
<table>
<colgroup>
<col align="left"/>
</colgroup>
<tbody>
<tr>
<td>• What is the Study Design?</td>
</tr>
<tr>
<td>• Is there a Prestudy Hypothesis?</td>
</tr>
<tr>
<td>• If a Randomized Controlled Trial:</td>
</tr>
<tr>
<td> ◦ Are there enough subjects in the study?</td>
</tr>
<tr>
<td>  • Effect size estimation</td>
</tr>
<tr>
<td>  • Power calculation</td>
</tr>
<tr>
<td>  • Sample size determination</td>
</tr>
<tr>
<td> ◦ True Randomization?</td>
</tr>
<tr>
<td> ◦ Concealed Allocation?</td>
</tr>
<tr>
<td> ◦ Blinding</td>
</tr>
<tr>
<td> ◦ Intention-to-Treat analysis (as opposed to “per protocol”—only those completing the trial)</td>
</tr>
<tr>
<td> ◦ Is there complete follow-up?</td>
</tr>
<tr>
<td>• Are the results accurate and unbiased and clinically meaningful?</td>
</tr>
<tr>
<td>• Can the results be applied to your patients?</td>
</tr>
<tr>
<td> ◦ Are your patients similar to the trial patients?</td>
</tr>
<tr>
<td> ◦ Are the results clinically meaningful and achievable in your practice?</td>
</tr>
<tr>
<td> ◦ What are the benefits and burdens? Cost and toxicities?</td>
</tr>
<tr>
<td>• Are other safe and effective and less expensive interventions or noninterventions available?</td>
</tr>
</tbody>
</table>
</table-wrap>
</sec>
</body>
<back>
<ack>
<p>I am indebted to 2 mentors in my life who have pushed me and nurtured my understanding of critical reading skills. The first is my former chief of gastroenterology when I was a GI fellow and the chief of medicine when I joined the attending staff at the Washington, DC Veterans Affairs Medical Center: Dr James D. Finkelstein. The other is a longtime colleague and friend: Dr Ronald L. Koretz, Emeritus Professor of Medicine at UCLA. To both, I am grateful.</p>
</ack>
<fn-group>
<fn fn-type="financial-disclosure">
<p>Financial disclosure: None declared.</p>
</fn>
<fn fn-type="other">
<p>This article originally appeared online on February 14, 2013.</p>
</fn>
</fn-group>
<ref-list>
<title>Suggested Reading</title>
<p>Rather than a reference list, I provide some of the sources that I have found quite valuable over the years.</p>
</ref-list>
<ref-list>
<title>Books</title>
<p>Gelbach SH. <italic>Interpreting the Medical Literature</italic>. 5th ed. New York: McGraw-Hill; 2006.</p>
<p>Riegelman RK. <italic>Studying a Study and Testing a Test: How to Read The Medical Evidence</italic>. 5th ed. Philadelphia, PA: Lippincott Williams &amp; Wilkins; 2005.</p>
<p>Sackett DL, Haynes RB, Guyatt GH, Tugwell P. <italic>Clinical Epidemiology, a Basic Science for Clinical Medicine</italic>. 2nd ed. Boston: Little, Brown; 1991.</p>
</ref-list>
<ref-list>
<title>Journal Articles</title>
<p>Cash BD, Schoenfeld P, Rex D. An evidence-based medicine approach to studies of diagnostic tests: assessing the validity of virtual colonoscopy. <italic>Clin Gastroenterol Hepatol</italic>. 2003;1:136-144.</p>
<p>Chalmers I. Why fair tests . . . [of treatment] . . . are needed. <italic>ACP J Club</italic>. 2006;145(1):A8.</p>
<p>Guyatt GH, Briel M, Glasziou P, et al. Problems of stopping trials early. <italic>BMJ</italic>. 2012;344:e3863. doi: 10.1136/bmj.e3863.</p>
<p>Guyatt GH, Haynes RB, Jaeschke RZ, et al. Users’ guides to the medical literature, XXV: evidence-based medicine: principles for applying the users’ guides to patient care. Evidence-Based Medicine Working Group. <italic>JAMA</italic>. 2000;284:1290-1296.</p>
<p>Guyatt GH, Oxman AD, Kunz R, Vist GE, Falck-Ytter Y, Schünemann HJ; GRADE Working Group. What is “quality of evidence” and why is it important to clinicians? <italic>BMJ</italic>. 2008;336:995-998.</p>
<p>Guyatt GH, Oxman AD, Vist G, et al. GRADE guidelines, 4: rating the quality of evidence—study limitations (risk of bias). <italic>J Clin Epidemiol</italic>. 2011;64:407-415.</p>
<p>Ioannidis JPA. Why most published research findings are false. <italic>PLoS Med</italic>. 2005;2(8):e124.</p>
<p>Kunz R, Vist G, Oxman AD. Randomization to protect against selection bias in healthcare trials. <italic>Cochrane Database Syst Rev</italic>. 2007;(2):MR000012.</p>
<p>Lehrer J. Annals of science: the truth wears off. Is there something wrong with the scientific method? <italic>New Yorker</italic>. December 13, 2010.</p>
<p>Moher D, Pham B, Jones A, et al. Does quality of reports of randomized trials affect estimates of intervention efficacy reported in meta-analyses? <italic>Lancet</italic>. 1998;352:609-613.</p>
<p>Schoenfeld P, Chey WD. An evidence-based approach to clinical practice guidelines: diagnosis and treatment of irritable bowel syndrome. <italic>Clin Gastroenterol Hepatol</italic>. 2003;1:322-327.</p>
<p>Schoenfeld P, Scheiman J. An evidence-based approach to studies of gastrointestinal therapies. <italic>Clin Gastroenterol Hepatol</italic>. 2003;1:57-63.</p>
<p>Schulz KF, Altman DG, Moher D, for the CONSORT Group. CONSORT 2010 statement: update of guidelines for reporting parallel group randomized trials. <italic>BMJ</italic>. 2010;340:c332.</p>
<p>Schulz KF, Chalmers I, Hayes RJ, Altman DG. Empirical evidence of bias: dimensions of methodological quality associated with estimates of treatment effects in controlled trials. <italic>JAMA</italic>. 1995;273:408-412.</p>
</ref-list>
<ref-list>
<title>Websites</title>
<p>Center for Health Evidence. <ext-link ext-link-type="uri" xlink:href="http://www.cche.net">http://www.cche.net</ext-link></p>
<p>White River Junction Outcomes Group. Healthy skepticism. <ext-link ext-link-type="uri" xlink:href="http://www.vaoutcomes.org/washpost.php">http://www.vaoutcomes.org/washpost.php</ext-link>
</p>
</ref-list>
<ref-list>
<title>Reference</title>
<ref id="bibr1-0884533612474041">
<label>1.</label>
<citation citation-type="journal">
<person-group person-group-type="author">
<name><surname>Saracino</surname><given-names>G</given-names></name>
<name><surname>Jennings</surname><given-names>L</given-names></name>
<name><surname>Hasse</surname><given-names>J</given-names></name>
</person-group>. <article-title>Basic statistical concepts in nutrition research</article-title>. <source>Nutr Clin Pract</source>. <year>2013</year>;<volume>28</volume>:<fpage>182</fpage>-<lpage>193</lpage>.</citation>
</ref>
</ref-list>
</back>
</article>